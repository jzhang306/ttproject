[
  {
    "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function\n  Decomposition",
    "authors": [
      "Thomas G. Dietterich"
    ],
    "summary": "This paper presents the MAXQ approach to hierarchical reinforcement learning\nbased on decomposing the target Markov decision process (MDP) into a hierarchy\nof smaller MDPs and decomposing the value function of the target MDP into an\nadditive combination of the value functions of the smaller MDPs. The paper\ndefines the MAXQ hierarchy, proves formal results on its representational\npower, and establishes five conditions for the safe use of state abstractions.\nThe paper presents an online model-free learning algorithm, MAXQ-Q, and proves\nthat it converges wih probability 1 to a kind of locally-optimal policy known\nas a recursively optimal policy, even in the presence of the five kinds of\nstate abstraction. The paper evaluates the MAXQ representation and MAXQ-Q\nthrough a series of experiments in three domains and shows experimentally that\nMAXQ-Q (with state abstractions) converges to a recursively optimal policy much\nfaster than flat Q learning. The fact that MAXQ learns a representation of the\nvalue function has an important benefit: it makes it possible to compute and\nexecute an improved, non-hierarchical policy via a procedure similar to the\npolicy improvement step of policy iteration. The paper demonstrates the\neffectiveness of this non-hierarchical execution experimentally. Finally, the\npaper concludes with a comparison to related work and a discussion of the\ndesign tradeoffs in hierarchical reinforcement learning.",
    "published": "1999-05-21T14:26:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "State Abstraction in MAXQ Hierarchical Reinforcement Learning",
    "authors": [
      "Thomas G. Dietterich"
    ],
    "summary": "Many researchers have explored methods for hierarchical reinforcement\nlearning (RL) with temporal abstractions, in which abstract actions are defined\nthat can perform many primitive actions before terminating. However, little is\nknown about learning with state abstractions, in which aspects of the state\nspace are ignored. In previous work, we developed the MAXQ method for\nhierarchical RL. In this paper, we define five conditions under which state\nabstraction can be combined with the MAXQ value function decomposition. We\nprove that the MAXQ-Q learning algorithm converges under these conditions and\nshow experimentally that state abstraction is important for the successful\napplication of MAXQ-Q learning.",
    "published": "1999-05-21T14:49:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiplicative Algorithm for Orthgonal Groups and Independent Component\n  Analysis",
    "authors": [
      "Toshinao Akuzawa"
    ],
    "summary": "The multiplicative Newton-like method developed by the author et al. is\nextended to the situation where the dynamics is restricted to the orthogonal\ngroup. A general framework is constructed without specifying the cost function.\nThough the restriction to the orthogonal groups makes the problem somewhat\ncomplicated, an explicit expression for the amount of individual jumps is\nobtained. This algorithm is exactly second-order-convergent. The global\ninstability inherent in the Newton method is remedied by a\nLevenberg-Marquardt-type variation. The method thus constructed can readily be\napplied to the independent component analysis. Its remarkable performance is\nillustrated by a numerical simulation.",
    "published": "2000-01-07T06:20:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiplicative Nonholonomic/Newton -like Algorithm",
    "authors": [
      "Toshinao Akuzawa",
      "Noboru Murata"
    ],
    "summary": "We construct new algorithms from scratch, which use the fourth order cumulant\nof stochastic variables for the cost function. The multiplicative updating rule\nhere constructed is natural from the homogeneous nature of the Lie group and\nhas numerous merits for the rigorous treatment of the dynamics. As one\nconsequence, the second order convergence is shown. For the cost function,\nfunctions invariant under the componentwise scaling are choosen. By identifying\npoints which can be transformed to each other by the scaling, we assume that\nthe dynamics is in a coset space. In our method, a point can move toward any\ndirection in this coset. Thus, no prewhitening is required.",
    "published": "2000-02-09T06:44:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Complexity analysis for algorithmically simple strings",
    "authors": [
      "Andrei N. Soklakov"
    ],
    "summary": "Given a reference computer, Kolmogorov complexity is a well defined function\non all binary strings. In the standard approach, however, only the asymptotic\nproperties of such functions are considered because they do not depend on the\nreference computer. We argue that this approach can be more useful if it is\nrefined to include an important practical case of simple binary strings.\nKolmogorov complexity calculus may be developed for this case if we restrict\nthe class of available reference computers. The interesting problem is to\ndefine a class of computers which is restricted in a {\\it natural} way modeling\nthe real-life situation where only a limited class of computers is physically\navailable to us. We give an example of what such a natural restriction might\nlook like mathematically, and show that under such restrictions some error\nterms, even logarithmic in complexity, can disappear from the standard\ncomplexity calculus.\n  Keywords: Kolmogorov complexity; Algorithmic information theory.",
    "published": "2000-09-05T18:54:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Robust Classification for Imprecise Environments",
    "authors": [
      "Foster Provost",
      "Tom Fawcett"
    ],
    "summary": "In real-world environments it usually is difficult to specify target\noperating conditions precisely, for example, target misclassification costs.\nThis uncertainty makes building robust classification systems problematic. We\nshow that it is possible to build a hybrid classifier that will perform at\nleast as well as the best available classifier for any target conditions. In\nsome cases, the performance of the hybrid actually can surpass that of the best\nknown classifier. This robust performance extends across a wide variety of\ncomparison frameworks, including the optimization of metrics such as accuracy,\nexpected cost, lift, precision, recall, and workforce utilization. The hybrid\nalso is efficient to build, to store, and to update. The hybrid is based on a\nmethod for the comparison of classifier performance that is robust to imprecise\nclass distributions and misclassification costs. The ROC convex hull (ROCCH)\nmethod combines techniques from ROC analysis, decision analysis and\ncomputational geometry, and adapts them to the particulars of analyzing learned\nclassifiers. The method is efficient and incremental, minimizes the management\nof classifier performance data, and allows for clear visual comparisons and\nsensitivity analyses. Finally, we point to empirical evidence that a robust\nhybrid classifier indeed is needed for many real-world problems.",
    "published": "2000-09-13T21:09:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Top-down induction of clustering trees",
    "authors": [
      "Hendrik Blockeel",
      "Luc De Raedt",
      "Jan Ramon"
    ],
    "summary": "An approach to clustering is presented that adapts the basic top-down\ninduction of decision trees method towards clustering. To this aim, it employs\nthe principles of instance based learning. The resulting methodology is\nimplemented in the TIC (Top down Induction of Clustering trees) system for\nfirst order clustering. The TIC system employs the first order logical decision\ntree representation of the inductive logic programming system Tilde. Various\nexperiments with TIC are presented, in both propositional and relational\ndomains.",
    "published": "2000-11-21T21:51:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scaling Up Inductive Logic Programming by Learning from Interpretations",
    "authors": [
      "Hendrik Blockeel",
      "Luc De Raedt",
      "Nico Jacobs",
      "Bart Demoen"
    ],
    "summary": "When comparing inductive logic programming (ILP) and attribute-value learning\ntechniques, there is a trade-off between expressive power and efficiency.\nInductive logic programming techniques are typically more expressive but also\nless efficient. Therefore, the data sets handled by current inductive logic\nprogramming systems are small according to general standards within the data\nmining community. The main source of inefficiency lies in the assumption that\nseveral examples may be related to each other, so they cannot be handled\nindependently.\n  Within the learning from interpretations framework for inductive logic\nprogramming this assumption is unnecessary, which allows to scale up existing\nILP algorithms. In this paper we explain this learning setting in the context\nof relational databases. We relate the setting to propositional data mining and\nto the classical ILP setting, and show that learning from interpretations\ncorresponds to learning from multiple relations and thus extends the\nexpressiveness of propositional learning, while maintaining its efficiency to a\nlarge extent (which is not the case in the classical ILP setting).\n  As a case study, we present two alternative implementations of the ILP system\nTilde (Top-down Induction of Logical DEcision trees): Tilde-classic, which\nloads all data in main memory, and Tilde-LDS, which loads the examples one by\none. We experimentally compare the implementations, showing Tilde-LDS can\nhandle large data sets (in the order of 100,000 examples or 100 MB) and indeed\nscales up linearly in the number of examples.",
    "published": "2000-11-29T12:14:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Policies with External Memory",
    "authors": [
      "Leonid Peshkin",
      "Nicolas Meuleau",
      "Leslie Kaelbling"
    ],
    "summary": "In order for an agent to perform well in partially observable domains, it is\nusually necessary for actions to depend on the history of observations. In this\npaper, we explore a {\\it stigmergic} approach, in which the agent's actions\ninclude the ability to set and clear bits in an external memory, and the\nexternal memory is included as part of the input to the agent. In this case, we\nneed to learn a reactive policy in a highly non-Markovian domain. We explore\ntwo algorithms: SARSA(\\lambda), which has had empirical success in partially\nobservable domains, and VAPS, a new algorithm due to Baird and Moore, with\nconvergence guarantees in partially observable domains. We compare the\nperformance of these two algorithms on benchmark problems.",
    "published": "2001-03-02T01:55:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient algorithms for decision tree cross-validation",
    "authors": [
      "Hendrik Blockeel",
      "Jan Struyf"
    ],
    "summary": "Cross-validation is a useful and generally applicable technique often\nemployed in machine learning, including decision tree induction. An important\ndisadvantage of straightforward implementation of the technique is its\ncomputational overhead. In this paper we show that, for decision trees, the\ncomputational overhead of cross-validation can be reduced significantly by\nintegrating the cross-validation with the normal decision tree induction\nprocess. We discuss how existing decision tree algorithms can be adapted to\nthis aim, and provide an analysis of the speedups these adaptations may yield.\nThe analysis is supported by experimental results.",
    "published": "2001-10-17T15:45:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evaluation of the Performance of the Markov Blanket Bayesian Classifier\n  Algorithm",
    "authors": [
      "Michael G. Madden"
    ],
    "summary": "The Markov Blanket Bayesian Classifier is a recently-proposed algorithm for\nconstruction of probabilistic classifiers. This paper presents an empirical\ncomparison of the MBBC algorithm with three other Bayesian classifiers: Naive\nBayes, Tree-Augmented Naive Bayes and a general Bayesian network. All of these\nare implemented using the K2 framework of Cooper and Herskovits. The\nclassifiers are compared in terms of their performance (using simple accuracy\nmeasures and ROC curves) and speed, on a range of standard benchmark data sets.\nIt is concluded that MBBC is competitive in terms of speed and accuracy with\nthe other algorithms considered.",
    "published": "2002-11-01T18:09:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Approximating Incomplete Kernel Matrices by the em Algorithm",
    "authors": [
      "Koji Tsuda",
      "Shotaro Akaho",
      "Kiyoshi Asai"
    ],
    "summary": "In biological data, it is often the case that observed data are available\nonly for a subset of samples. When a kernel matrix is derived from such data,\nwe have to leave the entries for unavailable samples as missing. In this paper,\nwe make use of a parametric model of kernel matrices, and estimate missing\nentries by fitting the model to existing entries. The parametric model is\ncreated as a set of spectral variants of a complete kernel matrix derived from\nanother information source. For model fitting, we adopt the em algorithm based\non the information geometry of positive definite matrices. We will report\npromising results on bacteria clustering experiments using two marker\nsequences: 16S and gyrB.",
    "published": "2002-11-07T07:21:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reliable and Efficient Inference of Bayesian Networks from Sparse Data\n  by Statistical Learning Theory",
    "authors": [
      "Dominik Janzing",
      "Daniel Herrmann"
    ],
    "summary": "To learn (statistical) dependencies among random variables requires\nexponentially large sample size in the number of observed random variables if\nany arbitrary joint probability distribution can occur.\n  We consider the case that sparse data strongly suggest that the probabilities\ncan be described by a simple Bayesian network, i.e., by a graph with small\nin-degree \\Delta. Then this simple law will also explain further data with high\nconfidence. This is shown by calculating bounds on the VC dimension of the set\nof those probability measures that correspond to simple graphs. This allows to\nselect networks by structural risk minimization and gives reliability bounds on\nthe error of the estimated joint measure without (in contrast to a previous\npaper) any prior assumptions on the set of possible joint measures.\n  The complexity for searching the optimal Bayesian networks of in-degree\n\\Delta increases only polynomially in the number of random varibales for\nconstant \\Delta and the optimal joint measure associated with a given graph can\nbe found by convex optimization.",
    "published": "2003-09-10T13:56:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Toward Attribute Efficient Learning Algorithms",
    "authors": [
      "Adam R. Klivans",
      "Rocco A. Servedio"
    ],
    "summary": "We make progress on two important problems regarding attribute efficient\nlearnability.\n  First, we give an algorithm for learning decision lists of length $k$ over\n$n$ variables using $2^{\\tilde{O}(k^{1/3})} \\log n$ examples and time\n$n^{\\tilde{O}(k^{1/3})}$. This is the first algorithm for learning decision\nlists that has both subexponential sample complexity and subexponential running\ntime in the relevant parameters. Our approach establishes a relationship\nbetween attribute efficient learning and polynomial threshold functions and is\nbased on a new construction of low degree, low weight polynomial threshold\nfunctions for decision lists. For a wide range of parameters our construction\nmatches a 1994 lower bound due to Beigel for the ODDMAXBIT predicate and gives\nan essentially optimal tradeoff between polynomial threshold function degree\nand weight.\n  Second, we give an algorithm for learning an unknown parity function on $k$\nout of $n$ variables using $O(n^{1-1/k})$ examples in time polynomial in $n$.\nFor $k=o(\\log n)$ this yields a polynomial time algorithm with sample\ncomplexity $o(n)$. This is the first polynomial time algorithm for learning\nparity on a superconstant number of variables with sublinear sample complexity.",
    "published": "2003-11-27T05:34:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improving spam filtering by combining Naive Bayes with simple k-nearest\n  neighbor searches",
    "authors": [
      "Daniel Etzold"
    ],
    "summary": "Using naive Bayes for email classification has become very popular within the\nlast few months. They are quite easy to implement and very efficient. In this\npaper we want to present empirical results of email classification using a\ncombination of naive Bayes and k-nearest neighbor searches. Using this\ntechnique we show that the accuracy of a Bayes filter can be improved slightly\nfor a high number of features and significantly for a small number of features.",
    "published": "2003-11-30T20:41:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "About Unitary Rating Score Constructing",
    "authors": [
      "Kromer Victor"
    ],
    "summary": "It is offered to pool test points of different subjects and different aspects\nof the same subject together in order to get the unitary rating score, by the\nway of nonlinear transformation of indicator points in accordance with Zipf's\ndistribution. It is proposed to use the well-studied distribution of\nIntellectuality Quotient IQ as the reference distribution for latent variable\n\"progress in studies\".",
    "published": "2004-01-08T07:50:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Mining Heterogeneous Multivariate Time-Series for Learning Meaningful\n  Patterns: Application to Home Health Telecare",
    "authors": [
      "Florence Duchene",
      "Catherine Garbay",
      "Vincent Rialle"
    ],
    "summary": "For the last years, time-series mining has become a challenging issue for\nresearchers. An important application lies in most monitoring purposes, which\nrequire analyzing large sets of time-series for learning usual patterns. Any\ndeviation from this learned profile is then considered as an unexpected\nsituation. Moreover, complex applications may involve the temporal study of\nseveral heterogeneous parameters. In that paper, we propose a method for mining\nheterogeneous multivariate time-series for learning meaningful patterns. The\nproposed approach allows for mixed time-series -- containing both pattern and\nnon-pattern data -- such as for imprecise matches, outliers, stretching and\nglobal translating of patterns instances in time. We present the early results\nof our approach in the context of monitoring the health status of a person at\nhome. The purpose is to build a behavioral profile of a person by analyzing the\ntime variations of several quantitative or qualitative parameters recorded\nthrough a provision of sensors installed in the home.",
    "published": "2004-12-01T16:32:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stability Analysis for Regularized Least Squares Regression",
    "authors": [
      "Cynthia Rudin"
    ],
    "summary": "We discuss stability for a class of learning algorithms with respect to noisy\nlabels. The algorithms we consider are for regression, and they involve the\nminimization of regularized risk functionals, such as L(f) := 1/N sum_i\n(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when\ny_i is a noisy version of f*(x_i) for some function f* in H, the output of the\nalgorithm converges to f* as the regularization term and noise simultaneously\nvanish. We consider two flavors of this problem, one where a data set of N\npoints remains fixed, and the other where N -> infinity. For the case where N\n-> infinity, we give conditions for convergence to f_E (the function which is\nthe expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we\ndescribe the limiting 'non-noisy', 'non-regularized' function f*, and give\nconditions for convergence. In the process, we develop a set of tools for\ndealing with functionals such as L(f), which are applicable to many other\nproblems in learning theory.",
    "published": "2005-02-03T19:54:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Probabilistic and Team PFIN-type Learning: General Properties",
    "authors": [
      "Andris Ambainis"
    ],
    "summary": "We consider the probability hierarchy for Popperian FINite learning and study\nthe general properties of this hierarchy. We prove that the probability\nhierarchy is decidable, i.e. there exists an algorithm that receives p_1 and\np_2 and answers whether PFIN-type learning with the probability of success p_1\nis equivalent to PFIN-type learning with the probability of success p_2.\n  To prove our result, we analyze the topological structure of the probability\nhierarchy. We prove that it is well-ordered in descending ordering and\norder-equivalent to ordinal epsilon_0. This shows that the structure of the\nhierarchy is very complicated.\n  Using similar methods, we also prove that, for PFIN-type learning, team\nlearning and probabilistic learning are of the same power.",
    "published": "2005-03-31T23:04:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Non-asymptotic calibration and resolution",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "We analyze a new algorithm for probability forecasting of binary observations\non the basis of the available data, without making any assumptions about the\nway the observations are generated. The algorithm is shown to be well\ncalibrated and to have good resolution for long enough sequences of\nobservations and for a suitable choice of its parameter, a kernel on the\nCartesian product of the forecast space $[0,1]$ and the data space. Our main\nresults are non-asymptotic: we establish explicit inequalities, shown to be\ntight, for the performance of the algorithm.",
    "published": "2005-06-01T14:03:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Defensive forecasting for linear protocols",
    "authors": [
      "Vladimir Vovk",
      "Ilia Nouretdinov",
      "Akimichi Takemura",
      "Glenn Shafer"
    ],
    "summary": "We consider a general class of forecasting protocols, called \"linear\nprotocols\", and discuss several important special cases, including multi-class\nforecasting. Forecasting is formalized as a game between three players:\nReality, whose role is to generate observations; Forecaster, whose goal is to\npredict the observations; and Skeptic, who tries to make money on any lack of\nagreement between Forecaster's predictions and the actual observations. Our\nmain mathematical result is that for any continuous strategy for Skeptic in a\nlinear protocol there exists a strategy for Forecaster that does not allow\nSkeptic's capital to grow. This result is a meta-theorem that allows one to\ntransform any continuous law of probability in a linear protocol into a\nforecasting strategy whose predictions are guaranteed to satisfy this law. We\napply this meta-theorem to a weak law of large numbers in Hilbert spaces to\nobtain a version of the K29 prediction algorithm for linear protocols and show\nthat this version also satisfies the attractive properties of proper\ncalibration and resolution under a suitable choice of its kernel parameter,\nwith no assumptions about the way the data is generated.",
    "published": "2005-06-02T13:26:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "About one 3-parameter Model of Testing",
    "authors": [
      "Kromer Victor"
    ],
    "summary": "This article offers a 3-parameter model of testing, with 1) the difference\nbetween the ability level of the examinee and item difficulty; 2) the examinee\ndiscrimination and 3) the item discrimination as model parameters.",
    "published": "2005-06-14T04:00:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Job Training",
    "authors": [
      "Jason E. Holt"
    ],
    "summary": "We propose a new framework for building and evaluating machine learning\nalgorithms. We argue that many real-world problems require an agent which must\nquickly learn to respond to demands, yet can continue to perform and respond to\nnew training throughout its useful life. We give a framework for how such\nagents can be built, describe several metrics for evaluating them, and show\nthat subtle changes in system construction can significantly affect agent\nperformance.",
    "published": "2005-06-22T21:21:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiresolution Kernels",
    "authors": [
      "Marco Cuturi",
      "Kenji Fukumizu"
    ],
    "summary": "We present in this work a new methodology to design kernels on data which is\nstructured with smaller components, such as text, images or sequences. This\nmethodology is a template procedure which can be applied on most kernels on\nmeasures and takes advantage of a more detailed \"bag of components\"\nrepresentation of the objects. To obtain such a detailed description, we\nconsider possible decompositions of the original bag into a collection of\nnested bags, following a prior knowledge on the objects' structure. We then\nconsider these smaller bags to compare two objects both in a detailed\nperspective, stressing local matches between the smaller bags, and in a global\nor coarse perspective, by considering the entire bag. This multiresolution\napproach is likely to be best suited for tasks where the coarse approach is not\nprecise enough, and where a more subtle mixture of both local and global\nsimilarities is necessary to compare objects. The approach presented here would\nnot be computationally tractable without a factorization trick that we\nintroduce before presenting promising results on an image retrieval task.",
    "published": "2005-07-13T05:45:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Defensive Universal Learning with Experts",
    "authors": [
      "Jan Poland",
      "Marcus Hutter"
    ],
    "summary": "This paper shows how universal learning can be achieved with expert advice.\nTo this aim, we specify an experts algorithm with the following\ncharacteristics: (a) it uses only feedback from the actions actually chosen\n(bandit setup), (b) it can be applied with countably infinite expert classes,\nand (c) it copes with losses that may grow in time appropriately slowly. We\nprove loss bounds against an adaptive adversary. From this, we obtain a master\nalgorithm for \"reactive\" experts problems, which means that the master's\nactions may influence the behavior of the adversary. Our algorithm can\nsignificantly outperform standard experts algorithms on such problems. Finally,\nwe combine it with a universal expert class. The resulting universal learner\nperforms -- in a certain sense -- almost as well as any computable strategy,\nfor any online decision problem. We also specify the (worst-case) convergence\nspeed, which is very slow.",
    "published": "2005-07-18T14:33:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "FPL Analysis for Adaptive Bandits",
    "authors": [
      "Jan Poland"
    ],
    "summary": "A main problem of \"Follow the Perturbed Leader\" strategies for online\ndecision problems is that regret bounds are typically proven against oblivious\nadversary. In partial observation cases, it was not clear how to obtain\nperformance guarantees against adaptive adversary, without worsening the\nbounds. We propose a conceptually simple argument to resolve this problem.\nUsing this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed\nbandit problem is shown. This bound holds for the common FPL variant using only\nthe observations from designated exploration rounds. Using all observations\nallows for the stronger bound of O(t^(1/2)), matching the best bound known so\nfar (and essentially the known lower bound) for adversarial bandits.\nSurprisingly, this variant does not even need explicit exploration, it is\nself-stabilizing. However the sampling probabilities have to be either\nexternally provided or approximated to sufficient accuracy, using O(t^2 log t)\nsamples in each step.",
    "published": "2005-07-26T05:00:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Optimal Augmented Bayes Networks",
    "authors": [
      "Vikas Hamine",
      "Paul Helman"
    ],
    "summary": "Naive Bayes is a simple Bayesian classifier with strong independence\nassumptions among the attributes. This classifier, desipte its strong\nindependence assumptions, often performs well in practice. It is believed that\nrelaxing the independence assumptions of a naive Bayes classifier may improve\nthe classification accuracy of the resulting structure. While finding an\noptimal unconstrained Bayesian Network (for most any reasonable scoring\nmeasure) is an NP-hard problem, it is possible to learn in polynomial time\noptimal networks obeying various structural restrictions. Several authors have\nexamined the possibilities of adding augmenting arcs between attributes of a\nNaive Bayes classifier. Friedman, Geiger and Goldszmidt define the TAN\nstructure in which the augmenting arcs form a tree on the attributes, and\npresent a polynomial time algorithm that learns an optimal TAN with respect to\nMDL score. Keogh and Pazzani define Augmented Bayes Networks in which the\naugmenting arcs form a forest on the attributes (a collection of trees, hence a\nrelaxation of the stuctural restriction of TAN), and present heuristic search\nmethods for learning good, though not optimal, augmenting arc sets. The\nauthors, however, evaluate the learned structure only in terms of observed\nmisclassification error and not against a scoring metric, such as MDL. In this\npaper, we present a simple, polynomial time greedy algorithm for learning an\noptimal Augmented Bayes Network with respect to MDL score.",
    "published": "2005-09-19T04:57:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Unions of $ω(1)$-Dimensional Rectangles",
    "authors": [
      "Alp Atici",
      "Rocco A. Servedio"
    ],
    "summary": "We consider the problem of learning unions of rectangles over the domain\n$[b]^n$, in the uniform distribution membership query learning setting, where\nboth b and n are \"large\". We obtain poly$(n, \\log b)$-time algorithms for the\nfollowing classes:\n  - poly$(n \\log b)$-way Majority of $O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log\nb)})$-dimensional rectangles.\n  - Union of poly$(\\log(n \\log b))$ many $O(\\frac{\\log^2 (n \\log b)} {(\\log\n\\log(n \\log b) \\log \\log \\log (n \\log b))^2})$-dimensional rectangles.\n  - poly$(n \\log b)$-way Majority of poly$(n \\log b)$-Or of disjoint\n$O(\\frac{\\log(n \\log b)} {\\log \\log(n \\log b)})$-dimensional rectangles.\n  Our main algorithmic tool is an extension of Jackson's boosting- and\nFourier-based Harmonic Sieve algorithm [Jackson 1997] to the domain $[b]^n$,\nbuilding on work of [Akavia, Goldwasser, Safra 2003]. Other ingredients used to\nobtain the results stated above are techniques from exact learning [Beimel,\nKushilevitz 1998] and ideas from recent work on learning augmented $AC^{0}$\ncircuits [Jackson, Klivans, Servedio 2002] and on representing Boolean\nfunctions as thresholds of parities [Klivans, Servedio 2001].",
    "published": "2005-10-14T19:26:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On-line regression competitive with reproducing kernel Hilbert spaces",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "We consider the problem of on-line prediction of real-valued labels, assumed\nbounded in absolute value by a known constant, of new objects from known\nlabeled objects. The prediction algorithm's performance is measured by the\nsquared deviation of the predictions from the actual labels. No stochastic\nassumptions are made about the way the labels and objects are generated.\nInstead, we are given a benchmark class of prediction rules some of which are\nhoped to produce good predictions. We show that for a wide range of\ninfinite-dimensional benchmark classes one can construct a prediction algorithm\nwhose cumulative loss over the first N examples does not exceed the cumulative\nloss of any prediction rule in the class plus O(sqrt(N)); the main differences\nfrom the known results are that we do not impose any upper bound on the norm of\nthe considered prediction rules and that we achieve an optimal leading term in\nthe excess loss of our algorithm. If the benchmark class is \"universal\" (dense\nin the class of continuous functions on each compact set), this provides an\non-line non-stochastic analogue of universally consistent prediction in\nnon-parametric statistics. We use two proof techniques: one is based on the\nAggregating Algorithm and the other on the recently developed method of\ndefensive forecasting.",
    "published": "2005-11-15T17:13:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounds on Query Convergence",
    "authors": [
      "Barak A. Pearlmutter"
    ],
    "summary": "The problem of finding an optimum using noisy evaluations of a smooth cost\nfunction arises in many contexts, including economics, business, medicine,\nexperiment design, and foraging theory. We derive an asymptotic bound E[ (x_t -\nx*)^2 ] >= O(1/sqrt(t)) on the rate of convergence of a sequence (x_0, x_1,\n>...) generated by an unbiased feedback process observing noisy evaluations of\nan unknown quadratic function maximised at x*. The bound is tight, as the proof\nleads to a simple algorithm which meets it. We further establish a bound on the\ntotal regret, E[ sum_{i=1..t} (x_i - x*)^2 ] >= O(sqrt(t)) These bounds may\nimpose practical limitations on an agent's performance, as O(eps^-4) queries\nare made before the queries converge to x* with eps accuracy.",
    "published": "2005-11-25T15:57:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Preference Learning in Terminology Extraction: A ROC-based approach",
    "authors": [
      "Jérôme Azé",
      "Mathieu Roche",
      "Yves Kodratoff",
      "Michèle Sebag"
    ],
    "summary": "A key data preparation step in Text Mining, Term Extraction selects the\nterms, or collocation of words, attached to specific concepts. In this paper,\nthe task of extracting relevant collocations is achieved through a supervised\nlearning algorithm, exploiting a few collocations manually labelled as\nrelevant/irrelevant. The candidate terms are described along 13 standard\nstatistical criteria measures. From these examples, an evolutionary learning\nalgorithm termed Roger, based on the optimization of the Area under the ROC\ncurve criterion, extracts an order on the candidate terms. The robustness of\nthe approach is demonstrated on two real-world domain applications, considering\ndifferent domains (biology and human resources) and different languages\n(English and French).",
    "published": "2005-12-13T13:25:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Competing with wild prediction rules",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "We consider the problem of on-line prediction competitive with a benchmark\nclass of continuous but highly irregular prediction rules. It is known that if\nthe benchmark class is a reproducing kernel Hilbert space, there exists a\nprediction algorithm whose average loss over the first N examples does not\nexceed the average loss of any prediction rule in the class plus a \"regret\nterm\" of O(N^(-1/2)). The elements of some natural benchmark classes, however,\nare so irregular that these classes are not Hilbert spaces. In this paper we\ndevelop Banach-space methods to construct a prediction algorithm with a regret\nterm of O(N^(-1/p)), where p is in [2,infty) and p-2 reflects the degree to\nwhich the benchmark class fails to be a Hilbert space.",
    "published": "2005-12-14T20:03:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Genetic Programming, Validation Sets, and Parsimony Pressure",
    "authors": [
      "Christian Gagné",
      "Marc Schoenauer",
      "Marc Parizeau",
      "Marco Tomassini"
    ],
    "summary": "Fitness functions based on test cases are very common in Genetic Programming\n(GP). This process can be assimilated to a learning task, with the inference of\nmodels from a limited number of samples. This paper is an investigation on two\nmethods to improve generalization in GP-based learning: 1) the selection of the\nbest-of-run individuals using a three data sets methodology, and 2) the\napplication of parsimony pressure in order to reduce the complexity of the\nsolutions. Results using GP in a binary classification setup show that while\nthe accuracy on the test sets is preserved, with less variances compared to\nbaseline results, the mean tree size obtained with the tested methods is\nsignificantly reduced.",
    "published": "2006-01-11T15:39:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Processing of Test Matrices with Guessing Correction",
    "authors": [
      "Kromer Victor"
    ],
    "summary": "It is suggested to insert into test matrix 1s for correct responses, 0s for\nresponse refusals, and negative corrective elements for incorrect responses.\nWith the classical test theory approach test scores of examinees and items are\ncalculated traditionally as sums of matrix elements, organized in rows and\ncolumns. Correlation coefficients are estimated using correction coefficients.\nIn item response theory approach examinee and item logits are estimated using\nmaximum likelihood method and probabilities of all matrix elements.",
    "published": "2006-01-20T05:40:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning rational stochastic languages",
    "authors": [
      "François Denis",
      "Yann Esposito",
      "Amaury Habrard"
    ],
    "summary": "Given a finite set of words w1,...,wn independently drawn according to a\nfixed unknown distribution law P called a stochastic language, an usual goal in\nGrammatical Inference is to infer an estimate of P in some class of\nprobabilistic models, such as Probabilistic Automata (PA). Here, we study the\nclass of rational stochastic languages, which consists in stochastic languages\nthat can be generated by Multiplicity Automata (MA) and which strictly includes\nthe class of stochastic languages generated by PA. Rational stochastic\nlanguages have minimal normal representation which may be very concise, and\nwhose parameters can be efficiently estimated from stochastic samples. We\ndesign an efficient inference algorithm DEES which aims at building a minimal\nnormal representation of the target. Despite the fact that no recursively\nenumerable class of MA computes exactly the set of rational stochastic\nlanguages over Q, we show that DEES strongly identifies tis set in the limit.\nWe study the intermediary MA output by DEES and show that they compute rational\nseries which converge absolutely to one and which can be used to provide\nstochastic languages which closely estimate the target.",
    "published": "2006-02-17T08:57:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "General Discounting versus Average Reward",
    "authors": [
      "Marcus Hutter"
    ],
    "summary": "Consider an agent interacting with an environment in cycles. In every\ninteraction cycle the agent is rewarded for its performance. We compare the\naverage reward U from cycle 1 to m (average value) with the future discounted\nreward V from cycle k to infinity (discounted value). We consider essentially\narbitrary (non-geometric) discount sequences and arbitrary reward sequences\n(non-MDP environments). We show that asymptotically U for m->infinity and V for\nk->infinity are equal, provided both limits exist. Further, if the effective\nhorizon grows linearly with k or faster, then existence of the limit of U\nimplies that the limit of V exists. Conversely, if the effective horizon grows\nlinearly with k or slower, then existence of the limit of V implies that the\nlimit of U exists.",
    "published": "2006-05-09T10:39:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Sequence Prediction for Arbitrary Measures",
    "authors": [
      "Daniil Ryabko",
      "Marcus Hutter"
    ],
    "summary": "Suppose we are given two probability measures on the set of one-way infinite\nfinite-alphabet sequences and consider the question when one of the measures\npredicts the other, that is, when conditional probabilities converge (in a\ncertain sense) when one of the measures is chosen to generate the sequence.\nThis question may be considered a refinement of the problem of sequence\nprediction in its most general formulation: for a given class of probability\nmeasures, does there exist a measure which predicts all of the measures in the\nclass? To address this problem, we find some conditions on local absolute\ncontinuity which are sufficient for prediction and which generalize several\ndifferent notions which are known to be sufficient for prediction. We also\nformulate some open questions to outline a direction for finding the conditions\non classes of measures for which prediction is possible.",
    "published": "2006-06-16T16:33:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Predictions as statements and decisions",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "Prediction is a complex notion, and different predictors (such as people,\ncomputer programs, and probabilistic theories) can pursue very different goals.\nIn this paper I will review some popular kinds of prediction and argue that the\ntheory of competitive on-line learning can benefit from the kinds of prediction\nthat are now foreign to it.",
    "published": "2006-06-22T04:31:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC Classification based on PAC Estimates of Label Class Distributions",
    "authors": [
      "Nick Palmer",
      "Paul W. Goldberg"
    ],
    "summary": "A standard approach in pattern classification is to estimate the\ndistributions of the label classes, and then to apply the Bayes classifier to\nthe estimates of the distributions in order to classify unlabeled examples. As\none might expect, the better our estimates of the label class distributions,\nthe better the resulting classifier will be. In this paper we make this\nobservation precise by identifying risk bounds of a classifier in terms of the\nquality of the estimates of the label class distributions. We show how PAC\nlearnability relates to estimates of the distributions that have a PAC\nguarantee on their $L_1$ distance from the true distribution, and we bound the\nincrease in negative log likelihood risk in terms of PAC bounds on the\nKL-divergence. We give an inefficient but general-purpose smoothing method for\nconverting an estimated distribution that is good under the $L_1$ metric into a\ndistribution that is good under the KL-divergence.",
    "published": "2006-07-11T13:52:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Competing with stationary prediction strategies",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "In this paper we introduce the class of stationary prediction strategies and\nconstruct a prediction algorithm that asymptotically performs as well as the\nbest continuous stationary strategy. We make mild compactness assumptions but\nno stochastic assumptions about the environment. In particular, no assumption\nof stationarity is made about the environment, and the stationarity of the\nconsidered strategies only means that they do not depend explicitly on time; we\nargue that it is natural to consider only stationary strategies even for highly\nnon-stationary environments.",
    "published": "2006-07-13T15:52:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Using Pseudo-Stochastic Rational Languages in Probabilistic Grammatical\n  Inference",
    "authors": [
      "Amaury Habrard",
      "Francois Denis",
      "Yann Esposito"
    ],
    "summary": "In probabilistic grammatical inference, a usual goal is to infer a good\napproximation of an unknown distribution P called a stochastic language. The\nestimate of P stands in some class of probabilistic models such as\nprobabilistic automata (PA). In this paper, we focus on probabilistic models\nbased on multiplicity automata (MA). The stochastic languages generated by MA\nare called rational stochastic languages; they strictly include stochastic\nlanguages generated by PA; they also admit a very concise canonical\nrepresentation. Despite the fact that this class is not recursively enumerable,\nit is efficiently identifiable in the limit by using the algorithm DEES,\nintroduced by the authors in a previous paper. However, the identification is\nnot proper and before the convergence of the algorithm, DEES can produce MA\nthat do not define stochastic languages. Nevertheless, it is possible to use\nthese MA to define stochastic languages. We show that they belong to a broader\nclass of rational series, that we call pseudo-stochastic rational languages.\nThe aim of this paper is twofold. First we provide a theoretical study of\npseudo-stochastic rational languages, the languages output by DEES, showing for\nexample that this class is decidable within polynomial time. Second, we have\ncarried out a lot of experiments in order to compare DEES to classical\ninference algorithms such as ALERGIA and MDI. They show that DEES outperforms\nthem in most cases.",
    "published": "2006-07-18T07:21:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Logical settings for concept learning from incomplete examples in First\n  Order Logic",
    "authors": [
      "Dominique Bouthinon",
      "Henry Soldano",
      "Véronique Ventos"
    ],
    "summary": "We investigate here concept learning from incomplete examples. Our first\npurpose is to discuss to what extent logical learning settings have to be\nmodified in order to cope with data incompleteness. More precisely we are\ninterested in extending the learning from interpretations setting introduced by\nL. De Raedt that extends to relational representations the classical\npropositional (or attribute-value) concept learning from examples framework. We\nare inspired here by ideas presented by H. Hirsh in a work extending the\nVersion space inductive paradigm to incomplete data. H. Hirsh proposes to\nslightly modify the notion of solution when dealing with incomplete examples: a\nsolution has to be a hypothesis compatible with all pieces of information\nconcerning the examples. We identify two main classes of incompleteness. First,\nuncertainty deals with our state of knowledge concerning an example. Second,\ngeneralization (or abstraction) deals with what part of the description of the\nexample is sufficient for the learning purpose. These two main sources of\nincompleteness can be mixed up when only part of the useful information is\nknown. We discuss a general learning setting, referred to as \"learning from\npossibilities\" that formalizes these ideas, then we present a more specific\nlearning setting, referred to as \"assumption-based learning\" that cope with\nexamples which uncertainty can be reduced when considering contextual\ninformation outside of the proper description of the examples. Assumption-based\nlearning is illustrated on a recent work concerning the prediction of a\nconsensus secondary structure common to a set of RNA sequences.",
    "published": "2006-07-20T14:52:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Theory of Probabilistic Boosting, Decision Trees and Matryoshki",
    "authors": [
      "Etienne Grossmann"
    ],
    "summary": "We present a theory of boosting probabilistic classifiers. We place ourselves\nin the situation of a user who only provides a stopping parameter and a\nprobabilistic weak learner/classifier and compare three types of boosting\nalgorithms: probabilistic Adaboost, decision tree, and tree of trees of ... of\ntrees, which we call matryoshka. \"Nested tree,\" \"embedded tree\" and \"recursive\ntree\" are also appropriate names for this algorithm, which is one of our\ncontributions. Our other contribution is the theoretical analysis of the\nalgorithms, in which we give training error bounds. This analysis suggests that\nthe matryoshka leverages probabilistic weak classifiers more efficiently than\nsimple decision trees.",
    "published": "2006-07-25T15:57:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Leading strategies in competitive on-line prediction",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "We start from a simple asymptotic result for the problem of on-line\nregression with the quadratic loss function: the class of continuous\nlimited-memory prediction strategies admits a \"leading prediction strategy\",\nwhich not only asymptotically performs at least as well as any continuous\nlimited-memory strategy but also satisfies the property that the excess loss of\nany continuous limited-memory strategy is determined by how closely it imitates\nthe leading strategy. More specifically, for any class of prediction strategies\nconstituting a reproducing kernel Hilbert space we construct a leading\nstrategy, in the sense that the loss of any prediction strategy whose norm is\nnot too large is determined by how closely it imitates the leading strategy.\nThis result is extended to the loss functions given by Bregman divergences and\nby strictly proper scoring rules.",
    "published": "2006-07-27T22:11:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Competing with Markov prediction strategies",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "Assuming that the loss function is convex in the prediction, we construct a\nprediction strategy universal for the class of Markov prediction strategies,\nnot necessarily continuous. Allowing randomization, we remove the requirement\nof convexity.",
    "published": "2006-07-28T21:45:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Study on Learnability for Rigid Lambek Grammars",
    "authors": [
      "Roberto Bonato"
    ],
    "summary": "We present basic notions of Gold's \"learnability in the limit\" paradigm,\nfirst presented in 1967, a formalization of the cognitive process by which a\nnative speaker gets to grasp the underlying grammar of his/her own native\nlanguage by being exposed to well formed sentences generated by that grammar.\nThen we present Lambek grammars, a formalism issued from categorial grammars\nwhich, although not as expressive as needed for a full formalization of natural\nlanguages, is particularly suited to easily implement a natural interface\nbetween syntax and semantics. In the last part of this work, we present a\nlearnability result for Rigid Lambek grammars from structured examples.",
    "published": "2006-08-06T16:10:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Massive Local Rules Search Approach to the Classification Problem",
    "authors": [
      "Vladislav Malyshkin",
      "Ray Bakhramov",
      "Andrey Gorodetsky"
    ],
    "summary": "An approach to the classification problem of machine learning, based on\nbuilding local classification rules, is developed. The local rules are\nconsidered as projections of the global classification rules to the event we\nwant to classify. A massive global optimization algorithm is used for\noptimization of quality criterion. The algorithm, which has polynomial\ncomplexity in typical case, is used to find all high--quality local rules. The\nother distinctive feature of the algorithm is the integration of attributes\nlevels selection (for ordered attributes) with rules searching and original\nconflicting rules resolution strategy. The algorithm is practical; it was\ntested on a number of data sets from UCI repository, and a comparison with the\nother predicting techniques is presented.",
    "published": "2006-09-03T21:30:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Metric entropy in competitive on-line prediction",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "Competitive on-line prediction (also known as universal prediction of\nindividual sequences) is a strand of learning theory avoiding making any\nstochastic assumptions about the way the observations are generated. The\npredictor's goal is to compete with a benchmark class of prediction rules,\nwhich is often a proper Banach function space. Metric entropy provides a\nunifying framework for competitive on-line prediction: the numerous known upper\nbounds on the metric entropy of various compact sets in function spaces readily\nimply bounds on the performance of on-line prediction strategies. This paper\ndiscusses strengths and limitations of the direct approach to competitive\non-line prediction via metric entropy, including comparisons to other\napproaches.",
    "published": "2006-09-09T11:31:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC Learning Mixtures of Axis-Aligned Gaussians with No Separation\n  Assumption",
    "authors": [
      "Jon Feldman",
      "Ryan O'Donnell",
      "Rocco A. Servedio"
    ],
    "summary": "We propose and analyze a new vantage point for the learning of mixtures of\nGaussians: namely, the PAC-style model of learning probability distributions\nintroduced by Kearns et al. Here the task is to construct a hypothesis mixture\nof Gaussians that is statistically indistinguishable from the actual mixture\ngenerating the data; specifically, the KL-divergence should be at most epsilon.\n  In this scenario, we give a poly(n/epsilon)-time algorithm that learns the\nclass of mixtures of any constant number of axis-aligned Gaussians in\nn-dimensional Euclidean space. Our algorithm makes no assumptions about the\nseparation between the means of the Gaussians, nor does it have any dependence\non the minimum mixing weight. This is in contrast to learning results known in\nthe ``clustering'' model, where such assumptions are unavoidable.\n  Our algorithm relies on the method of moments, and a subalgorithm developed\nin previous work by the authors (FOCS 2005) for a discrete mixture-learning\nproblem.",
    "published": "2006-09-16T14:43:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hedging predictions in machine learning",
    "authors": [
      "Alexander Gammerman",
      "Vladimir Vovk"
    ],
    "summary": "Recent advances in machine learning make it possible to design efficient\nprediction algorithms for data sets with huge numbers of parameters. This paper\ndescribes a new technique for \"hedging\" the predictions output by many such\nalgorithms, including support vector machines, kernel ridge regression, kernel\nnearest neighbours, and by many other state-of-the-art methods. The hedged\npredictions for the labels of new objects include quantitative measures of\ntheir own accuracy and reliability. These measures are provably valid under the\nassumption of randomness, traditional in machine learning: the objects and\ntheir labels are assumed to be generated independently from the same\nprobability distribution. In particular, it becomes possible to control (up to\nstatistical fluctuations) the number of erroneous predictions by selecting a\nsuitable confidence level. Validity being achieved automatically, the remaining\ngoal of hedged prediction is efficiency: taking full account of the new\nobjects' features and other available information to produce as accurate\npredictions as possible. This can be done successfully using the powerful\nmachinery of modern machine learning.",
    "published": "2006-11-02T18:44:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Unified View of TD Algorithms; Introducing Full-Gradient TD and\n  Equi-Gradient Descent TD",
    "authors": [
      "Manuel Loth",
      "Philippe Preux"
    ],
    "summary": "This paper addresses the issue of policy evaluation in Markov Decision\nProcesses, using linear function approximation. It provides a unified view of\nalgorithms such as TD(lambda), LSTD(lambda), iLSTD, residual-gradient TD. It is\nasserted that they all consist in minimizing a gradient function and differ by\nthe form of this function and their means of minimizing it. Two new schemes are\nintroduced in that framework: Full-gradient TD which uses a generalization of\nthe principle introduced in iLSTD, and EGD TD, which reduces the gradient by\nsuccessive equi-gradient descents. These three algorithms form a new\nintermediate family with the interesting property of making much better use of\nthe samples than TD while keeping a gradient descent scheme, which is useful\nfor complexity issues and optimistic policy iteration.",
    "published": "2006-11-29T00:00:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bandit Algorithms for Tree Search",
    "authors": [
      "Pierre-Arnaud Coquelin",
      "Rémi Munos"
    ],
    "summary": "Bandit based methods for tree search have recently gained popularity when\napplied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT\nalgorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper\nConfidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to\nthe effective smoothness of the tree. However, we show that UCT is too\n``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the\ndepth of the tree. We propose alternative bandit algorithms for tree search.\nFirst, a modification of UCT using a confidence sequence that scales\nexponentially with the horizon depth is proven to have a regret O(2^D\n\\sqrt{n}), but does not adapt to possible smoothness in the tree. We then\nanalyze Flat-UCB performed on the leaves and provide a finite regret bound with\nhigh probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth\nTrees which takes into account actual smoothness of the rewards for performing\nefficient ``cuts'' of sub-optimal branches with high confidence. Finally, we\npresent an incremental tree search version which applies when the full tree is\ntoo big (possibly infinite) to be entirely represented and show that with high\nprobability, essentially only the optimal branches is indefinitely developed.\nWe illustrate these methods on a global optimization problem of a Lipschitz\nfunction, given noisy data.",
    "published": "2007-03-13T08:53:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Intrinsic dimension of a dataset: what properties does one expect?",
    "authors": [
      "Vladimir Pestov"
    ],
    "summary": "We propose an axiomatic approach to the concept of an intrinsic dimension of\na dataset, based on a viewpoint of geometry of high-dimensional structures. Our\nfirst axiom postulates that high values of dimension be indicative of the\npresence of the curse of dimensionality (in a certain precise mathematical\nsense). The second axiom requires the dimension to depend smoothly on a\ndistance between datasets (so that the dimension of a dataset and that of an\napproximating principal manifold would be close to each other). The third axiom\nis a normalization condition: the dimension of the Euclidean $n$-sphere $\\s^n$\nis $\\Theta(n)$. We give an example of a dimension function satisfying our\naxioms, even though it is in general computationally unfeasible, and discuss a\ncomputationally cheap function satisfying most but not all of our axioms (the\n``intrinsic dimensionality'' of Ch\\'avez et al.)",
    "published": "2007-03-25T01:19:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Parametric Learning and Monte Carlo Optimization",
    "authors": [
      "David H. Wolpert",
      "Dev G. Rajnarayan"
    ],
    "summary": "This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.",
    "published": "2007-04-10T17:01:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supervised Feature Selection via Dependence Estimation",
    "authors": [
      "Le Song",
      "Alex Smola",
      "Arthur Gretton",
      "Karsten Borgwardt",
      "Justin Bedo"
    ],
    "summary": "We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.",
    "published": "2007-04-20T08:26:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "HMM Speaker Identification Using Linear and Non-linear Merging\n  Techniques",
    "authors": [
      "Unathi Mahola",
      "Fulufhelo V. Nelwamondo",
      "Tshilidzi Marwala"
    ],
    "summary": "Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved",
    "published": "2007-05-11T04:54:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers\n  Taking Values in R^Q",
    "authors": [
      "Yann Guermeur"
    ],
    "summary": "Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \"VC dimensions\" exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.",
    "published": "2007-06-25T17:28:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Consistency of the group Lasso and multiple kernel learning",
    "authors": [
      "Francis Bach"
    ],
    "summary": "We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.",
    "published": "2007-07-23T14:35:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cost-minimising strategies for data labelling : optimal stopping and\n  active learning",
    "authors": [
      "Christos Dimitrakakis",
      "Christian Savu-Krohn"
    ],
    "summary": "Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.",
    "published": "2007-08-09T10:21:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Defensive forecasting for optimal prediction with expert advice",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \"second-guessing\" experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.",
    "published": "2007-08-10T19:19:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Continuous and randomized defensive forecasting: unified view",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \"continuous\", in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \"randomized\", in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.",
    "published": "2007-08-17T12:18:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Filtering Additive Measurement Noise with Maximum Entropy in the Mean",
    "authors": [
      "Henryk Gzyl",
      "Enrique ter Horst"
    ],
    "summary": "The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.",
    "published": "2007-09-04T19:36:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction with expert advice for the Brier game",
    "authors": [
      "Vladimir Vovk",
      "Fedor Zhdanov"
    ],
    "summary": "We show that the Brier game of prediction is mixable and find the optimal\nlearning rate and substitution function for it. The resulting prediction\nalgorithm is applied to predict results of football and tennis matches. The\ntheoretical performance guarantee turns out to be rather tight on these data\nsets, especially in the case of the more extensive tennis data.",
    "published": "2007-10-02T10:08:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Consistency of trace norm minimization",
    "authors": [
      "Francis Bach"
    ],
    "summary": "Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled.",
    "published": "2007-10-15T15:38:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering with Transitive Distance and K-Means Duality",
    "authors": [
      "Chunjing Xu",
      "Jianzhuang Liu",
      "Xiaoou Tang"
    ],
    "summary": "Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.",
    "published": "2007-11-22T15:05:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Covariance and PCA for Categorical Variables",
    "authors": [
      "Hirotaka Niitsuma",
      "Takashi Okada"
    ],
    "summary": "Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical",
    "published": "2007-11-28T12:05:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Relationship between the Posterior and Optimal Similarity",
    "authors": [
      "Thomas M. Breuel"
    ],
    "summary": "For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.",
    "published": "2007-12-02T09:38:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Equations of States in Singular Statistical Estimation",
    "authors": [
      "Sumio Watanabe"
    ],
    "summary": "Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.",
    "published": "2007-12-05T05:39:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Density estimation in linear time",
    "authors": [
      "Satyaki Mahalanabis",
      "Daniel Stefankovic"
    ],
    "summary": "We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.",
    "published": "2007-12-18T03:30:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph kernels between point clouds",
    "authors": [
      "Francis Bach"
    ],
    "summary": "Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.",
    "published": "2007-12-20T13:06:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online variants of the cross-entropy method",
    "authors": [
      "Istvan Szita",
      "Andras Lorincz"
    ],
    "summary": "The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.",
    "published": "2008-01-14T06:56:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The optimal assignment kernel is not positive definite",
    "authors": [
      "Jean-Philippe Vert"
    ],
    "summary": "We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.",
    "published": "2008-01-26T07:32:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "New Estimation Procedures for PLS Path Modelling",
    "authors": [
      "Xavier Bry"
    ],
    "summary": "Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \"external\" estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \"internal\" estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.",
    "published": "2008-02-07T15:18:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A New Approach to Collaborative Filtering: Operator Estimation with\n  Spectral Regularization",
    "authors": [
      "Jacob Abernethy",
      "Francis Bach",
      "Theodoros Evgeniou",
      "Jean-Philippe Vert"
    ],
    "summary": "We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \"users\" to the \"objects\" they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.",
    "published": "2008-02-11T12:55:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiple Random Oracles Are Better Than One",
    "authors": [
      "Jan Arpe",
      "Elchanan Mossel"
    ],
    "summary": "We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.",
    "published": "2008-04-23T23:18:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Introduction to Relational Networks for Classification",
    "authors": [
      "Vukosi Marivate",
      "Tshilidzi Marwala"
    ],
    "summary": "The use of computational intelligence techniques for classification has been\nused in numerous applications. This paper compares the use of a Multi Layer\nPerceptron Neural Network and a new Relational Network on classifying the HIV\nstatus of women at ante-natal clinics. The paper discusses the architecture of\nthe relational network and its merits compared to a neural network and most\nother computational intelligence classifiers. Results gathered from the study\nindicate comparable classification accuracies as well as revealed relationships\nbetween data features in the classification data. Much higher classification\naccuracies are recommended for future research in the area of HIV\nclassification as well as missing data estimation.",
    "published": "2008-04-29T19:25:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Effect of Structural Diversity of an Ensemble of Classifiers on\n  Classification Accuracy",
    "authors": [
      "Lesedi Masisi",
      "Fulufhelo V. Nelwamondo",
      "Tshilidzi Marwala"
    ],
    "summary": "This paper aims to showcase the measure of structural diversity of an\nensemble of 9 classifiers and then map a relationship between this structural\ndiversity and accuracy. The structural diversity was induced by having\ndifferent architectures or structures of the classifiers The Genetical\nAlgorithms (GA) were used to derive the relationship between diversity and the\nclassification accuracy by evolving the classifiers and then picking 9\nclassifiers out on an ensemble of 60 classifiers. It was found that as the\nensemble became diverse the accuracy improved. However at a certain diversity\nmeasure the accuracy began to drop. The Kohavi-Wolpert variance method is used\nto measure the diversity of the ensemble. A method of voting is used to\naggregate the results from each classifier. The lowest error was observed at a\ndiversity measure of 0.16 with a mean square error of 0.274, when taking 0.2024\nas maximum diversity measured. The parameters that were varied were: the number\nof hidden nodes, learning rate and the activation function.",
    "published": "2008-04-30T06:07:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Quadratic Loss Multi-Class SVM",
    "authors": [
      "Emmanuel Monfrini",
      "Yann Guermeur"
    ],
    "summary": "Using a support vector machine requires to set two types of hyperparameters:\nthe soft margin parameter C and the parameters of the kernel. To perform this\nmodel selection task, the method of choice is cross-validation. Its\nleave-one-out variant is known to produce an estimator of the generalization\nerror which is almost unbiased. Its major drawback rests in its time\nrequirement. To overcome this difficulty, several upper bounds on the\nleave-one-out error of the pattern recognition SVM have been derived. Among\nthose bounds, the most popular one is probably the radius-margin bound. It\napplies to the hard margin pattern recognition SVM, and by extension to the\n2-norm SVM. In this report, we introduce a quadratic loss M-SVM, the M-SVM^2,\nas a direct extension of the 2-norm SVM to the multi-class case. For this\nmachine, a generalized radius-margin bound is then established.",
    "published": "2008-04-30T19:59:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Recovery of Sparse Signals via $\\ell_1$ Minimization",
    "authors": [
      "T. Tony Cai",
      "Guangwu Xu",
      "Jun Zhang"
    ],
    "summary": "This article considers constrained $\\ell_1$ minimization methods for the\nrecovery of high dimensional sparse signals in three settings: noiseless,\nbounded error and Gaussian noise. A unified and elementary treatment is given\nin these noise settings for two $\\ell_1$ minimization methods: the Dantzig\nselector and $\\ell_1$ minimization with an $\\ell_2$ constraint. The results of\nthis paper improve the existing results in the literature by weakening the\nconditions and tightening the error bounds. The improvement on the conditions\nshows that signals with larger support can be recovered accurately. This paper\nalso establishes connections between restricted isometry property and the\nmutual incoherence property. Some results of Candes, Romberg and Tao (2006) and\nDonoho, Elad, and Temlyakov (2006) are extended.",
    "published": "2008-05-01T20:25:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Margitron: A Generalised Perceptron with Margin",
    "authors": [
      "Constantinos Panagiotakopoulos",
      "Petroula Tsampouka"
    ],
    "summary": "We identify the classical Perceptron algorithm with margin as a member of a\nbroader family of large margin classifiers which we collectively call the\nMargitron. The Margitron, (despite its) sharing the same update rule with the\nPerceptron, is shown in an incremental setting to converge in a finite number\nof updates to solutions possessing any desirable fraction of the maximum\nmargin. Experiments comparing the Margitron with decomposition SVMs on tasks\ninvolving linear kernels and 2-norm soft margin are also reported.",
    "published": "2008-05-18T20:07:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sample Selection Bias Correction Theory",
    "authors": [
      "Corinna Cortes",
      "Mehryar Mohri",
      "Michael Riley",
      "Afshin Rostamizadeh"
    ],
    "summary": "This paper presents a theoretical analysis of sample selection bias\ncorrection. The sample bias correction technique commonly used in machine\nlearning consists of reweighting the cost of an error on each training point of\na biased sample to more closely reflect the unbiased distribution. This relies\non weights derived by various estimation techniques based on finite samples. We\nanalyze the effect of an error in that estimation on the accuracy of the\nhypothesis returned by the learning algorithm for two estimation techniques: a\ncluster-based estimation technique and kernel mean matching. We also report the\nresults of sample bias correction experiments with several data sets using\nthese techniques. Our analysis is based on the novel concept of distributional\nstability which generalizes the existing concept of point-based stability. Much\nof our work and proof techniques can be used to analyze other importance\nweighting techniques and their effect on accuracy when using a distributionally\nstable algorithm.",
    "published": "2008-05-19T02:55:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "From Data Topology to a Modular Classifier",
    "authors": [
      "Abdel Ennaji",
      "Arnaud Ribert",
      "Yves Lecourtier"
    ],
    "summary": "This article describes an approach to designing a distributed and modular\nneural classifier. This approach introduces a new hierarchical clustering that\nenables one to determine reliable regions in the representation space by\nexploiting supervised information. A multilayer perceptron is then associated\nwith each of these detected clusters and charged with recognizing elements of\nthe associated cluster while rejecting all others. The obtained global\nclassifier is comprised of a set of cooperating neural networks and completed\nby a K-nearest neighbor classifier charged with treating elements rejected by\nall the neural networks. Experimental results for the handwritten digit\nrecognition problem and comparison with neural and statistical nonmodular\nclassifiers are given.",
    "published": "2008-05-28T09:16:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Utilisation des grammaires probabilistes dans les tâches de\n  segmentation et d'annotation prosodique",
    "authors": [
      "Irina Nesterenko",
      "Stéphane Rauzy"
    ],
    "summary": "Nous pr\\'esentons dans cette contribution une approche \\`a la fois symbolique\net probabiliste permettant d'extraire l'information sur la segmentation du\nsignal de parole \\`a partir d'information prosodique. Nous utilisons pour ce\nfaire des grammaires probabilistes poss\\'edant une structure hi\\'erarchique\nminimale. La phase de construction des grammaires ainsi que leur pouvoir de\npr\\'ediction sont \\'evalu\\'es qualitativement ainsi que quantitativement.\n  -----\n  Methodologically oriented, the present work sketches an approach for prosodic\ninformation retrieval and speech segmentation, based on both symbolic and\nprobabilistic information. We have recourse to probabilistic grammars, within\nwhich we implement a minimal hierarchical structure. Both the stages of\nprobabilistic grammar building and its testing in prediction are explored and\nquantitatively and qualitatively evaluated.",
    "published": "2008-06-06T13:33:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Statistical Learning of Arbitrary Computable Classifiers",
    "authors": [
      "David Soloveichik"
    ],
    "summary": "Statistical learning theory chiefly studies restricted hypothesis classes,\nparticularly those with finite Vapnik-Chervonenkis (VC) dimension. The\nfundamental quantity of interest is the sample complexity: the number of\nsamples required to learn to a specified level of accuracy. Here we consider\nlearning over the set of all computable labeling functions. Since the\nVC-dimension is infinite and a priori (uniform) bounds on the number of samples\nare impossible, we let the learning algorithm decide when it has seen\nsufficient samples to have learned. We first show that learning in this setting\nis indeed possible, and develop a learning algorithm. We then show, however,\nthat bounding sample complexity independently of the distribution is\nimpossible. Notably, this impossibility is entirely due to the requirement that\nthe learning algorithm be computable, and not due to the statistical nature of\nthe problem.",
    "published": "2008-06-22T01:28:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Agnostically Learning Juntas from Random Walks",
    "authors": [
      "Jan Arpe",
      "Elchanan Mossel"
    ],
    "summary": "We prove that the class of functions g:{-1,+1}^n -> {-1,+1} that only depend\non an unknown subset of k<<n variables (so-called k-juntas) is agnostically\nlearnable from a random walk in time polynomial in n, 2^{k^2}, epsilon^{-k},\nand log(1/delta). In other words, there is an algorithm with the claimed\nrunning time that, given epsilon, delta > 0 and access to a random walk on\n{-1,+1}^n labeled by an arbitrary function f:{-1,+1}^n -> {-1,+1}, finds with\nprobability at least 1-delta a k-junta that is (opt(f)+epsilon)-close to f,\nwhere opt(f) denotes the distance of a closest k-junta to f.",
    "published": "2008-06-25T23:18:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Computationally Efficient Estimators for Dimension Reductions Using\n  Stable Random Projections",
    "authors": [
      "Ping Li"
    ],
    "summary": "The method of stable random projections is a tool for efficiently computing\nthe $l_\\alpha$ distances using low memory, where $0<\\alpha \\leq 2$ is a tuning\nparameter. The method boils down to a statistical estimation task and various\nestimators have been proposed, based on the geometric mean, the harmonic mean,\nand the fractional power etc.\n  This study proposes the optimal quantile estimator, whose main operation is\nselecting, which is considerably less expensive than taking fractional power,\nthe main operation in previous estimators. Our experiments report that the\noptimal quantile estimator is nearly one order of magnitude more\ncomputationally efficient than previous estimators. For large-scale learning\ntasks in which storing and computing pairwise distances is a serious\nbottleneck, this estimator should be desirable.\n  In addition to its computational advantages, the optimal quantile estimator\nexhibits nice theoretical properties. It is more accurate than previous\nestimators when $\\alpha>1$. We derive its theoretical error bounds and\nestablish the explicit (i.e., no hidden constants) sample complexity bound.",
    "published": "2008-06-27T05:19:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Approximating the Lp Distances for p>2",
    "authors": [
      "Ping Li"
    ],
    "summary": "Applications in machine learning and data mining require computing pairwise\nLp distances in a data matrix A. For massive high-dimensional data, computing\nall pairwise distances of A can be infeasible. In fact, even storing A or all\npairwise distances of A in the memory may be also infeasible. This paper\nproposes a simple method for p = 2, 4, 6, ... We first decompose the l_p (where\np is even) distances into a sum of 2 marginal norms and p-1 ``inner products''\nat different orders. Then we apply normal or sub-Gaussian random projections to\napproximate the resultant ``inner products,'' assuming that the marginal norms\ncan be computed exactly by a linear scan. We propose two strategies for\napplying random projections. The basic projection strategy requires only one\nprojection matrix but it is more difficult to analyze, while the alternative\nprojection strategy requires p-1 projection matrices but its theoretical\nanalysis is much easier. In terms of the accuracy, at least for p=4, the basic\nstrategy is always more accurate than the alternative strategy if the data are\nnon-negative, which is common in reality.",
    "published": "2008-06-27T05:36:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph Kernels",
    "authors": [
      "S. V. N. Vishwanathan",
      "Karsten M. Borgwardt",
      "Imre Risi Kondor",
      "Nicol N. Schraudolph"
    ],
    "summary": "We present a unified framework to study graph kernels, special cases of which\ninclude the random walk graph kernel \\citep{GaeFlaWro03,BorOngSchVisetal05},\nmarginalized graph kernel \\citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},\nand geometric kernel on graphs \\citep{Gaertner02}. Through extensions of linear\nalgebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a\nSylvester equation, we construct an algorithm that improves the time complexity\nof kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,\nconjugate gradient solvers or fixed-point iterations bring our algorithm into\nthe sub-cubic domain. Experiments on graphs from bioinformatics and other\napplication domains show that it is often more than a thousand times faster\nthan previous approaches. We then explore connections between diffusion kernels\n\\citep{KonLaf02}, regularization on graphs \\citep{SmoKon03}, and graph kernels,\nand use these connections to propose new graph kernels. Finally, we show that\nrational kernels \\citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized\nto graphs reduce to the random walk graph kernel.",
    "published": "2008-07-01T09:46:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Probability Distributions for Trees: Representations, Inference and\n  Learning",
    "authors": [
      "François Denis",
      "Amaury Habrard",
      "Rémi Gilleron",
      "Marc Tommasi",
      "Édouard Gilbert"
    ],
    "summary": "We study probability distributions over free algebras of trees. Probability\ndistributions can be seen as particular (formal power) tree series [Berstel et\nal 82, Esik et al 03], i.e. mappings from trees to a semiring K . A widely\nstudied class of tree series is the class of rational (or recognizable) tree\nseries which can be defined either in an algebraic way or by means of\nmultiplicity tree automata. We argue that the algebraic representation is very\nconvenient to model probability distributions over a free algebra of trees.\nFirst, as in the string case, the algebraic representation allows to design\nlearning algorithms for the whole class of probability distributions defined by\nrational tree series. Note that learning algorithms for rational tree series\ncorrespond to learning algorithms for weighted tree automata where both the\nstructure and the weights are learned. Second, the algebraic representation can\nbe easily extended to deal with unranked trees (like XML trees where a symbol\nmay have an unbounded number of children). Both properties are particularly\nrelevant for applications: nondeterministic automata are required for the\ninference problem to be relevant (recall that Hidden Markov Models are\nequivalent to nondeterministic string automata); nowadays applications for Web\nInformation Extraction, Web Services and document processing consider unranked\ntrees.",
    "published": "2008-07-18T14:41:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Positive factor networks: A graphical framework for modeling\n  non-negative sequential data",
    "authors": [
      "Brian K. Vogel"
    ],
    "summary": "We present a novel graphical framework for modeling non-negative sequential\ndata with hierarchical structure. Our model corresponds to a network of coupled\nnon-negative matrix factorization (NMF) modules, which we refer to as a\npositive factor network (PFN). The data model is linear, subject to\nnon-negativity constraints, so that observation data consisting of an additive\ncombination of individually representable observations is also representable by\nthe network. This is a desirable property for modeling problems in\ncomputational auditory scene analysis, since distinct sound sources in the\nenvironment are often well-modeled as combining additively in the corresponding\nmagnitude spectrogram. We propose inference and learning algorithms that\nleverage existing NMF algorithms and that are straightforward to implement. We\npresent a target tracking example and provide results for synthetic observation\ndata which serve to illustrate the interesting properties of PFNs and motivate\ntheir potential usefulness in applications such as music transcription, source\nseparation, and speech recognition. We show how a target process characterized\nby a hierarchical state transition model can be represented as a PFN. Our\nresults illustrate that a PFN which is defined in terms of a single target\nobservation can then be used to effectively track the states of multiple\nsimultaneous targets. Our results show that the quality of the inferred target\nstates degrades gradually as the observation noise is increased. We also\npresent results for an example in which meaningful hierarchical features are\nextracted from a spectrogram. Such a hierarchical representation could be\nuseful for music transcription and source separation applications. We also\npropose a network for language modeling.",
    "published": "2008-07-25T22:50:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "When is there a representer theorem? Vector versus matrix regularizers",
    "authors": [
      "Andreas Argyriou",
      "Charles Micchelli",
      "Massimiliano Pontil"
    ],
    "summary": "We consider a general class of regularization methods which learn a vector of\nparameters on the basis of linear measurements. It is well known that if the\nregularizer is a nondecreasing function of the inner product then the learned\nvector is a linear combination of the input data. This result, known as the\n{\\em representer theorem}, is at the basis of kernel-based methods in machine\nlearning. In this paper, we prove the necessity of the above condition, thereby\ncompleting the characterization of kernel methods based on regularization. We\nfurther extend our analysis to regularization methods which learn a matrix, a\nproblem which is motivated by the application to multi-task learning. In this\ncontext, we study a more general representer theorem, which holds for a larger\nclass of regularizers. We provide a necessary and sufficient condition for\nthese class of matrix regularizers and highlight them with some concrete\nexamples of practical importance. Our analysis uses basic principles from\nmatrix theory, especially the useful notion of matrix nondecreasing function.",
    "published": "2008-09-09T16:11:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustered Multi-Task Learning: A Convex Formulation",
    "authors": [
      "Laurent Jacob",
      "Francis Bach",
      "Jean-Philippe Vert"
    ],
    "summary": "In multi-task learning several related tasks are considered simultaneously,\nwith the hope that by an appropriate sharing of information across tasks, each\ntask may benefit from the others. In the context of learning linear functions\nfor supervised classification or regression, this can be achieved by including\na priori information about the weight vectors associated with the tasks, and\nhow they are expected to be related to each other. In this paper, we assume\nthat tasks are clustered into groups, which are unknown beforehand, and that\ntasks within a group have similar weight vectors. We design a new spectral norm\nthat encodes this a priori assumption, without the prior knowledge of the\npartition of tasks into groups, resulting in a new convex optimization\nformulation for multi-task learning. We show in simulations on synthetic\nexamples and on the IEDB MHC-I binding dataset, that our approach outperforms\nwell-known convex methods for multi-task learning, as well as related non\nconvex methods dedicated to the same problem.",
    "published": "2008-09-11T19:01:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Surrogate Learning - An Approach for Semi-Supervised Classification",
    "authors": [
      "Sriharsha Veeramachaneni",
      "Ravikumar Kondadadi"
    ],
    "summary": "We consider the task of learning a classifier from the feature space\n$\\mathcal{X}$ to the set of classes $\\mathcal{Y} = \\{0, 1\\}$, when the features\ncan be partitioned into class-conditionally independent feature sets\n$\\mathcal{X}_1$ and $\\mathcal{X}_2$. We show the surprising fact that the\nclass-conditional independence can be used to represent the original learning\ntask in terms of 1) learning a classifier from $\\mathcal{X}_2$ to\n$\\mathcal{X}_1$ and 2) learning the class-conditional distribution of the\nfeature set $\\mathcal{X}_1$. This fact can be exploited for semi-supervised\nlearning because the former task can be accomplished purely from unlabeled\nsamples. We present experimental evaluation of the idea in two real world\napplications.",
    "published": "2008-09-26T13:47:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Isometric Separation Maps",
    "authors": [
      "Nikolaos Vasiloglou",
      "Alexander G. Gray",
      "David V. Anderson"
    ],
    "summary": "Maximum Variance Unfolding (MVU) and its variants have been very successful\nin embedding data-manifolds in lower dimensional spaces, often revealing the\ntrue intrinsic dimension. In this paper we show how to also incorporate\nsupervised class information into an MVU-like method without breaking its\nconvexity. We call this method the Isometric Separation Map and we show that\nthe resulting kernel matrix can be used as a binary/multiclass Support Vector\nMachine-like method in a semi-supervised (transductive) framework. We also show\nthat the method always finds a kernel matrix that linearly separates the\ntraining data exactly without projecting them in infinite dimensional spaces.\nIn traditional SVMs we choose a kernel and hope that the data become linearly\nseparable in the kernel space. In this paper we show how the hyperplane can be\nchosen ad-hoc and the kernel is trained so that data are always linearly\nseparable. Comparisons with Large Margin SVMs show comparable performance.",
    "published": "2008-10-25T15:09:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Entropy, Perception, and Relativity",
    "authors": [
      "Stefan Jaeger"
    ],
    "summary": "In this paper, I expand Shannon's definition of entropy into a new form of\nentropy that allows integration of information from different random events.\nShannon's notion of entropy is a special case of my more general definition of\nentropy. I define probability using a so-called performance function, which is\nde facto an exponential distribution. Assuming that my general notion of\nentropy reflects the true uncertainty about a probabilistic event, I understand\nthat our perceived uncertainty differs. I claim that our perception is the\nresult of two opposing forces similar to the two famous antagonists in Chinese\nphilosophy: Yin and Yang. Based on this idea, I show that our perceived\nuncertainty matches the true uncertainty in points determined by the golden\nratio. I demonstrate that the well-known sigmoid function, which we typically\nemploy in artificial neural networks as a non-linear threshold function,\ndescribes the actual performance. Furthermore, I provide a motivation for the\ntime dilation in Einstein's Special Relativity, basically claiming that\nalthough time dilation conforms with our perception, it does not correspond to\nreality. At the end of the paper, I show how to apply this theoretical\nframework to practical applications. I present recognition rates for a pattern\nrecognition problem, and also propose a network architecture that can take\nadvantage of general entropy to solve complex decision problems.",
    "published": "2008-11-02T08:02:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stability Bound for Stationary Phi-mixing and Beta-mixing Processes",
    "authors": [
      "Mehryar Mohri",
      "Afshin Rostamizadeh"
    ],
    "summary": "Most generalization bounds in learning theory are based on some measure of\nthe complexity of the hypothesis class used, independently of any algorithm. In\ncontrast, the notion of algorithmic stability can be used to derive tight\ngeneralization bounds that are tailored to specific learning algorithms by\nexploiting their particular properties. However, as in much of learning theory,\nexisting stability analyses and bounds apply only in the scenario where the\nsamples are independently and identically distributed. In many machine learning\napplications, however, this assumption does not hold. The observations received\nby the learning algorithm often have some inherent temporal dependence.\n  This paper studies the scenario where the observations are drawn from a\nstationary phi-mixing or beta-mixing sequence, a widely adopted assumption in\nthe study of non-i.i.d. processes that implies a dependence between\nobservations weakening over time. We prove novel and distinct stability-based\ngeneralization bounds for stationary phi-mixing and beta-mixing sequences.\nThese bounds strictly generalize the bounds given in the i.i.d. case and apply\nto all stable learning algorithms, thereby extending the use of\nstability-bounds to non-i.i.d. scenarios.\n  We also illustrate the application of our phi-mixing generalization bounds to\ngeneral classes of learning algorithms, including Support Vector Regression,\nKernel Ridge Regression, and Support Vector Machines, and many other kernel\nregularization-based and relative entropy-based regularization algorithms.\nThese novel bounds can thus be viewed as the first theoretical basis for the\nuse of these algorithms in non-i.i.d. scenarios.",
    "published": "2008-11-11T05:09:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Land Cover Mapping Using Ensemble Feature Selection Methods",
    "authors": [
      "A. Gidudu",
      "B. Abe",
      "T. Marwala"
    ],
    "summary": "Ensemble classification is an emerging approach to land cover mapping whereby\nthe final classification output is a result of a consensus of classifiers.\nIntuitively, an ensemble system should consist of base classifiers which are\ndiverse i.e. classifiers whose decision boundaries err differently. In this\npaper ensemble feature selection is used to impose diversity in ensembles. The\nfeatures of the constituent base classifiers for each ensemble were created\nthrough an exhaustive search algorithm using different separability indices.\nFor each ensemble, the classification accuracy was derived as well as a\ndiversity measure purported to give a measure of the inensemble diversity. The\ncorrelation between ensemble classification accuracy and diversity measure was\ndetermined to establish the interplay between the two variables. From the\nfindings of this paper, diversity measures as currently formulated do not\nprovide an adequate means upon which to constitute ensembles for land cover\nmapping.",
    "published": "2008-11-13T01:23:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Novel Clustering Algorithm Based on Quantum Random Walk",
    "authors": [
      "Qiang Li",
      "Yan He",
      "Jing-ping Jiang"
    ],
    "summary": "The enormous successes have been made by quantum algorithms during the last\ndecade. In this paper, we combine the quantum random walk (QRW) with the\nproblem of data clustering, and develop two clustering algorithms based on the\none dimensional QRW. Then, the probability distributions on the positions\ninduced by QRW in these algorithms are investigated, which also indicates the\npossibility of obtaining better results. Consequently, the experimental results\nhave demonstrated that data points in datasets are clustered reasonably and\nefficiently, and the clustering algorithms are of fast rates of convergence.\nMoreover, the comparison with other algorithms also provides an indication of\nthe effectiveness of the proposed approach.",
    "published": "2008-12-07T15:22:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Convex Sparse Matrix Factorizations",
    "authors": [
      "Francis Bach",
      "Julien Mairal",
      "Jean Ponce"
    ],
    "summary": "We present a convex formulation of dictionary learning for sparse signal\ndecomposition. Convexity is obtained by replacing the usual explicit upper\nbound on the dictionary size by a convex rank-reducing term similar to the\ntrace norm. In particular, our formulation introduces an explicit trade-off\nbetween size and sparsity of the decomposition of rectangular matrices. Using a\nlarge set of synthetic examples, we compare the estimation abilities of the\nconvex and non-convex approaches, showing that while the convex formulation has\na single local minimum, this may lead in some cases to performance which is\ninferior to the local minima of the non-convex formulation.",
    "published": "2008-12-10T09:00:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Binary Classification Based on Potentials",
    "authors": [
      "Erik Boczko",
      "Andrew DiLullo",
      "Todd Young"
    ],
    "summary": "We introduce a simple and computationally trivial method for binary\nclassification based on the evaluation of potential functions. We demonstrate\nthat despite the conceptual and computational simplicity of the method its\nperformance can match or exceed that of standard Support Vector Machine\nmethods.",
    "published": "2008-12-16T20:41:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Linearly Parameterized Bandits",
    "authors": [
      "Paat Rusmevichientong",
      "John N. Tsitsiklis"
    ],
    "summary": "We consider bandit problems involving a large (possibly infinite) collection\nof arms, in which the expected reward of each arm is a linear function of an\n$r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$.\nThe objective is to minimize the cumulative regret and Bayes risk. When the set\nof arms corresponds to the unit sphere, we prove that the regret and Bayes risk\nis of order $\\Theta(r \\sqrt{T})$, by establishing a lower bound for an\narbitrary policy, and showing that a matching upper bound is obtained through a\npolicy that alternates between exploration and exploitation phases. The\nphase-based policy is also shown to be effective if the set of arms satisfies a\nstrong convexity condition. For the case of a general set of arms, we describe\na near-optimal policy whose regret and Bayes risk admit upper bounds of the\nform $O(r \\sqrt{T} \\log^{3/2} T)$.",
    "published": "2008-12-18T07:59:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Importance Weighted Active Learning",
    "authors": [
      "Alina Beygelzimer",
      "Sanjoy Dasgupta",
      "John Langford"
    ],
    "summary": "We present a practical and statistically consistent scheme for actively\nlearning binary classifiers under general loss functions. Our algorithm uses\nimportance weighting to correct sampling bias, and by controlling the variance,\nwe are able to give rigorous label complexity bounds for the learning process.\nExperiments on passively labeled data show that this approach reduces the label\ncomplexity required to achieve good predictive performance on many learning\nproblems.",
    "published": "2008-12-29T18:29:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Distributed Preemption Decisions: Probabilistic Graphical Model,\n  Algorithm and Near-Optimality",
    "authors": [
      "Sung-eok Jeon",
      "Chuanyi Ji"
    ],
    "summary": "Cooperative decision making is a vision of future network management and\ncontrol. Distributed connection preemption is an important example where nodes\ncan make intelligent decisions on allocating resources and controlling traffic\nflows for multi-class service networks. A challenge is that nodal decisions are\nspatially dependent as traffic flows trespass multiple nodes in a network.\nHence the performance-complexity trade-off becomes important, i.e., how\naccurate decisions are versus how much information is exchanged among nodes.\nConnection preemption is known to be NP-complete. Centralized preemption is\noptimal but computationally intractable. Decentralized preemption is\ncomputationally efficient but may result in a poor performance. This work\ninvestigates distributed preemption where nodes decide whether and which flows\nto preempt using only local information exchange with neighbors. We develop,\nbased on the probabilistic graphical models, a near-optimal distributed\nalgorithm. The algorithm is used by each node to make collectively near-optimal\npreemption decisions. We study trade-offs between near-optimal performance and\ncomplexity that corresponds to the amount of information-exchange of the\ndistributed algorithm. The algorithm is validated by both analysis and\nsimulation.",
    "published": "2009-01-07T04:36:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Limit Theorem in Singular Regression Problem",
    "authors": [
      "Sumio Watanabe"
    ],
    "summary": "In statistical problems, a set of parameterized probability distributions is\nused to estimate the true probability distribution. If Fisher information\nmatrix at the true distribution is singular, then it has been left unknown what\nwe can estimate about the true distribution from random samples. In this paper,\nwe study a singular regression problem and prove a limit theorem which shows\nthe relation between the singular regression problem and two birational\ninvariants, a real log canonical threshold and a singular fluctuation. The\nobtained theorem has an important application to statistics, because it enables\nus to estimate the generalization error from the training error without any\nknowledge of the true probability distribution.",
    "published": "2009-01-16T01:00:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cross-situational and supervised learning in the emergence of\n  communication",
    "authors": [
      "José F. Fontanari",
      "Angelo Cangelosi"
    ],
    "summary": "Scenarios for the emergence or bootstrap of a lexicon involve the repeated\ninteraction between at least two agents who must reach a consensus on how to\nname N objects using H words. Here we consider minimal models of two types of\nlearning algorithms: cross-situational learning, in which the individuals\ndetermine the meaning of a word by looking for something in common across all\nobserved uses of that word, and supervised operant conditioning learning, in\nwhich there is strong feedback between individuals about the intended meaning\nof the words. Despite the stark differences between these learning schemes, we\nshow that they yield the same communication accuracy in the realistic limits of\nlarge N and H, which coincides with the result of the classical occupancy\nproblem of randomly assigning N objects to H words.",
    "published": "2009-01-26T15:12:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extraction de concepts sous contraintes dans des données d'expression\n  de gènes",
    "authors": [
      "Baptiste Jeudy",
      "François Rioult"
    ],
    "summary": "In this paper, we propose a technique to extract constrained formal concepts.",
    "published": "2009-02-07T18:01:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Database Transposition for Constrained (Closed) Pattern Mining",
    "authors": [
      "Baptiste Jeudy",
      "François Rioult"
    ],
    "summary": "Recently, different works proposed a new way to mine patterns in databases\nwith pathological size. For example, experiments in genome biology usually\nprovide databases with thousands of attributes (genes) but only tens of objects\n(experiments). In this case, mining the \"transposed\" database runs through a\nsmaller search space, and the Galois connection allows to infer the closed\npatterns of the original database. We focus here on constrained pattern mining\nfor those unusual databases and give a theoretical framework for database and\nconstraint transposition. We discuss the properties of constraint transposition\nand look into classical constraints. We then address the problem of generating\nthe closed patterns of the original database satisfying the constraint,\nstarting from those mined in the \"transposed\" database. Finally, we show how to\ngenerate all the patterns satisfying the constraint from the closed ones.",
    "published": "2009-02-07T18:01:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Label Prediction via Compressed Sensing",
    "authors": [
      "Daniel Hsu",
      "Sham M. Kakade",
      "John Langford",
      "Tong Zhang"
    ],
    "summary": "We consider multi-label prediction problems with large output spaces under\nthe assumption of output sparsity -- that the target (label) vectors have small\nsupport. We develop a general theory for a variant of the popular error\ncorrecting output code scheme, using ideas from compressed sensing for\nexploiting this sparsity. The method can be regarded as a simple reduction from\nmulti-label regression problems to binary regression problems. We show that the\nnumber of subproblems need only be logarithmic in the total number of possible\nlabels, making this approach radically more efficient than others. We also\nstate and prove robustness guarantees for this method in the form of regret\ntransform bounds (in general), and also provide a more detailed analysis for\nthe linear prediction setting.",
    "published": "2009-02-08T02:30:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning rules from multisource data for cardiac monitoring",
    "authors": [
      "Marie-Odile Cordier",
      "Elisa Fromont",
      "René Quiniou"
    ],
    "summary": "This paper formalises the concept of learning symbolic rules from multisource\ndata in a cardiac monitoring context. Our sources, electrocardiograms and\narterial blood pressure measures, describe cardiac behaviours from different\nviewpoints. To learn interpretable rules, we use an Inductive Logic Programming\n(ILP) method. We develop an original strategy to cope with the dimensionality\nissues caused by using this ILP technique on a rich multisource language. The\nresults show that our method greatly improves the feasibility and the\nefficiency of the process while staying accurate. They also confirm the\nbenefits of using multiple sources to improve the diagnosis of cardiac\narrhythmias.",
    "published": "2009-02-19T13:47:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Uniqueness of Low-Rank Matrix Completion by Rigidity Theory",
    "authors": [
      "Amit Singer",
      "Mihai Cucuringu"
    ],
    "summary": "The problem of completing a low-rank matrix from a subset of its entries is\noften encountered in the analysis of incomplete data sets exhibiting an\nunderlying factor model with applications in collaborative filtering, computer\nvision and control. Most recent work had been focused on constructing efficient\nalgorithms for exact or approximate recovery of the missing matrix entries and\nproving lower bounds for the number of known entries that guarantee a\nsuccessful recovery with high probability. A related problem from both the\nmathematical and algorithmic point of view is the distance geometry problem of\nrealizing points in a Euclidean space from a given subset of their pairwise\ndistances. Rigidity theory answers basic questions regarding the uniqueness of\nthe realization satisfying a given partial set of distances. We observe that\nbasic ideas and tools of rigidity theory can be adapted to determine uniqueness\nof low-rank matrix completion, where inner products play the role that\ndistances play in rigidity theory. This observation leads to an efficient\nrandomized algorithm for testing both local and global unique completion.\nCrucial to our analysis is a new matrix, which we call the completion matrix,\nthat serves as the analogue of the rigidity matrix.",
    "published": "2009-02-23T04:05:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction with expert evaluators' advice",
    "authors": [
      "Alexey Chernov",
      "Vladimir Vovk"
    ],
    "summary": "We introduce a new protocol for prediction with expert advice in which each\nexpert evaluates the learner's and his own performance using a loss function\nthat may change over time and may be different from the loss functions used by\nthe other experts. The learner's goal is to perform better or not much worse\nthan each expert, as evaluated by that expert, for all experts simultaneously.\nIf the loss functions used by the experts are all proper scoring rules and all\nmixable, we show that the defensive forecasting algorithm enjoys the same\nperformance guarantee as that attainable by the Aggregating Algorithm in the\nstandard setting and known to be optimal. This result is also applied to the\ncase of \"specialist\" (or \"sleeping\") experts. In this case, the defensive\nforecasting algorithm reduces to a simple modification of the Aggregating\nAlgorithm.",
    "published": "2009-02-24T11:47:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiplicative updates For Non-Negative Kernel SVM",
    "authors": [
      "Vamsi K. Potluru",
      "Sergey M. Plis",
      "Morten Morup",
      "Vince D. Calhoun",
      "Terran Lane"
    ],
    "summary": "We present multiplicative updates for solving hard and soft margin support\nvector machines (SVM) with non-negative kernels. They follow as a natural\nextension of the updates for non-negative matrix factorization. No additional\nparam- eter setting, such as choosing learning, rate is required. Ex- periments\ndemonstrate rapid convergence to good classifiers. We analyze the rates of\nasymptotic convergence of the up- dates and establish tight bounds. We test the\nperformance on several datasets using various non-negative kernels and report\nequivalent generalization errors to that of a standard SVM.",
    "published": "2009-02-24T20:38:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Human Computation",
    "authors": [
      "Ran Gilad-Bachrach",
      "Aharon Bar-Hillel",
      "Liat Ein-Dor"
    ],
    "summary": "Collecting large labeled data sets is a laborious and expensive task, whose\nscaling up requires division of the labeling workload between many teachers.\nWhen the number of classes is large, miscorrespondences between the labels\ngiven by the different teachers are likely to occur, which, in the extreme\ncase, may reach total inconsistency. In this paper we describe how globally\nconsistent labels can be obtained, despite the absence of teacher coordination,\nand discuss the possible efficiency of this process in terms of human labor. We\ndefine a notion of label efficiency, measuring the ratio between the number of\nglobally consistent labels obtained and the number of labels provided by\ndistributed teachers. We show that the efficiency depends critically on the\nratio alpha between the number of data instances seen by a single teacher, and\nthe number of classes. We suggest several algorithms for the distributed\nlabeling problem, and analyze their efficiency as a function of alpha. In\naddition, we provide an upper bound on label efficiency for the case of\ncompletely uncoordinated teachers, and show that efficiency approaches 0 as the\nratio between the number of labels each teacher provides and the number of\nclasses drops (i.e. alpha goes to 0).",
    "published": "2009-03-05T22:39:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Differential Contrastive Divergence",
    "authors": [
      "David McAllester"
    ],
    "summary": "This paper has been retracted.",
    "published": "2009-03-13T13:47:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On $p$-adic Classification",
    "authors": [
      "Patrick Erik Bradley"
    ],
    "summary": "A $p$-adic modification of the split-LBG classification method is presented\nin which first clusterings and then cluster centers are computed which locally\nminimise an energy function. The outcome for a fixed dataset is independent of\nthe prime number $p$ with finitely many exceptions. The methods are applied to\nthe construction of $p$-adic classifiers in the context of learning.",
    "published": "2009-03-16T22:52:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stability Analysis and Learning Bounds for Transductive Regression\n  Algorithms",
    "authors": [
      "Corinna Cortes",
      "Mehryar Mohri",
      "Dmitry Pechyony",
      "Ashish Rastogi"
    ],
    "summary": "This paper uses the notion of algorithmic stability to derive novel\ngeneralization bounds for several families of transductive regression\nalgorithms, both by using convexity and closed-form solutions. Our analysis\nhelps compare the stability of these algorithms. It also shows that a number of\nwidely used transductive regression algorithms are in fact unstable. Finally,\nit reports the results of experiments with local transductive regression\ndemonstrating the benefit of our stability bounds for model selection, for one\nof the algorithms, in particular for determining the radius of the local\nneighborhood used by the algorithm.",
    "published": "2009-04-05T20:08:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Inferring Dynamic Bayesian Networks using Frequent Episode Mining",
    "authors": [
      "Debprakash Patnaik",
      "Srivatsan Laxman",
      "Naren Ramakrishnan"
    ],
    "summary": "Motivation: Several different threads of research have been proposed for\nmodeling and mining temporal data. On the one hand, approaches such as dynamic\nBayesian networks (DBNs) provide a formal probabilistic basis to model\nrelationships between time-indexed random variables but these models are\nintractable to learn in the general case. On the other, algorithms such as\nfrequent episode mining are scalable to large datasets but do not exhibit the\nrigorous probabilistic interpretations that are the mainstay of the graphical\nmodels literature.\n  Results: We present a unification of these two seemingly diverse threads of\nresearch, by demonstrating how dynamic (discrete) Bayesian networks can be\ninferred from the results of frequent episode mining. This helps bridge the\nmodeling emphasis of the former with the counting emphasis of the latter.\nFirst, we show how, under reasonable assumptions on data characteristics and on\ninfluences of random variables, the optimal DBN structure can be computed using\na greedy, local, algorithm. Next, we connect the optimality of the DBN\nstructure with the notion of fixed-delay episodes and their counts of distinct\noccurrences. Finally, to demonstrate the practical feasibility of our approach,\nwe focus on a specific (but broadly applicable) class of networks, called\nexcitatory networks, and show how the search for the optimal DBN structure can\nbe conducted using just information from frequent episodes. Application on\ndatasets gathered from mathematical models of spiking neurons as well as real\nneuroscience datasets are presented.\n  Availability: Algorithmic implementations, simulator codebases, and datasets\nare available from our website at http://neural-code.cs.vt.edu/dbn",
    "published": "2009-04-14T17:32:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "authors": [
      "Amnon Shashua"
    ],
    "summary": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "published": "2009-04-23T11:40:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Limits of Learning about a Categorical Latent Variable under Prior\n  Near-Ignorance",
    "authors": [
      "Alberto Piatti",
      "Marco Zaffalon",
      "Fabio Trojani",
      "Marcus Hutter"
    ],
    "summary": "In this paper, we consider the coherent theory of (epistemic) uncertainty of\nWalley, in which beliefs are represented through sets of probability\ndistributions, and we focus on the problem of modeling prior ignorance about a\ncategorical random variable. In this setting, it is a known result that a state\nof prior ignorance is not compatible with learning. To overcome this problem,\nanother state of beliefs, called \\emph{near-ignorance}, has been proposed.\nNear-ignorance resembles ignorance very closely, by satisfying some principles\nthat can arguably be regarded as necessary in a state of ignorance, and allows\nlearning to take place. What this paper does, is to provide new and substantial\nevidence that also near-ignorance cannot be really regarded as a way out of the\nproblem of starting statistical inference in conditions of very weak beliefs.\nThe key to this result is focusing on a setting characterized by a variable of\ninterest that is \\emph{latent}. We argue that such a setting is by far the most\ncommon case in practice, and we provide, for the case of categorical latent\nvariables (and general \\emph{manifest} variables) a condition that, if\nsatisfied, prevents learning to take place under prior near-ignorance. This\ncondition is shown to be easily satisfied even in the most common statistical\nproblems. We regard these results as a strong form of evidence against the\npossibility to adopt a condition of prior near-ignorance in real statistical\nproblems.",
    "published": "2009-04-29T03:16:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Temporal data mining for root-cause analysis of machine faults in\n  automotive assembly lines",
    "authors": [
      "Srivatsan Laxman",
      "Basel Shadid",
      "P. S. Sastry",
      "K. P. Unnikrishnan"
    ],
    "summary": "Engine assembly is a complex and heavily automated distributed-control\nprocess, with large amounts of faults data logged everyday. We describe an\napplication of temporal data mining for analyzing fault logs in an engine\nassembly plant. Frequent episode discovery framework is a model-free method\nthat can be used to deduce (temporal) correlations among events from the logs\nin an efficient manner. In addition to being theoretically elegant and\ncomputationally efficient, frequent episodes are also easy to interpret in the\nform actionable recommendations. Incorporation of domain-specific information\nis critical to successful application of the method for analyzing fault logs in\nthe manufacturing domain. We show how domain-specific knowledge can be\nincorporated using heuristic rules that act as pre-filters and post-filters to\nfrequent episode discovery. The system described here is currently being used\nin one of the engine assembly plants of General Motors and is planned for\nadaptation in other plants. To the best of our knowledge, this paper presents\nthe first real, large-scale application of temporal data mining in the\nmanufacturing domain. We believe that the ideas presented in this paper can\nhelp practitioners engineer tools for analysis in other similar or related\napplication domains as well.",
    "published": "2009-04-29T13:17:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Combining Supervised and Unsupervised Learning for GIS Classification",
    "authors": [
      "Juan-Manuel Torres-Moreno",
      "Laurent Bougrain",
      "Frdéric Alexandre"
    ],
    "summary": "This paper presents a new hybrid learning algorithm for unsupervised\nclassification tasks. We combined Fuzzy c-means learning algorithm and a\nsupervised version of Minimerror to develop a hybrid incremental strategy\nallowing unsupervised classifications. We applied this new approach to a\nreal-world database in order to know if the information contained in unlabeled\nfeatures of a Geographic Information System (GIS), allows to well classify it.\nFinally, we compared our results to a classical supervised classification\nobtained by a multilayer perceptron.",
    "published": "2009-05-14T14:59:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Average-Case Active Learning with Costs",
    "authors": [
      "Andrew Guillory",
      "Jeff Bilmes"
    ],
    "summary": "We analyze the expected cost of a greedy active learning algorithm. Our\nanalysis extends previous work to a more general setting in which different\nqueries have different costs. Moreover, queries may have more than two possible\nresponses and the distribution over hypotheses may be non uniform. Specific\napplications include active learning with label costs, active learning for\nmulticlass and partial label queries, and batch mode active learning. We also\ndiscuss an approximate version of interest when there are very many queries.",
    "published": "2009-05-18T23:21:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Transfer Learning Using Feature Selection",
    "authors": [
      "Paramveer S. Dhillon",
      "Dean Foster",
      "Lyle Ungar"
    ],
    "summary": "We present three related ways of using Transfer Learning to improve feature\nselection. The three methods address different problems, and hence share\ndifferent kinds of information between tasks or feature classes, but all three\nare based on the information theoretic Minimum Description Length (MDL)\nprinciple and share the same underlying Bayesian interpretation. The first\nmethod, MIC, applies when predictive models are to be built simultaneously for\nmultiple tasks (``simultaneous transfer'') that share the same set of features.\nMIC allows each feature to be added to none, some, or all of the task models\nand is most beneficial for selecting a small set of predictive features from a\nlarge pool of features, as is common in genomic and biological datasets. Our\nsecond method, TPC (Three Part Coding), uses a similar methodology for the case\nwhen the features can be divided into feature classes. Our third method,\nTransfer-TPC, addresses the ``sequential transfer'' problem in which the task\nto which we want to transfer knowledge may not be known in advance and may have\ndifferent amounts of data than the other tasks. Transfer-TPC is most beneficial\nwhen we want to transfer knowledge between tasks which have unequal amounts of\nlabeled data, for example the data for disambiguating the senses of different\nverbs. We demonstrate the effectiveness of these approaches with experimental\nresults on real world data pertaining to genomics and to Word Sense\nDisambiguation (WSD).",
    "published": "2009-05-25T14:29:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Equations of States in Statistical Learning for a Nonparametrizable and\n  Regular Case",
    "authors": [
      "Sumio Watanabe"
    ],
    "summary": "Many learning machines that have hierarchical structure or hidden variables\nare now being used in information science, artificial intelligence, and\nbioinformatics. However, several learning machines used in such fields are not\nregular but singular statistical models, hence their generalization performance\nis still left unknown. To overcome these problems, in the previous papers, we\nproved new equations in statistical learning, by which we can estimate the\nBayes generalization loss from the Bayes training loss and the functional\nvariance, on the condition that the true distribution is a singularity\ncontained in a learning machine. In this paper, we prove that the same\nequations hold even if a true distribution is not contained in a parametric\nmodel. Also we prove that, the proposed equations in a regular case are\nasymptotically equivalent to the Takeuchi information criterion. Therefore, the\nproposed equations are always applicable without any condition on the unknown\ntrue distribution.",
    "published": "2009-06-01T04:47:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An optimal linear separator for the Sonar Signals Classification task",
    "authors": [
      "Juan-Manuel Torres-Moreno",
      "Mirta B. Gordon"
    ],
    "summary": "The problem of classifying sonar signals from rocks and mines first studied\nby Gorman and Sejnowski has become a benchmark against which many learning\nalgorithms have been tested. We show that both the training set and the test\nset of this benchmark are linearly separable, although with different\nhyperplanes. Moreover, the complete set of learning and test patterns together,\nis also linearly separable. We give the weights that separate these sets, which\nmay be used to compare results found by other algorithms.",
    "published": "2009-06-02T11:52:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bayesian History Reconstruction of Complex Human Gene Clusters on a\n  Phylogeny",
    "authors": [
      "Tomáš Vinař",
      "Broňa Brejová",
      "Giltae Song",
      "Adam Siepel"
    ],
    "summary": "Clusters of genes that have evolved by repeated segmental duplication present\ndifficult challenges throughout genomic analysis, from sequence assembly to\nfunctional analysis. Improved understanding of these clusters is of utmost\nimportance, since they have been shown to be the source of evolutionary\ninnovation, and have been linked to multiple diseases, including HIV and a\nvariety of cancers. Previously, Zhang et al. (2008) developed an algorithm for\nreconstructing parsimonious evolutionary histories of such gene clusters, using\nonly human genomic sequence data. In this paper, we propose a probabilistic\nmodel for the evolution of gene clusters on a phylogeny, and an MCMC algorithm\nfor reconstruction of duplication histories from genomic sequences in multiple\nspecies. Several projects are underway to obtain high quality BAC-based\nassemblies of duplicated clusters in multiple species, and we anticipate that\nour method will be useful in analyzing these valuable new data sets.",
    "published": "2009-06-15T08:43:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bayesian two-sample tests",
    "authors": [
      "Karsten M. Borgwardt",
      "Zoubin Ghahramani"
    ],
    "summary": "In this paper, we present two classes of Bayesian approaches to the\ntwo-sample problem. Our first class of methods extends the Bayesian t-test to\ninclude all parametric models in the exponential family and their conjugate\npriors. Our second class of methods uses Dirichlet process mixtures (DPM) of\nsuch conjugate-exponential distributions as flexible nonparametric priors over\nthe unknown distributions.",
    "published": "2009-06-22T15:25:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Acquiring Knowledge for Evaluation of Teachers Performance in Higher\n  Education using a Questionnaire",
    "authors": [
      "Hafeez Ullah Amin",
      "Abdur Rashid Khan"
    ],
    "summary": "In this paper, we present the step by step knowledge acquisition process by\nchoosing a structured method through using a questionnaire as a knowledge\nacquisition tool. Here we want to depict the problem domain as, how to evaluate\nteachers performance in higher education through the use of expert system\ntechnology. The problem is how to acquire the specific knowledge for a selected\nproblem efficiently and effectively from human experts and encode it in the\nsuitable computer format. Acquiring knowledge from human experts in the process\nof expert systems development is one of the most common problems cited till\nyet. This questionnaire was sent to 87 domain experts within all public and\nprivate universities in Pakistani. Among them 25 domain experts sent their\nvaluable opinions. Most of the domain experts were highly qualified, well\nexperienced and highly responsible persons. The whole questionnaire was divided\ninto 15 main groups of factors, which were further divided into 99 individual\nquestions. These facts were analyzed further to give a final shape to the\nquestionnaire. This knowledge acquisition technique may be used as a learning\ntool for further research work.",
    "published": "2009-06-25T11:09:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unsupervised Search-based Structured Prediction",
    "authors": [
      "Hal Daumé III"
    ],
    "summary": "We describe an adaptation and application of a search-based structured\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\nis possible to reduce unsupervised learning to supervised learning and\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\nadditionally show a close connection between unsupervised Searn and expectation\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\nextension. The key idea that enables this is an application of the predict-self\nidea for unsupervised learning.",
    "published": "2009-06-28T17:47:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Random DFAs are Efficiently PAC Learnable",
    "authors": [
      "Leonid Aryeh Kontorovich"
    ],
    "summary": "This paper has been withdrawn due to an error found by Dana Angluin and Lev\nReyzin.",
    "published": "2009-07-02T17:54:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bayesian Multitask Learning with Latent Hierarchies",
    "authors": [
      "Hal Daumé III"
    ],
    "summary": "We learn multiple hypotheses for related tasks under a latent hierarchical\nrelationship between tasks. We exploit the intuition that for domain\nadaptation, we wish to share classifier structure, but for multitask learning,\nwe wish to share covariance structure. Our hierarchical model is seen to\nsubsume several previously proposed multitask learning models and performs well\non three distinct real-world data sets.",
    "published": "2009-07-04T18:35:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Bayesian Model for Supervised Clustering with the Dirichlet Process\n  Prior",
    "authors": [
      "Hal Daumé III",
      "Daniel Marcu"
    ],
    "summary": "We develop a Bayesian framework for tackling the supervised clustering\nproblem, the generic problem encountered in tasks such as reference matching,\ncoreference resolution, identity uncertainty and record linkage. Our clustering\nmodel is based on the Dirichlet process prior, which enables us to define\ndistributions over the countably infinite sets that naturally arise in this\nproblem. We add supervision to our model by positing the existence of a set of\nunobserved random variables (we call these \"reference types\") that are generic\nacross all clusters. Inference in our framework, which requires integrating\nover infinitely many parameters, is solved using Markov chain Monte Carlo\ntechniques. We present algorithms for both conjugate and non-conjugate priors.\nWe present a simple--but general--parameterization of our model based on a\nGaussian assumption. We evaluate this model on one artificial task and three\nreal-world tasks, comparing it against both unsupervised and state-of-the-art\nsupervised algorithms. Our results show that our model is able to outperform\nother models across a variety of tasks and performance metrics.",
    "published": "2009-07-04T22:32:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast search for Dirichlet process mixture models",
    "authors": [
      "Hal Daumé III"
    ],
    "summary": "Dirichlet process (DP) mixture models provide a flexible Bayesian framework\nfor density estimation. Unfortunately, their flexibility comes at a cost:\ninference in DP mixture models is computationally expensive, even when\nconjugate distributions are used. In the common case when one seeks only a\nmaximum a posteriori assignment of data points to clusters, we show that search\nalgorithms provide a practical alternative to expensive MCMC and variational\ntechniques. When a true posterior sample is desired, the solution found by\nsearch can serve as a good initializer for MCMC. Experimental results show that\nusing these techniques is it possible to apply DP mixture models to very large\ndata sets.",
    "published": "2009-07-10T13:23:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering for Improved Learning in Maze Traversal Problem",
    "authors": [
      "Eddie White"
    ],
    "summary": "The maze traversal problem (finding the shortest distance to the goal from\nany position in a maze) has been an interesting challenge in computational\nintelligence. Recent work has shown that the cellular simultaneous recurrent\nneural network (CSRN) can solve this problem for simple mazes. This thesis\nfocuses on exploiting relevant information about the maze to improve learning\nand decrease the training time for the CSRN to solve mazes. Appropriate\nvariables are identified to create useful clusters using relevant information.\nThe CSRN was next modified to allow for an additional external input. With this\nadditional input, several methods were tested and results show that clustering\nthe mazes improves the overall learning of the traversal problem for the CSRN.",
    "published": "2009-08-06T19:48:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Randomized Algorithms for Large scale SVMs",
    "authors": [
      "Vinay Jethava",
      "Krishnan Suresh",
      "Chiranjib Bhattacharyya",
      "Ramesh Hariharan"
    ],
    "summary": "We propose a randomized algorithm for training Support vector machines(SVMs)\non large datasets. By using ideas from Random projections we show that the\ncombinatorial dimension of SVMs is $O({log} n)$ with high probability. This\nestimate of combinatorial dimension is used to derive an iterative algorithm,\ncalled RandSVM, which at each step calls an existing solver to train SVMs on a\nrandomly chosen subset of size $O({log} n)$. The algorithm has probabilistic\nguarantees and is capable of training SVMs with Kernels for both classification\nand regression problems. Experiments done on synthetic and real life data sets\ndemonstrate that the algorithm scales up existing SVM learners, without loss of\naccuracy.",
    "published": "2009-09-19T23:40:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scalable Inference for Latent Dirichlet Allocation",
    "authors": [
      "James Petterson",
      "Tiberio Caetano"
    ],
    "summary": "We investigate the problem of learning a topic model - the well-known Latent\nDirichlet Allocation - in a distributed manner, using a cluster of C processors\nand dividing the corpus to be learned equally among them. We propose a simple\napproximated method that can be tuned, trading speed for accuracy according to\nthe task at hand. Our approach is asynchronous, and therefore suitable for\nclusters of heterogenous machines.",
    "published": "2009-09-25T05:23:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Post-Processing of Discovered Association Rules Using Ontologies",
    "authors": [
      "Claudia Marinica",
      "Fabrice Guillet",
      "Henri Briand"
    ],
    "summary": "In Data Mining, the usefulness of association rules is strongly limited by\nthe huge amount of delivered rules. In this paper we propose a new approach to\nprune and filter discovered rules. Using Domain Ontologies, we strengthen the\nintegration of user knowledge in the post-processing task. Furthermore, an\ninteractive and iterative framework is designed to assist the user along the\nanalyzing task. On the one hand, we represent user domain knowledge using a\nDomain Ontology over database. On the other hand, a novel technique is\nsuggested to prune and to filter discovered rules. The proposed framework was\napplied successfully over the client database provided by Nantes Habitat.",
    "published": "2009-10-02T08:40:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Variable sigma Gaussian processes: An expectation propagation\n  perspective",
    "authors": [
      "Yuan Qi",
      "Ahmed H. Abdel-Gawad",
      "Thomas P. Minka"
    ],
    "summary": "Gaussian processes (GPs) provide a probabilistic nonparametric representation\nof functions in regression, classification, and other problems. Unfortunately,\nexact learning with GPs is intractable for large datasets. A variety of\napproximate GP methods have been proposed that essentially map the large\ndataset into a small set of basis points. The most advanced of these, the\nvariable-sigma GP (VSGP) (Walder et al., 2008), allows each basis point to have\nits own length scale. However, VSGP was only derived for regression. We\ndescribe how VSGP can be applied to classification and other problems, by\nderiving it as an expectation propagation algorithm. In this view, sparse GP\napproximations correspond to a KL-projection of the true posterior onto a\ncompact exponential family of GPs. VSGP constitutes one such family, and we\nshow how to enlarge this family to get additional accuracy. In particular, we\nshow that endowing each basis point with its own full covariance matrix\nprovides a significant increase in approximation power.",
    "published": "2009-10-05T03:30:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Effectiveness and Limitations of Statistical Spam Filters",
    "authors": [
      "M. Tariq Banday",
      "Tariq R. Jan"
    ],
    "summary": "In this paper we discuss the techniques involved in the design of the famous\nstatistical spam filters that include Naive Bayes, Term Frequency-Inverse\nDocument Frequency, K-Nearest Neighbor, Support Vector Machine, and Bayes\nAdditive Regression Tree. We compare these techniques with each other in terms\nof accuracy, recall, precision, etc. Further, we discuss the effectiveness and\nlimitations of statistical filters in filtering out various types of spam from\nlegitimate e-mails.",
    "published": "2009-10-14T07:43:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Competing with Gaussian linear experts",
    "authors": [
      "Fedor Zhdanov",
      "Vladimir Vovk"
    ],
    "summary": "We study the problem of online regression. We prove a theoretical bound on\nthe square loss of Ridge Regression. We do not make any assumptions about input\nvectors or outcomes. We also show that Bayesian Ridge Regression can be thought\nof as an online algorithm competing with all the Gaussian linear experts.",
    "published": "2009-10-24T22:40:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Anomaly Detection with Score functions based on Nearest Neighbor Graphs",
    "authors": [
      "Manqi Zhao",
      "Venkatesh Saligrama"
    ],
    "summary": "We propose a novel non-parametric adaptive anomaly detection algorithm for\nhigh dimensional data based on score functions derived from nearest neighbor\ngraphs on $n$-point nominal data. Anomalies are declared whenever the score of\na test sample falls below $\\alpha$, which is supposed to be the desired false\nalarm level. The resulting anomaly detector is shown to be asymptotically\noptimal in that it is uniformly most powerful for the specified false alarm\nlevel, $\\alpha$, for the case when the anomaly density is a mixture of the\nnominal and a known density. Our algorithm is computationally efficient, being\nlinear in dimension and quadratic in data size. It does not require choosing\ncomplicated tuning parameters or function approximation classes and it can\nadapt to local structure such as local change in dimensionality. We demonstrate\nthe algorithm on both artificial and real data sets in high dimensional feature\nspaces.",
    "published": "2009-10-28T18:46:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Mirroring Theorem and its Application to a New Method of Unsupervised\n  Hierarchical Pattern Classification",
    "authors": [
      "Dasika Ratna Deepthi",
      "K. Eswaran"
    ],
    "summary": "In this paper, we prove a crucial theorem called Mirroring Theorem which\naffirms that given a collection of samples with enough information in it such\nthat it can be classified into classes and subclasses then (i) There exists a\nmapping which classifies and subclassifies these samples (ii) There exists a\nhierarchical classifier which can be constructed by using Mirroring Neural\nNetworks (MNNs) in combination with a clustering algorithm that can approximate\nthis mapping. Thus, the proof of the Mirroring theorem provides a theoretical\nbasis for the existence and a practical feasibility of constructing\nhierarchical classifiers, given the maps. Our proposed Mirroring Theorem can\nalso be considered as an extension to Kolmogrovs theorem in providing a\nrealistic solution for unsupervised classification. The techniques we develop,\nare general in nature and have led to the construction of learning machines\nwhich are (i) tree like in structure, (ii) modular (iii) with each module\nrunning on a common algorithm (tandem algorithm) and (iv) selfsupervised. We\nhave actually built the architecture, developed the tandem algorithm of such a\nhierarchical classifier and demonstrated it on an example problem.",
    "published": "2009-11-02T19:53:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sequential anomaly detection in the presence of noise and limited\n  feedback",
    "authors": [
      "Maxim Raginsky",
      "Rebecca Willett",
      "Corinne Horn",
      "Jorge Silva",
      "Roummel Marcia"
    ],
    "summary": "This paper describes a methodology for detecting anomalies from sequentially\nobserved and potentially noisy data. The proposed approach consists of two main\nelements: (1) {\\em filtering}, or assigning a belief or likelihood to each\nsuccessive measurement based upon our ability to predict it from previous noisy\nobservations, and (2) {\\em hedging}, or flagging potential anomalies by\ncomparing the current belief against a time-varying and data-adaptive\nthreshold. The threshold is adjusted based on the available feedback from an\nend user. Our algorithms, which combine universal prediction with recent work\non online convex programming, do not require computing posterior distributions\ngiven all current observations and involve simple primal-dual parameter\nupdates. At the heart of the proposed approach lie exponential-family models\nwhich can be used in a wide variety of contexts and applications, and which\nyield methods that achieve sublinear per-round regret against both static and\nslowly varying product distributions with marginals drawn from the same\nexponential family. Moreover, the regret against static distributions coincides\nwith the minimax value of the corresponding online strongly convex game. We\nalso prove bounds on the number of mistakes made during the hedging step\nrelative to the best offline choice of the threshold with access to all\nestimated beliefs and feedback signals. We validate the theory on synthetic\ndata drawn from a time-varying distribution over binary vectors of high\ndimensionality, as well as on the Enron email dataset.",
    "published": "2009-11-15T18:43:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Keystroke Dynamics Authentication For Collaborative Systems",
    "authors": [
      "Romain Giot",
      "Mohamad El-Abed",
      "Christophe Rosenberger"
    ],
    "summary": "We present in this paper a study on the ability and the benefits of using a\nkeystroke dynamics authentication method for collaborative systems.\nAuthentication is a challenging issue in order to guarantee the security of use\nof collaborative systems during the access control step. Many solutions exist\nin the state of the art such as the use of one time passwords or smart-cards.\nWe focus in this paper on biometric based solutions that do not necessitate any\nadditional sensor. Keystroke dynamics is an interesting solution as it uses\nonly the keyboard and is invisible for users. Many methods have been published\nin this field. We make a comparative study of many of them considering the\noperational constraints of use for collaborative systems.",
    "published": "2009-11-17T13:35:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Statistical exponential families: A digest with flash cards",
    "authors": [
      "Frank Nielsen",
      "Vincent Garcia"
    ],
    "summary": "This document describes concisely the ubiquitous class of exponential family\ndistributions met in statistics. The first part recalls definitions and\nsummarizes main properties and duality with Bregman divergences (all proofs are\nskipped). The second part lists decompositions and related formula of common\nexponential family distributions. We recall the Fisher-Rao-Riemannian\ngeometries and the dual affine connection information geometries of statistical\nmanifolds. It is intended to maintain and update this document and catalog by\nadding new distribution items.",
    "published": "2009-11-25T14:26:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Mixtures of Gaussians using the k-means Algorithm",
    "authors": [
      "Kamalika Chaudhuri",
      "Sanjoy Dasgupta",
      "Andrea Vattani"
    ],
    "summary": "One of the most popular algorithms for clustering in Euclidean space is the\n$k$-means algorithm; $k$-means is difficult to analyze mathematically, and few\ntheoretical guarantees are known about it, particularly when the data is {\\em\nwell-clustered}. In this paper, we attempt to fill this gap in the literature\nby analyzing the behavior of $k$-means on well-clustered data. In particular,\nwe study the case when each cluster is distributed as a different Gaussian --\nor, in other words, when the input comes from a mixture of Gaussians.\n  We analyze three aspects of the $k$-means algorithm under this assumption.\nFirst, we show that when the input comes from a mixture of two spherical\nGaussians, a variant of the 2-means algorithm successfully isolates the\nsubspace containing the means of the mixture components. Second, we show an\nexact expression for the convergence of our variant of the 2-means algorithm,\nwhen the input is a very large number of samples from a mixture of spherical\nGaussians. Our analysis does not require any lower bound on the separation\nbetween the mixture components.\n  Finally, we study the sample requirement of $k$-means; for a mixture of 2\nspherical Gaussians, we show an upper bound on the number of samples required\nby a variant of 2-means to get close to the true solution. The sample\nrequirement grows with increasing dimensionality of the data, and decreasing\nseparation between the means of the Gaussians. To match our upper bound, we\nshow an information-theoretic lower bound on any algorithm that learns mixtures\nof two spherical Gaussians; our lower bound indicates that in the case when the\noverlap between the probability masses of the two distributions is small, the\nsample requirement of $k$-means is {\\em near-optimal}.",
    "published": "2009-12-01T19:10:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Delay-Optimal Power and Subcarrier Allocation for OFDMA Systems via\n  Stochastic Approximation",
    "authors": [
      "Vincent K. N. Lau",
      "Ying Cui"
    ],
    "summary": "In this paper, we consider delay-optimal power and subcarrier allocation\ndesign for OFDMA systems with $N_F$ subcarriers, $K$ mobiles and one base\nstation. There are $K$ queues at the base station for the downlink traffic to\nthe $K$ mobiles with heterogeneous packet arrivals and delay requirements. We\nshall model the problem as a $K$-dimensional infinite horizon average reward\nMarkov Decision Problem (MDP) where the control actions are assumed to be a\nfunction of the instantaneous Channel State Information (CSI) as well as the\njoint Queue State Information (QSI). This problem is challenging because it\ncorresponds to a stochastic Network Utility Maximization (NUM) problem where\ngeneral solution is still unknown. We propose an {\\em online stochastic value\niteration} solution using {\\em stochastic approximation}. The proposed power\ncontrol algorithm, which is a function of both the CSI and the QSI, takes the\nform of multi-level water-filling. We prove that under two mild conditions in\nTheorem 1 (One is the stepsize condition. The other is the condition on\naccessibility of the Markov Chain, which can be easily satisfied in most of the\ncases we are interested.), the proposed solution converges to the optimal\nsolution almost surely (with probability 1) and the proposed framework offers a\npossible solution to the general stochastic NUM problem. By exploiting the\nbirth-death structure of the queue dynamics, we obtain a reduced complexity\ndecomposed solution with linear $\\mathcal{O}(KN_F)$ complexity and\n$\\mathcal{O}(K)$ memory requirement.",
    "published": "2009-12-07T10:35:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Association Rule Pruning based on Interestingness Measures with\n  Clustering",
    "authors": [
      "S. Kannan",
      "R. Bhaskaran"
    ],
    "summary": "Association rule mining plays vital part in knowledge mining. The difficult\ntask is discovering knowledge or useful rules from the large number of rules\ngenerated for reduced support. For pruning or grouping rules, several\ntechniques are used such as rule structure cover methods, informative cover\nmethods, rule clustering, etc. Another way of selecting association rules is\nbased on interestingness measures such as support, confidence, correlation, and\nso on. In this paper, we study how rule clusters of the pattern Xi - Y are\ndistributed over different interestingness measures.",
    "published": "2009-12-09T18:11:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Early Detection of Breast Cancer using SVM Classifier Technique",
    "authors": [
      "Y. Ireaneus Anna Rejani",
      "S. Thamarai Selvi"
    ],
    "summary": "This paper presents a tumor detection algorithm from mammogram. The proposed\nsystem focuses on the solution of two problems. One is how to detect tumors as\nsuspicious regions with a very weak contrast to their background and another is\nhow to extract features which categorize tumors. The tumor detection method\nfollows the scheme of (a) mammogram enhancement. (b) The segmentation of the\ntumor area. (c) The extraction of features from the segmented tumor area. (d)\nThe use of SVM classifier. The enhancement can be defined as conversion of the\nimage quality to a better and more understandable level. The mammogram\nenhancement procedure includes filtering, top hat operation, DWT. Then the\ncontrast stretching is used to increase the contrast of the image. The\nsegmentation of mammogram images has been playing an important role to improve\nthe detection and diagnosis of breast cancer. The most common segmentation\nmethod used is thresholding. The features are extracted from the segmented\nbreast area. Next stage include, which classifies the regions using the SVM\nclassifier. The method was tested on 75 mammographic images, from the mini-MIAS\ndatabase. The methodology achieved a sensitivity of 88.75%.",
    "published": "2009-12-11T18:50:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Performance Analysis of AIM-K-means & K-means in Quality Cluster\n  Generation",
    "authors": [
      "Samarjeet Borah",
      "Mrinal Kanti Ghose"
    ],
    "summary": "Among all the partition based clustering algorithms K-means is the most\npopular and well known method. It generally shows impressive results even in\nconsiderably large data sets. The computational complexity of K-means does not\nsuffer from the size of the data set. The main disadvantage faced in performing\nthis clustering is that the selection of initial means. If the user does not\nhave adequate knowledge about the data set, it may lead to erroneous results.\nThe algorithm Automatic Initialization of Means (AIM), which is an extension to\nK-means, has been proposed to overcome the problem of initial mean generation.\nIn this paper an attempt has been made to compare the performance of the\nalgorithms through implementation",
    "published": "2009-12-20T05:21:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and\n  Experimental Design",
    "authors": [
      "Niranjan Srinivas",
      "Andreas Krause",
      "Sham M. Kakade",
      "Matthias Seeger"
    ],
    "summary": "Many applications require optimizing an unknown, noisy function that is\nexpensive to evaluate. We formalize this task as a multi-armed bandit problem,\nwhere the payoff function is either sampled from a Gaussian process (GP) or has\nlow RKHS norm. We resolve the important open problem of deriving regret bounds\nfor this setting, which imply novel convergence rates for GP optimization. We\nanalyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its\ncumulative regret in terms of maximal information gain, establishing a novel\nconnection between GP optimization and experimental design. Moreover, by\nbounding the latter in terms of operator spectra, we obtain explicit sublinear\nregret bounds for many commonly used covariance functions. In some important\ncases, our bounds have surprisingly weak dependence on the dimensionality. In\nour experiments on real sensor data, GP-UCB compares favorably with other\nheuristical GP optimization approaches.",
    "published": "2009-12-21T00:08:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Query Complexity for Reconstructing Hypergraphs",
    "authors": [
      "Nader H. Bshouty",
      "Hanna Mazzawi"
    ],
    "summary": "In this paper we consider the problem of reconstructing a hidden weighted\nhypergraph of constant rank using additive queries. We prove the following: Let\n$G$ be a weighted hidden hypergraph of constant rank with n vertices and $m$\nhyperedges. For any $m$ there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log n}{\\log m}) $$\nadditive queries. This solves the open problem in [S. Choi, J. H. Kim. Optimal\nQuery Complexity Bounds for Finding Graphs. {\\em STOC}, 749--758,~2008].\n  When the weights of the hypergraph are integers that are less than\n$O(poly(n^d/m))$ where $d$ is the rank of the hypergraph (and therefore for\nunweighted hypergraphs) there exists a non-adaptive algorithm that finds the\nedges of the graph and their weights using $$ O(\\frac{m\\log \\frac{n^d}{m}}{\\log\nm}). $$ additive queries.\n  Using the information theoretic bound the above query complexities are tight.",
    "published": "2010-01-03T19:54:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Linear Probability Forecasting",
    "authors": [
      "Fedor Zhdanov",
      "Yuri Kalnishkan"
    ],
    "summary": "Multi-class classification is one of the most important tasks in machine\nlearning. In this paper we consider two online multi-class classification\nproblems: classification by a linear model and by a kernelized model. The\nquality of predictions is measured by the Brier loss function. We suggest two\ncomputationally efficient algorithms to work with these problems and prove\ntheoretical guarantees on their losses. We kernelize one of the algorithms and\nprove theoretical guarantees on its loss. We perform experiments and compare\nour algorithms with logistic regression.",
    "published": "2010-01-06T12:40:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Measuring Latent Causal Structure",
    "authors": [
      "Ricardo Silva"
    ],
    "summary": "Discovering latent representations of the observed world has become\nincreasingly more relevant in data analysis. Much of the effort concentrates on\nbuilding latent variables which can be used in prediction problems, such as\nclassification and regression. A related goal of learning latent structure from\ndata is that of identifying which hidden common causes generate the\nobservations, such as in applications that require predicting the effect of\npolicies. This will be the main problem tackled in our contribution: given a\ndataset of indicators assumed to be generated by unknown and unmeasured common\ncauses, we wish to discover which hidden common causes are those, and how they\ngenerate our data. This is possible under the assumption that observed\nvariables are linear functions of the latent causes with additive noise.\nPrevious results in the literature present solutions for the case where each\nobserved variable is a noisy function of a single latent variable. We show how\nto extend the existing results for some cases where observed variables measure\nmore than one latent variable.",
    "published": "2010-01-07T14:41:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Asymptotic Learning Curve and Renormalizable Condition in Statistical\n  Learning Theory",
    "authors": [
      "Sumio Watanabe"
    ],
    "summary": "Bayes statistics and statistical physics have the common mathematical\nstructure, where the log likelihood function corresponds to the random\nHamiltonian. Recently, it was discovered that the asymptotic learning curves in\nBayes estimation are subject to a universal law, even if the log likelihood\nfunction can not be approximated by any quadratic form. However, it is left\nunknown what mathematical property ensures such a universal law. In this paper,\nwe define a renormalizable condition of the statistical estimation problem, and\nshow that, under such a condition, the asymptotic learning curves are ensured\nto be subject to the universal law, even if the true distribution is\nunrealizable and singular for a statistical model. Also we study a\nnonrenormalizable case, in which the learning curves have the different\nasymptotic behaviors from the universal law.",
    "published": "2010-01-18T05:34:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Role of Interestingness Measures in CAR Rule Ordering for Associative\n  Classifier: An Empirical Approach",
    "authors": [
      "S. Kannan",
      "R. Bhaskaran"
    ],
    "summary": "Associative Classifier is a novel technique which is the integration of\nAssociation Rule Mining and Classification. The difficult task in building\nAssociative Classifier model is the selection of relevant rules from a large\nnumber of class association rules (CARs). A very popular method of ordering\nrules for selection is based on confidence, support and antecedent size (CSA).\nOther methods are based on hybrid orderings in which CSA method is combined\nwith other measures. In the present work, we study the effect of using\ndifferent interestingness measures of Association rules in CAR rule ordering\nand selection for associative classifier.",
    "published": "2010-01-20T07:30:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Trajectory Clustering and an Application to Airspace Monitoring",
    "authors": [
      "Maxime Gariel",
      "Ashok N. Srivastava",
      "Eric Feron"
    ],
    "summary": "This paper presents a framework aimed at monitoring the behavior of aircraft\nin a given airspace. Nominal trajectories are determined and learned using data\ndriven methods. Standard procedures are used by air traffic controllers (ATC)\nto guide aircraft, ensure the safety of the airspace, and to maximize the\nrunway occupancy. Even though standard procedures are used by ATC, the control\nof the aircraft remains with the pilots, leading to a large variability in the\nflight patterns observed. Two methods to identify typical operations and their\nvariability from recorded radar tracks are presented. This knowledge base is\nthen used to monitor the conformance of current operations against operations\npreviously identified as standard. A tool called AirTrajectoryMiner is\npresented, aiming at monitoring the instantaneous health of the airspace, in\nreal time. The airspace is \"healthy\" when all aircraft are flying according to\nthe nominal procedures. A measure of complexity is introduced, measuring the\nconformance of current flight to nominal flight patterns. When an aircraft does\nnot conform, the complexity increases as more attention from ATC is required to\nensure a safe separation between aircraft.",
    "published": "2010-01-27T19:24:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Aggregating Algorithm competing with Banach lattices",
    "authors": [
      "Fedor Zhdanov",
      "Alexey Chernov",
      "Yuri Kalnishkan"
    ],
    "summary": "The paper deals with on-line regression settings with signals belonging to a\nBanach lattice. Our algorithms work in a semi-online setting where all the\ninputs are known in advance and outcomes are unknown and given step by step. We\napply the Aggregating Algorithm to construct a prediction method whose\ncumulative loss over all the input vectors is comparable with the cumulative\nloss of any linear functional on the Banach lattice. As a by-product we get an\nalgorithm that takes signals from an arbitrary domain. Its cumulative loss is\ncomparable with the cumulative loss of any predictor function from Besov and\nTriebel-Lizorkin spaces. We describe several applications of our setting.",
    "published": "2010-02-03T11:31:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A CHAID Based Performance Prediction Model in Educational Data Mining",
    "authors": [
      "M. Ramaswami",
      "R. Bhaskaran"
    ],
    "summary": "The performance in higher secondary school education in India is a turning\npoint in the academic lives of all students. As this academic performance is\ninfluenced by many factors, it is essential to develop predictive data mining\nmodel for students' performance so as to identify the slow learners and study\nthe influence of the dominant factors on their academic performance. In the\npresent investigation, a survey cum experimental methodology was adopted to\ngenerate a database and it was constructed from a primary and a secondary\nsource. While the primary data was collected from the regular students, the\nsecondary data was gathered from the school and office of the Chief Educational\nOfficer (CEO). A total of 1000 datasets of the year 2006 from five different\nschools in three different districts of Tamilnadu were collected. The raw data\nwas preprocessed in terms of filling up missing values, transforming values in\none form into another and relevant attribute/ variable selection. As a result,\nwe had 772 student records, which were used for CHAID prediction model\nconstruction. A set of prediction rules were extracted from CHIAD prediction\nmodel and the efficiency of the generated CHIAD prediction model was found. The\naccuracy of the present model was compared with other model and it has been\nfound to be satisfactory.",
    "published": "2010-02-05T08:27:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dimensionality Reduction: An Empirical Study on the Usability of IFE-CF\n  (Independent Feature Elimination- by C-Correlation and F-Correlation)\n  Measures",
    "authors": [
      "M. Babu Reddy",
      "L. S. S. Reddy"
    ],
    "summary": "The recent increase in dimensionality of data has thrown a great challenge to\nthe existing dimensionality reduction methods in terms of their effectiveness.\nDimensionality reduction has emerged as one of the significant preprocessing\nsteps in machine learning applications and has been effective in removing\ninappropriate data, increasing learning accuracy, and improving\ncomprehensibility. Feature redundancy exercises great influence on the\nperformance of classification process. Towards the better classification\nperformance, this paper addresses the usefulness of truncating the highly\ncorrelated and redundant attributes. Here, an effort has been made to verify\nthe utility of dimensionality reduction by applying LVQ (Learning Vector\nQuantization) method on two Benchmark datasets of 'Pima Indian Diabetic\npatients' and 'Lung cancer patients'.",
    "published": "2010-02-05T08:59:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Distributed Sensor Selection",
    "authors": [
      "Daniel Golovin",
      "Matthew Faulkner",
      "Andreas Krause"
    ],
    "summary": "A key problem in sensor networks is to decide which sensors to query when, in\norder to obtain the most useful information (e.g., for performing accurate\nprediction), subject to constraints (e.g., on power and bandwidth). In many\napplications the utility function is not known a priori, must be learned from\ndata, and can even change over time. Furthermore for large sensor networks\nsolving a centralized optimization problem to select sensors is not feasible,\nand thus we seek a fully distributed solution. In this paper, we present\nDistributed Online Greedy (DOG), an efficient, distributed algorithm for\nrepeatedly selecting sensors online, only receiving feedback about the utility\nof the selected sensors. We prove very strong theoretical no-regret guarantees\nthat apply whenever the (unknown) utility function satisfies a natural\ndiminishing returns property called submodularity. Our algorithm has extremely\nlow communication requirements, and scales well to large sensor deployments. We\nextend DOG to allow observation-dependent sensor selection. We empirically\ndemonstrate the effectiveness of our algorithm on several real-world sensing\ntasks.",
    "published": "2010-02-09T07:32:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Stability of Empirical Risk Minimization in the Presence of\n  Multiple Risk Minimizers",
    "authors": [
      "Benjamin I. P. Rubinstein",
      "Aleksandr Simma"
    ],
    "summary": "Recently Kutin and Niyogi investigated several notions of algorithmic\nstability--a property of a learning map conceptually similar to\ncontinuity--showing that training-stability is sufficient for consistency of\nEmpirical Risk Minimization while distribution-free CV-stability is necessary\nand sufficient for having finite VC-dimension. This paper concerns a phase\ntransition in the training stability of ERM, conjectured by the same authors.\nKutin and Niyogi proved that ERM on finite hypothesis spaces containing a\nunique risk minimizer has training stability that scales exponentially with\nsample size, and conjectured that the existence of multiple risk minimizers\nprevents even super-quadratic convergence. We prove this result for the\nstrictly weaker notion of CV-stability, positively resolving the conjecture.",
    "published": "2010-02-10T09:08:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Collaborative Filtering in a Non-Uniform World: Learning with the\n  Weighted Trace Norm",
    "authors": [
      "Ruslan Salakhutdinov",
      "Nathan Srebro"
    ],
    "summary": "We show that matrix completion with trace-norm regularization can be\nsignificantly hurt when entries of the matrix are sampled non-uniformly. We\nintroduce a weighted version of the trace-norm regularizer that works well also\nwith non-uniform sampling. Our experimental results demonstrate that the\nweighted trace-norm regularization indeed yields significant gains on the\n(highly non-uniformly sampled) Netflix dataset.",
    "published": "2010-02-14T16:37:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Interactive Submodular Set Cover",
    "authors": [
      "Andrew Guillory",
      "Jeff Bilmes"
    ],
    "summary": "We introduce a natural generalization of submodular set cover and exact\nactive learning with a finite hypothesis class (query learning). We call this\nnew problem interactive submodular set cover. Applications include advertising\nin social networks with hidden information. We give an approximation guarantee\nfor a novel greedy algorithm and give a hardness of approximation result which\nmatches up to constant factors. We also discuss negative results for simpler\napproaches and present encouraging early experimental results.",
    "published": "2010-02-17T18:43:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
    "authors": [
      "Ram Sarkar",
      "Nibaran Das",
      "Subhadip Basu",
      "Mahantapas Kundu",
      "Mita Nasipuri",
      "Dipak Kumar Basu"
    ],
    "summary": "India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.",
    "published": "2010-02-21T19:48:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Contextual Bandit Algorithms with Supervised Learning Guarantees",
    "authors": [
      "Alina Beygelzimer",
      "John Langford",
      "Lihong Li",
      "Lev Reyzin",
      "Robert E. Schapire"
    ],
    "summary": "We address the problem of learning in an online, bandit setting where the\nlearner must repeatedly select among $K$ actions, but only receives partial\nfeedback based on its choices. We establish two new facts: First, using a new\nalgorithm called Exp4.P, we show that it is possible to compete with the best\nin a set of $N$ experts with probability $1-\\delta$ while incurring regret at\nmost $O(\\sqrt{KT\\ln(N/\\delta)})$ over $T$ time steps. The new algorithm is\ntested empirically in a large-scale, real-world dataset. Second, we give a new\nalgorithm called VE that competes with a possibly infinite set of policies of\nVC-dimension $d$ while incurring regret at most $O(\\sqrt{T(d\\ln(T) + \\ln\n(1/\\delta))})$ with probability $1-\\delta$. These guarantees improve on those\nof all previous algorithms, whether in a stochastic or adversarial environment,\nand bring us closer to providing supervised learning type guarantees for the\ncontextual bandit setting.",
    "published": "2010-02-22T07:11:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Bound Optimization for Online Convex Optimization",
    "authors": [
      "H. Brendan McMahan",
      "Matthew Streeter"
    ],
    "summary": "We introduce a new online convex optimization algorithm that adaptively\nchooses its regularization function based on the loss functions observed so\nfar. This is in contrast to previous algorithms that use a fixed regularization\nfunction such as L2-squared, and modify it only via a single time-dependent\nparameter. Our algorithm's regret bounds are worst-case optimal, and for\ncertain realistic classes of loss functions they are much better than existing\nbounds. These bounds are problem-dependent, which means they can exploit the\nstructure of the actual problem instance. Critically, however, our algorithm\ndoes not need to know this structure in advance. Rather, we prove competitive\nguarantees that show the algorithm provides a bound within a constant factor of\nthe best possible bound (of a certain functional form) in hindsight.",
    "published": "2010-02-26T01:36:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Asymptotic Analysis of Generative Semi-Supervised Learning",
    "authors": [
      "Joshua V Dillon",
      "Krishnakumar Balasubramanian",
      "Guy Lebanon"
    ],
    "summary": "Semisupervised learning has emerged as a popular framework for improving\nmodeling accuracy while controlling labeling cost. Based on an extension of\nstochastic composite likelihood we quantify the asymptotic accuracy of\ngenerative semi-supervised learning. In doing so, we complement\ndistribution-free analysis by providing an alternative framework to measure the\nvalue associated with different labeling policies and resolve the fundamental\nquestion of how much data to label and in what manner. We demonstrate our\napproach with both simulation studies and real world experiments using naive\nBayes for text classification and MRFs and CRFs for structured prediction in\nNLP.",
    "published": "2010-02-26T21:59:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unsupervised Supervised Learning II: Training Margin Based Classifiers\n  without Labels",
    "authors": [
      "Krishnakumar Balasubramanian",
      "Pinar Donmez",
      "Guy Lebanon"
    ],
    "summary": "Many popular linear classifiers, such as logistic regression, boosting, or\nSVM, are trained by optimizing a margin-based risk function. Traditionally,\nthese risk functions are computed based on a labeled dataset. We develop a\nnovel technique for estimating such risks using only unlabeled data and the\nmarginal label distribution. We prove that the proposed risk estimator is\nconsistent on high-dimensional datasets and demonstrate it on synthetic and\nreal-world data. In particular, we show how the estimate is used for evaluating\nclassifiers in transfer learning, and for training classifiers with no labeled\ndata whatsoever.",
    "published": "2010-03-01T22:32:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Model Selection with the Loss Rank Principle",
    "authors": [
      "Marcus Hutter",
      "Minh-Ngoc Tran"
    ],
    "summary": "A key issue in statistics and machine learning is to automatically select the\n\"right\" model complexity, e.g., the number of neighbors to be averaged over in\nk nearest neighbor (kNN) regression or the polynomial degree in regression with\npolynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) -\nfor model selection in regression and classification. It is based on the loss\nrank, which counts how many other (fictitious) data would be fitted better.\nLoRP selects the model that has minimal loss rank. Unlike most penalized\nmaximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the\nregression functions and the loss function. It works without a stochastic noise\nmodel, and is directly applicable to any non-parametric regressor, like kNN.",
    "published": "2010-03-02T08:21:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Statistical and Computational Tradeoffs in Stochastic Composite\n  Likelihood",
    "authors": [
      "Joshua V Dillon",
      "Guy Lebanon"
    ],
    "summary": "Maximum likelihood estimators are often of limited practical use due to the\nintensive computation they require. We propose a family of alternative\nestimators that maximize a stochastic variation of the composite likelihood\nfunction. Each of the estimators resolve the computation-accuracy tradeoff\ndifferently, and taken together they span a continuous spectrum of\ncomputation-accuracy tradeoff resolutions. We prove the consistency of the\nestimators, provide formulas for their asymptotic variance, statistical\nrobustness, and computational complexity. We discuss experimental results in\nthe context of Boltzmann machines and conditional random fields. The\ntheoretical and experimental studies demonstrate the effectiveness of the\nestimators when the computational resources are insufficient. They also\ndemonstrate that in some cases reduced computational complexity is associated\nwith robustness thereby increasing statistical accuracy.",
    "published": "2010-03-02T21:54:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exponential Family Hybrid Semi-Supervised Learning",
    "authors": [
      "Arvind Agarwal",
      "Hal Daume III"
    ],
    "summary": "We present an approach to semi-supervised learning based on an exponential\nfamily characterization. Our approach generalizes previous work on coupled\npriors for hybrid generative/discriminative models. Our model is more flexible\nand natural than previous approaches. Experimental results on several data sets\nshow that our approach also performs better in practice.",
    "published": "2010-03-02T22:27:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A New Clustering Approach based on Page's Path Similarity for Navigation\n  Patterns Mining",
    "authors": [
      "Heidar Mamosian",
      "Amir Masoud Rahmani",
      "Mashalla Abbasi Dezfouli"
    ],
    "summary": "In recent years, predicting the user's next request in web navigation has\nreceived much attention. An information source to be used for dealing with such\nproblem is the left information by the previous web users stored at the web\naccess log on the web servers. Purposed systems for this problem work based on\nthis idea that if a large number of web users request specific pages of a\nwebsite on a given session, it can be concluded that these pages are satisfying\nsimilar information needs, and therefore they are conceptually related. In this\nstudy, a new clustering approach is introduced that employs logical path\nstoring of a website pages as another parameter which is regarded as a\nsimilarity parameter and conceptual relation between web pages. The results of\nsimulation have shown that the proposed approach is more than others precise in\ndetermining the clusters.",
    "published": "2010-03-07T11:08:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hierarchical Web Page Classification Based on a Topic Model and\n  Neighboring Pages Integration",
    "authors": [
      "Wongkot Sriurai",
      "Phayung Meesad",
      "Choochart Haruechaiyasak"
    ],
    "summary": "Most Web page classification models typically apply the bag of words (BOW)\nmodel to represent the feature space. The original BOW representation, however,\nis unable to recognize semantic relationships between terms. One possible\nsolution is to apply the topic model approach based on the Latent Dirichlet\nAllocation algorithm to cluster the term features into a set of latent topics.\nTerms assigned into the same topic are semantically related. In this paper, we\npropose a novel hierarchical classification method based on a topic model and\nby integrating additional term features from neighboring pages. Our\nhierarchical classification method consists of two phases: (1) feature\nrepresentation by using a topic model and integrating neighboring pages, and\n(2) hierarchical Support Vector Machines (SVM) classification model constructed\nfrom a confusion matrix. From the experimental results, the approach of using\nthe proposed hierarchical SVM model by integrating current page with\nneighboring pages via the topic model yielded the best performance with the\naccuracy equal to 90.33% and the F1 measure of 90.14%; an improvement of 5.12%\nand 5.13% over the original SVM model, respectively.",
    "published": "2010-03-07T18:32:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supermartingales in Prediction with Expert Advice",
    "authors": [
      "Alexey Chernov",
      "Yuri Kalnishkan",
      "Fedor Zhdanov",
      "Vladimir Vovk"
    ],
    "summary": "We apply the method of defensive forecasting, based on the use of\ngame-theoretic supermartingales, to prediction with expert advice. In the\ntraditional setting of a countable number of experts and a finite number of\noutcomes, the Defensive Forecasting Algorithm is very close to the well-known\nAggregating Algorithm. Not only the performance guarantees but also the\npredictions are the same for these two methods of fundamentally different\nnature. We discuss also a new setting where the experts can give advice\nconditional on the learner's future decision. Both the algorithms can be\nadapted to the new setting and give the same performance guarantees as in the\ntraditional setting. Finally, we outline an application of defensive\nforecasting to a setting with several loss functions.",
    "published": "2010-03-10T21:53:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "State-Space Dynamics Distance for Clustering Sequential Data",
    "authors": [
      "Darío García-García",
      "Emilio Parrado-Hernández",
      "Fernando Díaz-de-María"
    ],
    "summary": "This paper proposes a novel similarity measure for clustering sequential\ndata. We first construct a common state-space by training a single\nprobabilistic model with all the sequences in order to get a unified\nrepresentation for the dataset. Then, distances are obtained attending to the\ntransition matrices induced by each sequence in that state-space. This approach\nsolves some of the usual overfitting and scalability issues of the existing\nsemi-parametric techniques, that rely on training a model for each sequence.\nEmpirical studies on both synthetic and real-world datasets illustrate the\nadvantages of the proposed similarity measure for clustering sequences.",
    "published": "2010-04-09T09:36:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable\n  Information Criterion in Singular Learning Theory",
    "authors": [
      "Sumio Watanabe"
    ],
    "summary": "In regular statistical models, the leave-one-out cross-validation is\nasymptotically equivalent to the Akaike information criterion. However, since\nmany learning machines are singular statistical models, the asymptotic behavior\nof the cross-validation remains unknown. In previous studies, we established\nthe singular learning theory and proposed a widely applicable information\ncriterion, the expectation value of which is asymptotically equal to the\naverage Bayes generalization loss. In the present paper, we theoretically\ncompare the Bayes cross-validation loss and the widely applicable information\ncriterion and prove two theorems. First, the Bayes cross-validation loss is\nasymptotically equivalent to the widely applicable information criterion as a\nrandom variable. Therefore, model selection and hyperparameter optimization\nusing these two values are asymptotically equivalent. Second, the sum of the\nBayes generalization error and the Bayes cross-validation error is\nasymptotically equal to $2\\lambda/n$, where $\\lambda$ is the real log canonical\nthreshold and $n$ is the number of training samples. Therefore the relation\nbetween the cross-validation error and the generalization error is determined\nby the algebraic geometrical structure of a learning machine. We also clarify\nthat the deviance information criteria are different from the Bayes\ncross-validation and the widely applicable information criterion.",
    "published": "2010-04-14T05:08:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generation and Interpretation of Temporal Decision Rules",
    "authors": [
      "Kamran Karimi",
      "Howard J. Hamilton"
    ],
    "summary": "We present a solution to the problem of understanding a system that produces\na sequence of temporally ordered observations. Our solution is based on\ngenerating and interpreting a set of temporal decision rules. A temporal\ndecision rule is a decision rule that can be used to predict or retrodict the\nvalue of a decision attribute, using condition attributes that are observed at\ntimes other than the decision attribute's time of observation. A rule set,\nconsisting of a set of temporal decision rules with the same decision\nattribute, can be interpreted by our Temporal Investigation Method for\nEnregistered Record Sequences (TIMERS) to signify an instantaneous, an acausal\nor a possibly causal relationship between the condition attributes and the\ndecision attribute. We show the effectiveness of our method, by describing a\nnumber of experiments with both synthetic and real temporal data.",
    "published": "2010-04-20T02:52:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bregman Distance to L1 Regularized Logistic Regression",
    "authors": [
      "Mithun Das Gupta",
      "Thomas S. Huang"
    ],
    "summary": "In this work we investigate the relationship between Bregman distances and\nregularized Logistic Regression model. We present a detailed study of Bregman\nDistance minimization, a family of generalized entropy measures associated with\nconvex functions. We convert the L1-regularized logistic regression into this\nmore general framework and propose a primal-dual method based algorithm for\nlearning the parameters. We pose L1-regularized logistic regression into\nBregman distance minimization and then apply non-linear constrained\noptimization techniques to estimate the parameters of the logistic model.",
    "published": "2010-04-21T23:09:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Learning with Partially Observed Attributes",
    "authors": [
      "Nicolò Cesa-Bianchi",
      "Shai Shalev-Shwartz",
      "Ohad Shamir"
    ],
    "summary": "We describe and analyze efficient algorithms for learning a linear predictor\nfrom examples when the learner can only view a few attributes of each training\nexample. This is the case, for instance, in medical research, where each\npatient participating in the experiment is only willing to go through a small\nnumber of tests. Our analysis bounds the number of additional examples\nsufficient to compensate for the lack of full information on each training\nexample. We demonstrate the efficiency of our algorithms by showing that when\nrunning on digit recognition data, they obtain a high prediction accuracy even\nwhen the learner gets to see only four pixels of each image.",
    "published": "2010-04-26T07:41:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning from Multiple Outlooks",
    "authors": [
      "Maayan Harel",
      "Shie Mannor"
    ],
    "summary": "We propose a novel problem formulation of learning a single task when the\ndata are provided in different feature spaces. Each such space is called an\noutlook, and is assumed to contain both labeled and unlabeled data. The\nobjective is to take advantage of the data from all the outlooks to better\nclassify each of the outlooks. We devise an algorithm that computes optimal\naffine mappings from different outlooks to a target outlook by matching moments\nof the empirical distributions. We further derive a probabilistic\ninterpretation of the resulting algorithm and a sample complexity bound\nindicating how many samples are needed to adequately find the mapping. We\nreport the results of extensive experiments on activity recognition tasks that\nshow the value of the proposed approach in boosting performance.",
    "published": "2010-04-30T21:52:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Geometric View of Conjugate Priors",
    "authors": [
      "Arvind Agarwal",
      "Hal Daume III"
    ],
    "summary": "In Bayesian machine learning, conjugate priors are popular, mostly due to\nmathematical convenience. In this paper, we show that there are deeper reasons\nfor choosing a conjugate prior. Specifically, we formulate the conjugate prior\nin the form of Bregman divergence and show that it is the inherent geometry of\nconjugate priors that makes them appropriate and intuitive. This geometric\ninterpretation allows one to view the hyperparameters of conjugate priors as\nthe {\\it effective} sample points, thus providing additional intuition. We use\nthis geometric understanding of conjugate priors to derive the hyperparameters\nand expression of the prior used to couple the generative and discriminative\ncomponents of a hybrid model for semi-supervised learning.",
    "published": "2010-05-01T06:06:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Distributive Stochastic Learning for Delay-Optimal OFDMA Power and\n  Subband Allocation",
    "authors": [
      "Ying Cui",
      "Vincent K. N. Lau"
    ],
    "summary": "In this paper, we consider the distributive queue-aware power and subband\nallocation design for a delay-optimal OFDMA uplink system with one base\nstation, $K$ users and $N_F$ independent subbands. Each mobile has an uplink\nqueue with heterogeneous packet arrivals and delay requirements. We model the\nproblem as an infinite horizon average reward Markov Decision Problem (MDP)\nwhere the control actions are functions of the instantaneous Channel State\nInformation (CSI) as well as the joint Queue State Information (QSI). To\naddress the distributive requirement and the issue of exponential memory\nrequirement and computational complexity, we approximate the subband allocation\nQ-factor by the sum of the per-user subband allocation Q-factor and derive a\ndistributive online stochastic learning algorithm to estimate the per-user\nQ-factor and the Lagrange multipliers (LM) simultaneously and determine the\ncontrol actions using an auction mechanism. We show that under the proposed\nauction mechanism, the distributive online learning converges almost surely\n(with probability 1). For illustration, we apply the proposed distributive\nstochastic learning framework to an application example with exponential packet\nsize distribution. We show that the delay-optimal power control has the {\\em\nmulti-level water-filling} structure where the CSI determines the instantaneous\npower allocation and the QSI determines the water-level. The proposed algorithm\nhas linear signaling overhead and computational complexity $\\mathcal O(KN)$,\nwhich is desirable from an implementation perspective.",
    "published": "2010-05-01T13:57:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Statistical Learning in Automated Troubleshooting: Application to LTE\n  Interference Mitigation",
    "authors": [
      "Moazzam Islam Tiwana",
      "Berna Sayrac",
      "Zwi Altman"
    ],
    "summary": "This paper presents a method for automated healing as part of off-line\nautomated troubleshooting. The method combines statistical learning with\nconstraint optimization. The automated healing aims at locally optimizing radio\nresource management (RRM) or system parameters of cells with poor performance\nin an iterative manner. The statistical learning processes the data using\nLogistic Regression (LR) to extract closed form (functional) relations between\nKey Performance Indicators (KPIs) and Radio Resource Management (RRM)\nparameters. These functional relations are then processed by an optimization\nengine which proposes new parameter values. The advantage of the proposed\nformulation is the small number of iterations required by the automated healing\nmethod to converge, making it suitable for off-line implementation. The\nproposed method is applied to heal an Inter-Cell Interference Coordination\n(ICIC) process in a 3G Long Term Evolution (LTE) network which is based on\nsoft-frequency reuse scheme. Numerical simulations illustrate the benefits of\nthe proposed approach.",
    "published": "2010-05-03T16:35:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Complex Gaussian Kernel LMS algorithm",
    "authors": [
      "Pantelis Bouboulis",
      "Sergios Theodoridis"
    ],
    "summary": "Although the real reproducing kernels are used in an increasing number of\nmachine learning problems, complex kernels have not, yet, been used, in spite\nof their potential interest in applications such as communications. In this\nwork, we focus our attention on the complex gaussian kernel and its possible\napplication in the complex Kernel LMS algorithm. In order to derive the\ngradients needed to develop the complex kernel LMS (CKLMS), we employ the\npowerful tool of Wirtinger's Calculus, which has recently attracted much\nattention in the signal processing community. Writinger's calculus simplifies\ncomputations and offers an elegant tool for treating complex signals. To this\nend, the notion of Writinger's calculus is extended to include complex RKHSs.\nExperiments verify that the CKLMS offers significant performance improvements\nover the traditional complex LMS or Widely Linear complex LMS (WL-LMS)\nalgorithms, when dealing with nonlinearities.",
    "published": "2010-05-06T06:42:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extension of Wirtinger Calculus in RKH Spaces and the Complex Kernel LMS",
    "authors": [
      "Pantelis Bouboulis",
      "Sergios Theodoridis"
    ],
    "summary": "Over the last decade, kernel methods for nonlinear processing have\nsuccessfully been used in the machine learning community. However, so far, the\nemphasis has been on batch techniques. It is only recently, that online\nadaptive techniques have been considered in the context of signal processing\ntasks. To the best of our knowledge, no kernel-based strategy has been\ndeveloped, so far, that is able to deal with complex valued signals. In this\npaper, we take advantage of a technique called complexification of real RKHSs\nto attack this problem. In order to derive gradients and subgradients of\noperators that need to be defined on the associated complex RKHSs, we employ\nthe powerful tool ofWirtinger's Calculus, which has recently attracted much\nattention in the signal processing community. Writinger's calculus simplifies\ncomputations and offers an elegant tool for treating complex signals. To this\nend, in this paper, the notion of Writinger's calculus is extended, for the\nfirst time, to include complex RKHSs and use it to derive the Complex Kernel\nLeast-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be\nused to derive nonlinear stable algorithms, which offer significant performance\nimprovements over the traditional complex LMS orWidely Linear complex LMS\n(WL-LMS) algorithms, when dealing with nonlinearities.",
    "published": "2010-05-06T06:59:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improving Semi-Supervised Support Vector Machines Through Unlabeled\n  Instances Selection",
    "authors": [
      "Yu-Feng Li",
      "Zhi-Hua Zhou"
    ],
    "summary": "Semi-supervised support vector machines (S3VMs) are a kind of popular\napproaches which try to improve learning performance by exploiting unlabeled\ndata. Though S3VMs have been found helpful in many situations, they may\ndegenerate performance and the resultant generalization ability may be even\nworse than using the labeled data only. In this paper, we try to reduce the\nchance of performance degeneration of S3VMs. Our basic idea is that, rather\nthan exploiting all unlabeled data, the unlabeled instances should be selected\nsuch that only the ones which are very likely to be helpful are exploited,\nwhile some highly risky unlabeled instances are avoided. We propose the\nS3VM-\\emph{us} method by using hierarchical clustering to select the unlabeled\ninstances. Experiments on a broad range of data sets over eighty-eight\ndifferent settings show that the chance of performance degeneration of\nS3VM-\\emph{us} is much smaller than that of existing S3VMs.",
    "published": "2010-05-10T13:49:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction with Expert Advice under Discounted Loss",
    "authors": [
      "Alexey Chernov",
      "Fedor Zhdanov"
    ],
    "summary": "We study prediction with expert advice in the setting where the losses are\naccumulated with some discounting---the impact of old losses may gradually\nvanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm\nfor Regression to this case, propose a suitable new variant of exponential\nweights algorithm, and prove respective loss bounds.",
    "published": "2010-05-11T19:27:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Detecting Blackholes and Volcanoes in Directed Networks",
    "authors": [
      "Zhongmou Li",
      "Hui Xiong",
      "Yanchi Liu"
    ],
    "summary": "In this paper, we formulate a novel problem for finding blackhole and volcano\npatterns in a large directed graph. Specifically, a blackhole pattern is a\ngroup which is made of a set of nodes in a way such that there are only inlinks\nto this group from the rest nodes in the graph. In contrast, a volcano pattern\nis a group which only has outlinks to the rest nodes in the graph. Both\npatterns can be observed in real world. For instance, in a trading network, a\nblackhole pattern may represent a group of traders who are manipulating the\nmarket. In the paper, we first prove that the blackhole mining problem is a\ndual problem of finding volcanoes. Therefore, we focus on finding the blackhole\npatterns. Along this line, we design two pruning schemes to guide the blackhole\nfinding process. In the first pruning scheme, we strategically prune the search\nspace based on a set of pattern-size-independent pruning rules and develop an\niBlackhole algorithm. The second pruning scheme follows a divide-and-conquer\nstrategy to further exploit the pruning results from the first pruning scheme.\nIndeed, a target directed graphs can be divided into several disconnected\nsubgraphs by the first pruning scheme, and thus the blackhole finding can be\nconducted in each disconnected subgraph rather than in a large graph. Based on\nthese two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally,\nexperimental results on real-world data show that the iBlackhole-DC algorithm\ncan be several orders of magnitude faster than the iBlackhole algorithm, which\nhas a huge computational advantage over a brute-force method.",
    "published": "2010-05-12T19:53:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Robustness and Generalization",
    "authors": [
      "Huan Xu",
      "Shie Mannor"
    ],
    "summary": "We derive generalization bounds for learning algorithms based on their\nrobustness: the property that if a testing sample is \"similar\" to a training\nsample, then the testing error is close to the training error. This provides a\nnovel approach, different from the complexity or stability arguments, to study\ngeneralization of learning algorithms. We further show that a weak notion of\nrobustness is both sufficient and necessary for generalizability, which implies\nthat robustness is a fundamental property for learning algorithms to work.",
    "published": "2010-05-13T01:59:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning of Noisy Data with Kernels",
    "authors": [
      "Nicolò Cesa-Bianchi",
      "Shai Shalev-Shwartz",
      "Ohad Shamir"
    ],
    "summary": "We study online learning when individual instances are corrupted by\nadversarially chosen random noise. We assume the noise distribution is unknown,\nand may change over time with no restriction other than having zero mean and\nbounded variance. Our technique relies on a family of unbiased estimators for\nnon-linear functions, which may be of independent interest. We show that a\nvariant of online gradient descent can learn functions in any dot-product\n(e.g., polynomial) or Gaussian kernel space with any analytic convex loss\nfunction. Our variant uses randomized estimates that need to query a random\nnumber of noisy copies of each instance, where with high probability this\nnumber is upper bounded by a constant. Allowing such multiple queries cannot be\navoided: Indeed, we show that online learning is in general impossible when\nonly one noisy copy of each instance can be accessed.",
    "published": "2010-05-13T10:56:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evolution with Drifting Targets",
    "authors": [
      "Varun Kanade",
      "Leslie G. Valiant",
      "Jennifer Wortman Vaughan"
    ],
    "summary": "We consider the question of the stability of evolutionary algorithms to\ngradual changes, or drift, in the target concept. We define an algorithm to be\nresistant to drift if, for some inverse polynomial drift rate in the target\nfunction, it converges to accuracy 1 -- \\epsilon , with polynomial resources,\nand then stays within that accuracy indefinitely, except with probability\n\\epsilon , at any one time. We show that every evolution algorithm, in the\nsense of Valiant (2007; 2009), can be converted using the Correlational Query\ntechnique of Feldman (2008), into such a drift resistant algorithm. For certain\nevolutionary algorithms, such as for Boolean conjunctions, we give bounds on\nthe rates of drift that they can resist. We develop some new evolution\nalgorithms that are resistant to significant drift. In particular, we give an\nalgorithm for evolving linear separators over the spherically symmetric\ndistribution that is resistant to a drift rate of O(\\epsilon /n), and another\nalgorithm over the more general product normal distributions that resists a\nsmaller drift rate.\n  The above translation result can be also interpreted as one on the robustness\nof the notion of evolvability itself under changes of definition. As a second\nresult in that direction we show that every evolution algorithm can be\nconverted to a quasi-monotonic one that can evolve from any starting point\nwithout the performance ever dipping significantly below that of the starting\npoint. This permits the somewhat unnatural feature of arbitrary performance\ndegradations to be removed from several known robustness translations.",
    "published": "2010-05-19T22:58:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Kernel-Based Halfspaces with the Zero-One Loss",
    "authors": [
      "Shai Shalev-Shwartz",
      "Ohad Shamir",
      "Karthik Sridharan"
    ],
    "summary": "We describe and analyze a new algorithm for agnostically learning\nkernel-based halfspaces with respect to the \\emph{zero-one} loss function.\nUnlike most previous formulations which rely on surrogate convex loss functions\n(e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite\ntime/sample guarantees with respect to the more natural zero-one loss function.\nThe proposed algorithm can learn kernel-based halfspaces in worst-case time\n$\\poly(\\exp(L\\log(L/\\epsilon)))$, for $\\emph{any}$ distribution, where $L$ is a\nLipschitz constant (which can be thought of as the reciprocal of the margin),\nand the learned classifier is worse than the optimal halfspace by at most\n$\\epsilon$. We also prove a hardness result, showing that under a certain\ncryptographic assumption, no algorithm can learn kernel-based halfspaces in\ntime polynomial in $L$.",
    "published": "2010-05-20T12:39:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the clustering aspect of nonnegative matrix factorization",
    "authors": [
      "Andri Mirzal",
      "Masashi Furukawa"
    ],
    "summary": "This paper provides a theoretical explanation on the clustering aspect of\nnonnegative matrix factorization (NMF). We prove that even without imposing\northogonality nor sparsity constraint on the basis and/or coefficient matrix,\nNMF still can give clustering results, thus providing a theoretical support for\nmany works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority\nof the standard NMF as a clustering method.",
    "published": "2010-05-29T15:27:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-View Active Learning in the Non-Realizable Case",
    "authors": [
      "Wei Wang",
      "Zhi-Hua Zhou"
    ],
    "summary": "The sample complexity of active learning under the realizability assumption\nhas been well-studied. The realizability assumption, however, rarely holds in\npractice. In this paper, we theoretically characterize the sample complexity of\nactive learning in the non-realizable case under multi-view setting. We prove\nthat, with unbounded Tsybakov noise, the sample complexity of multi-view active\nlearning can be $\\widetilde{O}(\\log\\frac{1}{\\epsilon})$, contrasting to\nsingle-view setting where the polynomial improvement is the best possible\nachievement. We also prove that in general multi-view setting the sample\ncomplexity of active learning with unbounded Tsybakov noise is\n$\\widetilde{O}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is\nindependent of the parameter in Tsybakov noise, contrasting to previous\npolynomial bounds where the order of $1/\\epsilon$ is related to the parameter\nin Tsybakov noise.",
    "published": "2010-05-31T03:59:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction with Advice of Unknown Number of Experts",
    "authors": [
      "Alexey Chernov",
      "Vladimir Vovk"
    ],
    "summary": "In the framework of prediction with expert advice, we consider a recently\nintroduced kind of regret bounds: the bounds that depend on the effective\ninstead of nominal number of experts. In contrast to the NormalHedge bound,\nwhich mainly depends on the effective number of experts and also weakly depends\non the nominal one, we obtain a bound that does not contain the nominal number\nof experts at all. We use the defensive forecasting method and introduce an\napplication of defensive forecasting to multivalued supermartingales.",
    "published": "2010-06-02T19:41:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Predictive PAC learnability: a paradigm for learning from exchangeable\n  input data",
    "authors": [
      "Vladimir Pestov"
    ],
    "summary": "Exchangeable random variables form an important and well-studied\ngeneralization of i.i.d. variables, however simple examples show that no\nnontrivial concept or function classes are PAC learnable under general\nexchangeable data inputs $X_1,X_2,\\ldots$. Inspired by the work of Berti and\nRigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new\nparadigm, adequate for learning from exchangeable data: predictive PAC\nlearnability. A learning rule $\\mathcal L$ for a function class $\\mathscr F$ is\npredictive PAC if for every $\\e,\\delta>0$ and each function $f\\in {\\mathscr\nF}$, whenever $\\abs{\\sigma}\\geq s(\\delta,\\e)$, we have with confidence\n$1-\\delta$ that the expected difference between $f(X_{n+1})$ and the image of\n$f\\vert\\sigma$ under $\\mathcal L$ does not exceed $\\e$ conditionally on\n$X_1,X_2,\\ldots,X_n$. Thus, instead of learning the function $f$ as such, we\nare learning to a given accuracy $\\e$ the predictive behaviour of $f$ at the\nfuture points $X_i(\\omega)$, $i>n$ of the sample path. Using de Finetti's\ntheorem, we show that if a universally separable function class $\\mathscr F$ is\ndistribution-free PAC learnable under i.i.d. inputs, then it is\ndistribution-free predictive PAC learnable under exchangeable inputs, with a\nslightly worse sample complexity.",
    "published": "2010-06-06T18:21:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian\n  approach",
    "authors": [
      "Gilles Meyer",
      "Silvere Bonnabel",
      "Rodolphe Sepulchre"
    ],
    "summary": "The paper addresses the problem of learning a regression model parameterized\nby a fixed-rank positive semidefinite matrix. The focus is on the nonlinear\nnature of the search space and on scalability to high-dimensional problems. The\nmathematical developments rely on the theory of gradient descent algorithms\nadapted to the Riemannian geometry that underlies the set of fixed-rank\npositive semidefinite matrices. In contrast with previous contributions in the\nliterature, no restrictions are imposed on the range space of the learned\nmatrix. The resulting algorithms maintain a linear complexity in the problem\nsize and enjoy important invariance properties. We apply the proposed\nalgorithms to the problem of learning a distance function parameterized by a\npositive semidefinite matrix. Good performance is observed on classical\nbenchmarks.",
    "published": "2010-06-07T16:20:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dyadic Prediction Using a Latent Feature Log-Linear Model",
    "authors": [
      "Aditya Krishna Menon",
      "Charles Elkan"
    ],
    "summary": "In dyadic prediction, labels must be predicted for pairs (dyads) whose\nmembers possess unique identifiers and, sometimes, additional features called\nside-information. Special cases of this problem include collaborative filtering\nand link prediction. We present the first model for dyadic prediction that\nsatisfies several important desiderata: (i) labels may be ordinal or nominal,\n(ii) side-information can be easily exploited if present, (iii) with or without\nside-information, latent features are inferred for dyad members, (iv) it is\nresistant to sample-selection bias, (v) it can learn well-calibrated\nprobabilities, and (vi) it can scale to very large datasets. To our knowledge,\nno existing method satisfies all the above criteria. In particular, many\nmethods assume that the labels are ordinal and ignore side-information when it\nis present. Experimental results show that the new method is competitive with\nstate-of-the-art methods for the special cases of collaborative filtering and\nlink prediction, and that it makes accurate predictions on nominal data.",
    "published": "2010-06-10T21:19:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Agnostic Active Learning Without Constraints",
    "authors": [
      "Alina Beygelzimer",
      "Daniel Hsu",
      "John Langford",
      "Tong Zhang"
    ],
    "summary": "We present and analyze an agnostic active learning algorithm that works\nwithout keeping a version space. This is unlike all previous approaches where a\nrestricted set of candidate hypotheses is maintained throughout learning, and\nonly hypotheses from this set are ever returned. By avoiding this version space\napproach, our algorithm sheds the computational burden and brittleness\nassociated with maintaining version spaces, yet still allows for substantial\nimprovements over supervised learning for classification.",
    "published": "2010-06-14T02:03:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extension of Wirtinger's Calculus to Reproducing Kernel Hilbert Spaces\n  and the Complex Kernel LMS",
    "authors": [
      "Pantelis Bouboulis",
      "Sergios Theodoridis"
    ],
    "summary": "Over the last decade, kernel methods for nonlinear processing have\nsuccessfully been used in the machine learning community. The primary\nmathematical tool employed in these methods is the notion of the Reproducing\nKernel Hilbert Space. However, so far, the emphasis has been on batch\ntechniques. It is only recently, that online techniques have been considered in\nthe context of adaptive signal processing tasks. Moreover, these efforts have\nonly been focussed on real valued data sequences. To the best of our knowledge,\nno adaptive kernel-based strategy has been developed, so far, for complex\nvalued signals. Furthermore, although the real reproducing kernels are used in\nan increasing number of machine learning problems, complex kernels have not,\nyet, been used, in spite of their potential interest in applications that deal\nwith complex signals, with Communications being a typical example. In this\npaper, we present a general framework to attack the problem of adaptive\nfiltering of complex signals, using either real reproducing kernels, taking\nadvantage of a technique called \\textit{complexification} of real RKHSs, or\ncomplex reproducing kernels, highlighting the use of the complex gaussian\nkernel. In order to derive gradients of operators that need to be defined on\nthe associated complex RKHSs, we employ the powerful tool of Wirtinger's\nCalculus, which has recently attracted attention in the signal processing\ncommunity. To this end, in this paper, the notion of Wirtinger's calculus is\nextended, for the first time, to include complex RKHSs and use it to derive\nseveral realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm.\nExperiments verify that the CKLMS offers significant performance improvements\nover several linear and nonlinear algorithms, when dealing with nonlinearities.",
    "published": "2010-06-15T17:09:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "MINLIP for the Identification of Monotone Wiener Systems",
    "authors": [
      "Kristiaan Pelckmans"
    ],
    "summary": "This paper studies the MINLIP estimator for the identification of Wiener\nsystems consisting of a sequence of a linear FIR dynamical model, and a\nmonotonically increasing (or decreasing) static function. Given $T$\nobservations, this algorithm boils down to solving a convex quadratic program\nwith $O(T)$ variables and inequality constraints, implementing an inference\ntechnique which is based entirely on model complexity control. The resulting\nestimates of the linear submodel are found to be almost consistent when no\nnoise is present in the data, under a condition of smoothness of the true\nnonlinearity and local Persistency of Excitation (local PE) of the data. This\nresult is novel as it does not rely on classical tools as a 'linearization'\nusing a Taylor decomposition, nor exploits stochastic properties of the data.\nIt is indicated how to extend the method to cope with noisy data, and empirical\nevidence contrasts performance of the estimator against other recently proposed\ntechniques.",
    "published": "2010-06-24T16:42:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC learnability of a concept class under non-atomic measures: a problem\n  by Vidyasagar",
    "authors": [
      "Vladimir Pestov"
    ],
    "summary": "In response to a 1997 problem of M. Vidyasagar, we state a necessary and\nsufficient condition for distribution-free PAC learnability of a concept class\n$\\mathscr C$ under the family of all non-atomic (diffuse) measures on the\ndomain $\\Omega$. Clearly, finiteness of the classical Vapnik-Chervonenkis\ndimension of $\\mathscr C$ is a sufficient, but no longer necessary, condition.\nBesides, learnability of $\\mathscr C$ under non-atomic measures does not imply\nthe uniform Glivenko-Cantelli property with regard to non-atomic measures. Our\nlearnability criterion is stated in terms of a combinatorial parameter\n$\\VC({\\mathscr C}\\,{\\mathrm{mod}}\\,\\omega_1)$ which we call the VC dimension of\n$\\mathscr C$ modulo countable sets. The new parameter is obtained by\n``thickening up'' single points in the definition of VC dimension to\nuncountable ``clusters''. Equivalently, $\\VC(\\mathscr C\\modd\\omega_1)\\leq d$ if\nand only if every countable subclass of $\\mathscr C$ has VC dimension $\\leq d$\noutside a countable subset of $\\Omega$. The new parameter can be also expressed\nas the classical VC dimension of $\\mathscr C$ calculated on a suitable subset\nof a compactification of $\\Omega$. We do not make any measurability assumptions\non $\\mathscr C$, assuming instead the validity of Martin's Axiom (MA).",
    "published": "2010-06-26T01:44:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Latent Bernoulli-Gauss Model for Data Analysis",
    "authors": [
      "Amnon Shashua",
      "Gabi Pragier"
    ],
    "summary": "We present a new latent-variable model employing a Gaussian mixture\nintegrated with a feature selection procedure (the Bernoulli part of the model)\nwhich together form a \"Latent Bernoulli-Gauss\" distribution. The model is\napplied to MAP estimation, clustering, feature selection and collaborative\nfiltering and fares favorably with the state-of-the-art latent-variable models.",
    "published": "2010-07-05T11:46:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Filtrage vaste marge pour l'étiquetage séquentiel à noyaux de\n  signaux",
    "authors": [
      "Rémi Flamary",
      "Benjamin Labbé",
      "Alain Rakotomamonjy"
    ],
    "summary": "We address in this paper the problem of multi-channel signal sequence\nlabeling. In particular, we consider the problem where the signals are\ncontaminated by noise or may present some dephasing with respect to their\nlabels. For that, we propose to jointly learn a SVM sample classifier with a\ntemporal filtering of the channels. This will lead to a large margin filtering\nthat is adapted to the specificity of each channel (noise and time-lag). We\nderive algorithms to solve the optimization problem and we discuss different\nfilter regularizations for automated scaling or selection of channels. Our\napproach is tested on a non-linear toy example and on a BCI dataset. Results\nshow that the classification performance on these problems can be improved by\nlearning a large margin filtering.",
    "published": "2010-07-06T07:47:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A note on sample complexity of learning binary output neural networks\n  under fixed input distributions",
    "authors": [
      "Vladimir Pestov"
    ],
    "summary": "We show that the learning sample complexity of a sigmoidal neural network\nconstructed by Sontag (1992) required to achieve a given misclassification\nerror under a fixed purely atomic distribution can grow arbitrarily fast: for\nany prescribed rate of growth there is an input distribution having this rate\nas the sample complexity, and the bound is asymptotically tight. The rate can\nbe superexponential, a non-recursive function, etc. We further observe that\nSontag's ANN is not Glivenko-Cantelli under any input distribution having a\nnon-atomic part.",
    "published": "2010-07-08T03:58:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reinforcement Learning via AIXI Approximation",
    "authors": [
      "Joel Veness",
      "Kee Siong Ng",
      "Marcus Hutter",
      "David Silver"
    ],
    "summary": "This paper introduces a principled approach for the design of a scalable\ngeneral reinforcement learning agent. This approach is based on a direct\napproximation of AIXI, a Bayesian optimality notion for general reinforcement\nlearning agents. Previously, it has been unclear whether the theory of AIXI\ncould motivate the design of practical algorithms. We answer this hitherto open\nquestion in the affirmative, by providing the first computationally feasible\napproximation to the AIXI agent. To develop our approximation, we introduce a\nMonte Carlo Tree Search algorithm along with an agent-specific extension of the\nContext Tree Weighting algorithm. Empirically, we present a set of encouraging\nresults on a number of stochastic, unknown, and partially observable domains.",
    "published": "2010-07-13T08:48:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adapting to the Shifting Intent of Search Queries",
    "authors": [
      "Umar Syed",
      "Aleksandrs Slivkins",
      "Nina Mishra"
    ],
    "summary": "Search engines today present results that are often oblivious to abrupt\nshifts in intent. For example, the query `independence day' usually refers to a\nUS holiday, but the intent of this query abruptly changed during the release of\na major film by that name. While no studies exactly quantify the magnitude of\nintent-shifting traffic, studies suggest that news events, seasonal topics, pop\nculture, etc account for 50% of all search queries. This paper shows that the\nsignals a search engine receives can be used to both determine that a shift in\nintent has happened, as well as find a result that is now more relevant. We\npresent a meta-algorithm that marries a classifier with a bandit algorithm to\nachieve regret that depends logarithmically on the number of query impressions,\nunder certain assumptions. We provide strong evidence that this regret is close\nto the best achievable. Finally, via a series of experiments, we demonstrate\nthat our algorithm outperforms prior approaches, particularly as the amount of\nintent-shifting traffic increases.",
    "published": "2010-07-22T04:58:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comparison of Support Vector Machine and Back Propagation Neural Network\n  in Evaluating the Enterprise Financial Distress",
    "authors": [
      "Ming-Chang Lee",
      "Chang To"
    ],
    "summary": "Recently, applying the novel data mining techniques for evaluating enterprise\nfinancial distress has received much research alternation. Support Vector\nMachine (SVM) and back propagation neural (BPN) network has been applied\nsuccessfully in many areas with excellent generalization results, such as rule\nextraction, classification and evaluation. In this paper, a model based on SVM\nwith Gaussian RBF kernel is proposed here for enterprise financial distress\nevaluation. BPN network is considered one of the simplest and are most general\nmethods used for supervised training of multilayered neural network. The\ncomparative results show that through the difference between the performance\nmeasures is marginal; SVM gives higher precision and lower error rates.",
    "published": "2010-07-29T07:36:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Close Clustering Based Automated Color Image Annotation",
    "authors": [
      "Ankit Garg",
      "Rahul Dwivedi",
      "Krishna Asawa"
    ],
    "summary": "Most image-search approaches today are based on the text based tags\nassociated with the images which are mostly human generated and are subject to\nvarious kinds of errors. The results of a query to the image database thus can\noften be misleading and may not satisfy the requirements of the user. In this\nwork we propose our approach to automate this tagging process of images, where\nimage results generated can be fine filtered based on a probabilistic tagging\nmechanism. We implement a tool which helps to automate the tagging process by\nmaintaining a training database, wherein the system is trained to identify\ncertain set of input images, the results generated from which are used to\ncreate a probabilistic tagging mechanism. Given a certain set of segments in an\nimage it calculates the probability of presence of particular keywords. This\nprobability table is further used to generate the candidate tags for input\nimages.",
    "published": "2010-08-02T16:30:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounded Coordinate-Descent for Biological Sequence Classification in\n  High Dimensional Predictor Space",
    "authors": [
      "Georgiana Ifrim",
      "Carsten Wiuf"
    ],
    "summary": "We present a framework for discriminative sequence classification where the\nlearner works directly in the high dimensional predictor space of all\nsubsequences in the training set. This is possible by employing a new\ncoordinate-descent algorithm coupled with bounding the magnitude of the\ngradient for selecting discriminative subsequences fast. We characterize the\nloss functions for which our generic learning algorithm can be applied and\npresent concrete implementations for logistic regression (binomial\nlog-likelihood loss) and support vector machines (squared hinge loss).\nApplication of our algorithm to protein remote homology detection and remote\nfold recognition results in performance comparable to that of state-of-the-art\nmethods (e.g., kernel support vector machines). Unlike state-of-the-art\nclassifiers, the resulting classification models are simply lists of weighted\ndiscriminative subsequences and can thus be interpreted and related to the\nbiological problem.",
    "published": "2010-08-03T12:10:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Semi-Supervised Kernel PCA",
    "authors": [
      "Christian Walder",
      "Ricardo Henao",
      "Morten Mørup",
      "Lars Kai Hansen"
    ],
    "summary": "We present three generalisations of Kernel Principal Components Analysis\n(KPCA) which incorporate knowledge of the class labels of a subset of the data\npoints. The first, MV-KPCA, penalises within class variances similar to Fisher\ndiscriminant analysis. The second, LSKPCA is a hybrid of least squares\nregression and kernel PCA. The final LR-KPCA is an iteratively reweighted\nversion of the previous which achieves a sigmoid loss function on the labeled\npoints. We provide a theoretical risk bound as well as illustrative experiments\non real and toy data sets.",
    "published": "2010-08-08T11:25:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning in Case of Unbounded Losses Using the Follow Perturbed\n  Leader Algorithm",
    "authors": [
      "Vladimir V. V'yugin"
    ],
    "summary": "In this paper the sequential prediction problem with expert advice is\nconsidered for the case where losses of experts suffered at each step cannot be\nbounded in advance. We present some modification of Kalai and Vempala algorithm\nof following the perturbed leader where weights depend on past losses of the\nexperts. New notions of a volume and a scaled fluctuation of a game are\nintroduced. We present a probabilistic algorithm protected from unrestrictedly\nlarge one-step losses. This algorithm has the optimal performance in the case\nwhen the scaled fluctuations of one-step losses of experts of the pool tend to\nzero.",
    "published": "2010-08-25T09:09:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Switching between Hidden Markov Models using Fixed Share",
    "authors": [
      "Wouter M. Koolen",
      "Tim van Erven"
    ],
    "summary": "In prediction with expert advice the goal is to design online prediction\nalgorithms that achieve small regret (additional loss on the whole data)\ncompared to a reference scheme. In the simplest such scheme one compares to the\nloss of the best expert in hindsight. A more ambitious goal is to split the\ndata into segments and compare to the best expert on each segment. This is\nappropriate if the nature of the data changes between segments. The standard\nfixed-share algorithm is fast and achieves small regret compared to this\nscheme.\n  Fixed share treats the experts as black boxes: there are no assumptions about\nhow they generate their predictions. But if the experts are learning, the\nfollowing question arises: should the experts learn from all data or only from\ndata in their own segment? The original algorithm naturally addresses the first\ncase. Here we consider the second option, which is more appropriate exactly\nwhen the nature of the data changes between segments. In general extending\nfixed share to this second case will slow it down by a factor of T on T\noutcomes. We show, however, that no such slowdown is necessary if the experts\nare hidden Markov models.",
    "published": "2010-08-26T15:36:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Freezing and Sleeping: Tracking Experts that Learn by Evolving Past\n  Posteriors",
    "authors": [
      "Wouter M. Koolen",
      "Tim van Erven"
    ],
    "summary": "A problem posed by Freund is how to efficiently track a small pool of experts\nout of a much larger set. This problem was solved when Bousquet and Warmuth\nintroduced their mixing past posteriors (MPP) algorithm in 2001.\n  In Freund's problem the experts would normally be considered black boxes.\nHowever, in this paper we re-examine Freund's problem in case the experts have\ninternal structure that enables them to learn. In this case the problem has two\npossible interpretations: should the experts learn from all data or only from\nthe subsequence on which they are being tracked? The MPP algorithm solves the\nfirst case. Our contribution is to generalise MPP to address the second option.\nThe results we obtain apply to any expert structure that can be formalised\nusing (expert) hidden Markov models. Curiously enough, for our interpretation\nthere are \\emph{two} natural reference schemes: freezing and sleeping. For each\nscheme, we provide an efficient prediction strategy and prove the relevant loss\nbound.",
    "published": "2010-08-27T06:53:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploring Language-Independent Emotional Acoustic Features via Feature\n  Selection",
    "authors": [
      "Arslan Shaukat",
      "Ke Chen"
    ],
    "summary": "We propose a novel feature selection strategy to discover\nlanguage-independent acoustic features that tend to be responsible for emotions\nregardless of languages, linguistics and other factors. Experimental results\nsuggest that the language-independent feature subset discovered yields the\nperformance comparable to the full feature set on various emotional speech\ncorpora.",
    "published": "2010-09-01T08:29:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Overlapping Group Lasso",
    "authors": [
      "Jun Liu",
      "Jieping Ye"
    ],
    "summary": "The group Lasso is an extension of the Lasso for feature selection on\n(predefined) non-overlapping groups of features. The non-overlapping group\nstructure limits its applicability in practice. There have been several recent\nattempts to study a more general formulation, where groups of features are\ngiven, potentially with overlaps between the groups. The resulting optimization\nis, however, much more challenging to solve due to the group overlaps. In this\npaper, we consider the efficient optimization of the overlapping group Lasso\npenalized problem. We reveal several key properties of the proximal operator\nassociated with the overlapping group Lasso, and compute the proximal operator\nby solving the smooth and convex dual problem, which allows the use of the\ngradient descent type of algorithms for the optimization. We have performed\nempirical evaluations using the breast cancer gene expression data set, which\nconsists of 8,141 genes organized into (overlapping) gene sets. Experimental\nresults demonstrate the efficiency and effectiveness of the proposed algorithm.",
    "published": "2010-09-02T00:25:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reinforcement Learning by Comparing Immediate Reward",
    "authors": [
      "Punit Pandey",
      "Deepshikha Pandey",
      "Shishir Kumar"
    ],
    "summary": "This paper introduces an approach to Reinforcement Learning Algorithm by\ncomparing their immediate rewards using a variation of Q-Learning algorithm.\nUnlike the conventional Q-Learning, the proposed algorithm compares current\nreward with immediate reward of past move and work accordingly. Relative reward\nbased Q-learning is an approach towards interactive learning. Q-Learning is a\nmodel free reinforcement learning method that used to learn the agents. It is\nobserved that under normal circumstances algorithm take more episodes to reach\noptimal Q-value due to its normal reward or sometime negative reward. In this\nnew form of algorithm agents select only those actions which have a higher\nimmediate reward signal in comparison to previous one. The contribution of this\narticle is the presentation of new Q-Learning Algorithm in order to maximize\nthe performance of algorithm and reduce the number of episode required to reach\noptimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20\nGrid world deterministic environment and the result for the two forms of\nQ-Learning Algorithms is given.",
    "published": "2010-09-14T03:53:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Unified View of Regularized Dual Averaging and Mirror Descent with\n  Implicit Updates",
    "authors": [
      "H. Brendan McMahan"
    ],
    "summary": "We study three families of online convex optimization algorithms:\nfollow-the-proximally-regularized-leader (FTRL-Proximal), regularized dual\naveraging (RDA), and composite-objective mirror descent. We first prove\nequivalence theorems that show all of these algorithms are instantiations of a\ngeneral FTRL update. This provides theoretical insight on previous experimental\nobservations. In particular, even though the FOBOS composite mirror descent\nalgorithm handles L1 regularization explicitly, it has been observed that RDA\nis even more effective at producing sparsity. Our results demonstrate that\nFOBOS uses subgradient approximations to the L1 penalty from previous rounds,\nleading to less sparsity than RDA, which handles the cumulative penalty in\nclosed form. The FTRL-Proximal algorithm can be seen as a hybrid of these two,\nand outperforms both on a large, real-world dataset.\n  Our second contribution is a unified analysis which produces regret bounds\nthat match (up to logarithmic terms) or improve the best previously known\nbounds. This analysis also extends these algorithms in two important ways: we\nsupport a more general type of composite objective and we analyze implicit\nupdates, which replace the subgradient approximation of the current loss\nfunction with an exact optimization.",
    "published": "2010-09-16T18:40:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Conditional Random Fields and Support Vector Machines: A Hybrid Approach",
    "authors": [
      "Qinfeng Shi",
      "Mark D. Reid",
      "Tiberio Caetano"
    ],
    "summary": "We propose a novel hybrid loss for multiclass and structured prediction\nproblems that is a convex combination of log loss for Conditional Random Fields\n(CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We\nprovide a sufficient condition for when the hybrid loss is Fisher consistent\nfor classification. This condition depends on a measure of dominance between\nlabels - specifically, the gap in per observation probabilities between the\nmost likely labels. We also prove Fisher consistency is necessary for\nparametric consistency when learning models such as CRFs.\n  We demonstrate empirically that the hybrid loss typically performs as least\nas well as - and often better than - both of its constituent losses on variety\nof tasks. In doing so we also provide an empirical comparison of the efficacy\nof probabilistic and margin based approaches to multiclass and structured\nprediction and the effects of label dominance on these results.",
    "published": "2010-09-17T06:47:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Geometric Decision Tree",
    "authors": [
      "Naresh Manwani",
      "P. S. Sastry"
    ],
    "summary": "In this paper we present a new algorithm for learning oblique decision trees.\nMost of the current decision tree algorithms rely on impurity measures to\nassess the goodness of hyperplanes at each node while learning a decision tree\nin a top-down fashion. These impurity measures do not properly capture the\ngeometric structures in the data. Motivated by this, our algorithm uses a\nstrategy to assess the hyperplanes in such a way that the geometric structure\nin the data is taken into account. At each node of the decision tree, we find\nthe clustering hyperplanes for both the classes and use their angle bisectors\nas the split rule at that node. We show through empirical studies that this\nidea leads to small decision trees and better performance. We also present some\nanalysis to show that the angle bisectors of clustering hyperplanes that we use\nas the split rules at each node, are solutions of an interesting optimization\nproblem and hence argue that this is a principled method of learning a decision\ntree.",
    "published": "2010-09-19T03:54:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Doubt about Margin Explanation of Boosting",
    "authors": [
      "Wei Gao",
      "Zhi-Hua Zhou"
    ],
    "summary": "Margin theory provides one of the most popular explanations to the success of\n\\texttt{AdaBoost}, where the central point lies in the recognition that\n\\textit{margin} is the key for characterizing the performance of\n\\texttt{AdaBoost}. This theory has been very influential, e.g., it has been\nused to argue that \\texttt{AdaBoost} usually does not overfit since it tends to\nenlarge the margin even after the training error reaches zero. Previously the\n\\textit{minimum margin bound} was established for \\texttt{AdaBoost}, however,\n\\cite{Breiman1999} pointed out that maximizing the minimum margin does not\nnecessarily lead to a better generalization. Later, \\cite{Reyzin:Schapire2006}\nemphasized that the margin distribution rather than minimum margin is crucial\nto the performance of \\texttt{AdaBoost}. In this paper, we first present the\n\\textit{$k$th margin bound} and further study on its relationship to previous\nwork such as the minimum margin bound and Emargin bound. Then, we improve the\nprevious empirical Bernstein bounds\n\\citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on such\nfindings, we defend the margin-based explanation against Breiman's doubts by\nproving a new generalization error bound that considers exactly the same\nfactors as \\cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than\n\\cite{Breiman1999}'s minimum margin bound. By incorporating factors such as\naverage margin and variance, we present a generalization error bound that is\nheavily related to the whole margin distribution. We also provide margin\ndistribution bounds for generalization error of voting classifiers in finite\nVC-dimension space.",
    "published": "2010-09-19T07:26:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Totally Corrective Multiclass Boosting with Binary Weak Learners",
    "authors": [
      "Zhihui Hao",
      "Chunhua Shen",
      "Nick Barnes",
      "Bo Wang"
    ],
    "summary": "In this work, we propose a new optimization framework for multiclass boosting\nlearning. In the literature, AdaBoost.MO and AdaBoost.ECC are the two\nsuccessful multiclass boosting algorithms, which can use binary weak learners.\nWe explicitly derive these two algorithms' Lagrange dual problems based on\ntheir regularized loss functions. We show that the Lagrange dual formulations\nenable us to design totally-corrective multiclass algorithms by using the\nprimal-dual optimization technique. Experiments on benchmark data sets suggest\nthat our multiclass boosting can achieve a comparable generalization capability\nwith state-of-the-art, but the convergence speed is much faster than stage-wise\ngradient descent boosting. In other words, the new totally corrective\nalgorithms can maximize the margin more aggressively.",
    "published": "2010-09-20T06:35:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimistic Rates for Learning with a Smooth Loss",
    "authors": [
      "Nathan Srebro",
      "Karthik Sridharan",
      "Ambuj Tewari"
    ],
    "summary": "We establish an excess risk bound of O(H R_n^2 + R_n \\sqrt{H L*}) for\nempirical risk minimization with an H-smooth loss function and a hypothesis\nclass with Rademacher complexity R_n, where L* is the best risk achievable by\nthe hypothesis class. For typical hypothesis classes where R_n = \\sqrt{R/n},\nthis translates to a learning rate of O(RH/n) in the separable (L*=0) case and\nO(RH/n + \\sqrt{L^* RH/n}) more generally. We also provide similar guarantees\nfor online and stochastic convex optimization with a smooth non-negative\nobjective.",
    "published": "2010-09-20T17:35:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient L1/Lq Norm Regularization",
    "authors": [
      "Jun Liu",
      "Jieping Ye"
    ],
    "summary": "Sparse learning has recently received increasing attention in many areas\nincluding machine learning, statistics, and applied mathematics. The mixed-norm\nregularization based on the L1/Lq norm with q > 1 is attractive in many\napplications of regression and classification in that it facilitates group\nsparsity in the model. The resulting optimization problem is, however,\nchallenging to solve due to the structure of the L1/Lq -regularization.\nExisting work deals with special cases including q = 2,infinity, and they\ncannot be easily extended to the general case. In this paper, we propose an\nefficient algorithm based on the accelerated gradient method for solving the\nL1/Lq -regularized problem, which is applicable for all values of q larger than\n1, thus significantly extending existing work. One key building block of the\nproposed algorithm is the L1/Lq -regularized Euclidean projection (EP1q). Our\ntheoretical analysis reveals the key properties of EP1q and illustrates why\nEP1q for the general q is significantly more challenging to solve than the\nspecial cases. Based on our theoretical analysis, we develop an efficient\nalgorithm for EP1q by solving two zero finding problems. Experimental results\ndemonstrate the efficiency of the proposed algorithm.",
    "published": "2010-09-24T05:53:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-parametric Solution-path Algorithm for Instance-weighted Support\n  Vector Machines",
    "authors": [
      "Masayuki Karasuyama",
      "Naoyuki Harada",
      "Masashi Sugiyama",
      "Ichiro Takeuchi"
    ],
    "summary": "An instance-weighted variant of the support vector machine (SVM) has\nattracted considerable attention recently since they are useful in various\nmachine learning tasks such as non-stationary data analysis, heteroscedastic\ndata modeling, transfer learning, learning to rank, and transduction. An\nimportant challenge in these scenarios is to overcome the computational\nbottleneck---instance weights often change dynamically or adaptively, and thus\nthe weighted SVM solutions must be repeatedly computed. In this paper, we\ndevelop an algorithm that can efficiently and exactly update the weighted SVM\nsolutions for arbitrary change of instance weights. Technically, this\ncontribution can be regarded as an extension of the conventional solution-path\nalgorithm for a single regularization parameter to multiple instance-weight\nparameters. However, this extension gives rise to a significant problem that\nbreakpoints (at which the solution path turns) have to be identified in\nhigh-dimensional space. To facilitate this, we introduce a parametric\nrepresentation of instance weights. We also provide a geometric interpretation\nin weight space using a notion of critical region: a polyhedron in which the\ncurrent affine solution remains to be optimal. Then we find breakpoints at\nintersections of the solution path and boundaries of polyhedrons. Through\nextensive experiments on various practical applications, we demonstrate the\nusefulness of the proposed algorithm.",
    "published": "2010-09-24T09:53:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Portfolio Allocation for Bayesian Optimization",
    "authors": [
      "Eric Brochu",
      "Matthew W. Hoffman",
      "Nando de Freitas"
    ],
    "summary": "Bayesian optimization with Gaussian processes has become an increasingly\npopular tool in the machine learning community. It is efficient and can be used\nwhen very little is known about the objective function, making it popular in\nexpensive black-box optimization scenarios. It uses Bayesian methods to sample\nthe objective efficiently using an acquisition function which incorporates the\nmodel's estimate of the objective and the uncertainty at any given point.\nHowever, there are several different parameterized acquisition functions in the\nliterature, and it is often unclear which one to use. Instead of using a single\nacquisition function, we adopt a portfolio of acquisition functions governed by\nan online multi-armed bandit strategy. We propose several portfolio strategies,\nthe best of which we call GP-Hedge, and show that this method outperforms the\nbest individual acquisition function. We also provide a theoretical bound on\nthe algorithm's performance.",
    "published": "2010-09-28T00:41:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Reinforcement Learning for Energy-Efficient Wireless Communications",
    "authors": [
      "Nicholas Mastronarde",
      "Mihaela van der Schaar"
    ],
    "summary": "We consider the problem of energy-efficient point-to-point transmission of\ndelay-sensitive data (e.g. multimedia data) over a fading channel. Existing\nresearch on this topic utilizes either physical-layer centric solutions, namely\npower-control and adaptive modulation and coding (AMC), or system-level\nsolutions based on dynamic power management (DPM); however, there is currently\nno rigorous and unified framework for simultaneously utilizing both\nphysical-layer centric and system-level techniques to achieve the minimum\npossible energy consumption, under delay constraints, in the presence of\nstochastic and a priori unknown traffic and channel conditions. In this report,\nwe propose such a framework. We formulate the stochastic optimization problem\nas a Markov decision process (MDP) and solve it online using reinforcement\nlearning. The advantages of the proposed online method are that (i) it does not\nrequire a priori knowledge of the traffic arrival and channel statistics to\ndetermine the jointly optimal power-control, AMC, and DPM policies; (ii) it\nexploits partial information about the system so that less information needs to\nbe learned than when using conventional reinforcement learning algorithms; and\n(iii) it obviates the need for action exploration, which severely limits the\nadaptation speed and run-time performance of conventional reinforcement\nlearning algorithms. Our results show that the proposed learning algorithms can\nconverge up to two orders of magnitude faster than a state-of-the-art learning\nalgorithm for physical layer power-control and up to three orders of magnitude\nfaster than conventional reinforcement learning algorithms.",
    "published": "2010-09-29T05:23:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Attentive Perceptron",
    "authors": [
      "Raphael Pelossof",
      "Zhiliang Ying"
    ],
    "summary": "We propose a focus of attention mechanism to speed up the Perceptron\nalgorithm. Focus of attention speeds up the Perceptron algorithm by lowering\nthe number of features evaluated throughout training and prediction. Whereas\nthe traditional Perceptron evaluates all the features of each example, the\nAttentive Perceptron evaluates less features for easy to classify examples,\nthereby achieving significant speedups and small losses in prediction accuracy.\nFocus of attention allows the Attentive Perceptron to stop the evaluation of\nfeatures at any interim point and filter the example. This creates an attentive\nfilter which concentrates computation at examples that are hard to classify,\nand quickly filters examples that are easy to classify.",
    "published": "2010-09-29T18:55:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Queue-Aware Distributive Resource Control for Delay-Sensitive Two-Hop\n  MIMO Cooperative Systems",
    "authors": [
      "Rui Wang",
      "Vincent K. N. Lau",
      "Ying Cui"
    ],
    "summary": "In this paper, we consider a queue-aware distributive resource control\nalgorithm for two-hop MIMO cooperative systems. We shall illustrate that relay\nbuffering is an effective way to reduce the intrinsic half-duplex penalty in\ncooperative systems. The complex interactions of the queues at the source node\nand the relays are modeled as an average-cost infinite horizon Markov Decision\nProcess (MDP). The traditional approach solving this MDP problem involves\ncentralized control with huge complexity. To obtain a distributive and low\ncomplexity solution, we introduce a linear structure which approximates the\nvalue function of the associated Bellman equation by the sum of per-node value\nfunctions. We derive a distributive two-stage two-winner auction-based control\npolicy which is a function of the local CSI and local QSI only. Furthermore, to\nestimate the best fit approximation parameter, we propose a distributive online\nstochastic learning algorithm using stochastic approximation theory. Finally,\nwe establish technical conditions for almost-sure convergence and show that\nunder heavy traffic, the proposed low complexity distributive control is global\noptimal.",
    "published": "2010-10-02T03:57:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Time Series Classification by Class-Specific Mahalanobis Distance\n  Measures",
    "authors": [
      "Zoltán Prekopcsák",
      "Daniel Lemire"
    ],
    "summary": "To classify time series by nearest neighbors, we need to specify or learn one\nor several distance measures. We consider variations of the Mahalanobis\ndistance measures which rely on the inverse covariance matrix of the data.\nUnfortunately --- for time series data --- the covariance matrix has often low\nrank. To alleviate this problem we can either use a pseudoinverse, covariance\nshrinking or limit the matrix to its diagonal. We review these alternatives and\nbenchmark them against competitive methods such as the related Large Margin\nNearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW)\ndistance. As we expected, we find that the DTW is superior, but the Mahalanobis\ndistance measures are one to two orders of magnitude faster. To get best\nresults with Mahalanobis distance measures, we recommend learning one distance\nmeasure per class using either covariance shrinking or the diagonal approach.",
    "published": "2010-10-07T19:48:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Algorithms for nonnegative matrix factorization with the beta-divergence",
    "authors": [
      "Cédric Févotte",
      "Jérôme Idier"
    ],
    "summary": "This paper describes algorithms for nonnegative matrix factorization (NMF)\nwith the beta-divergence (beta-NMF). The beta-divergence is a family of cost\nfunctions parametrized by a single shape parameter beta that takes the\nEuclidean distance, the Kullback-Leibler divergence and the Itakura-Saito\ndivergence as special cases (beta = 2,1,0, respectively). The proposed\nalgorithms are based on a surrogate auxiliary function (a local majorization of\nthe criterion function). We first describe a majorization-minimization (MM)\nalgorithm that leads to multiplicative updates, which differ from standard\nheuristic multiplicative updates by a beta-dependent power exponent. The\nmonotonicity of the heuristic algorithm can however be proven for beta in (0,1)\nusing the proposed auxiliary function. Then we introduce the concept of\nmajorization-equalization (ME) algorithm which produces updates that move along\nconstant level sets of the auxiliary function and lead to larger steps than MM.\nSimulations on synthetic and real data illustrate the faster convergence of the\nME approach. The paper also describes how the proposed algorithms can be\nadapted to two common variants of NMF : penalized NMF (i.e., when a penalty\nfunction of the factors is added to the criterion function) and convex-NMF\n(when the dictionary is assumed to belong to a known subspace).",
    "published": "2010-10-08T18:53:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hardness Results for Agnostically Learning Low-Degree Polynomial\n  Threshold Functions",
    "authors": [
      "Ilias Diakonikolas",
      "Ryan O'Donnell",
      "Rocco A. Servedio",
      "Yi Wu"
    ],
    "summary": "Hardness results for maximum agreement problems have close connections to\nhardness results for proper learning in computational learning theory. In this\npaper we prove two hardness results for the problem of finding a low degree\npolynomial threshold function (PTF) which has the maximum possible agreement\nwith a given set of labeled examples in $\\R^n \\times \\{-1,1\\}.$ We prove that\nfor any constants $d\\geq 1, \\eps > 0$,\n  {itemize}\n  Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a\ndegree-$d$ PTF that is consistent with a $(\\half + \\eps)$ fraction of a given\nset of labeled examples in $\\R^n \\times \\{-1,1\\}$, even if there exists a\ndegree-$d$ PTF that is consistent with a $1-\\eps$ fraction of the examples.\n  It is $\\NP$-hard to find a degree-2 PTF that is consistent with a $(\\half +\n\\eps)$ fraction of a given set of labeled examples in $\\R^n \\times \\{-1,1\\}$,\neven if there exists a halfspace (degree-1 PTF) that is consistent with a $1 -\n\\eps$ fraction of the examples.\n  {itemize}\n  These results immediately imply the following hardness of learning results:\n(i) Assuming the Unique Games Conjecture, there is no better-than-trivial\nproper learning algorithm that agnostically learns degree-$d$ PTFs under\narbitrary distributions; (ii) There is no better-than-trivial learning\nalgorithm that outputs degree-2 PTFs and agnostically learns halfspaces (i.e.\ndegree-1 PTFs) under arbitrary distributions.",
    "published": "2010-10-18T05:46:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Matrix Completion with Gaussian Models",
    "authors": [
      "Flavien Léger",
      "Guoshen Yu",
      "Guillermo Sapiro"
    ],
    "summary": "A general framework based on Gaussian models and a MAP-EM algorithm is\nintroduced in this paper for solving matrix/table completion problems. The\nnumerical experiments with the standard and challenging movie ratings data show\nthat the proposed approach, based on probably one of the simplest probabilistic\nmodels, leads to the results in the same ballpark as the state-of-the-art, at a\nlower computational cost.",
    "published": "2010-10-19T21:01:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-Scale Clustering Based on Data Compression",
    "authors": [
      "Xudong Ma"
    ],
    "summary": "This paper considers the clustering problem for large data sets. We propose\nan approach based on distributed optimization. The clustering problem is\nformulated as an optimization problem of maximizing the classification gain. We\nshow that the optimization problem can be reformulated and decomposed into\nsmall-scale sub optimization problems by using the Dantzig-Wolfe decomposition\nmethod. Generally speaking, the Dantzig-Wolfe method can only be used for\nconvex optimization problems, where the duality gaps are zero. Even though, the\nconsidered optimization problem in this paper is non-convex, we prove that the\nduality gap goes to zero, as the problem size goes to infinity. Therefore, the\nDantzig-Wolfe method can be applied here. In the proposed approach, the\nclustering problem is iteratively solved by a group of computers coordinated by\none center processor, where each computer solves one independent small-scale\nsub optimization problem during each iteration, and only a small amount of data\ncommunication is needed between the computers and center processor. Numerical\nresults show that the proposed approach is effective and efficient.",
    "published": "2010-10-20T17:21:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sublinear Optimization for Machine Learning",
    "authors": [
      "Kenneth L. Clarkson",
      "Elad Hazan",
      "David P. Woodruff"
    ],
    "summary": "We give sublinear-time approximation algorithms for some optimization\nproblems arising in machine learning, such as training linear classifiers and\nfinding minimum enclosing balls. Our algorithms can be extended to some\nkernelized versions of these problems, such as SVDD, hard margin SVM, and\nL2-SVM, for which sublinear-time algorithms were not known before. These new\nalgorithms use a combination of a novel sampling techniques and a new\nmultiplicative update algorithm. We give lower bounds which show the running\ntimes of many of our algorithms to be nearly best possible in the unit-cost RAM\nmodel. We also give implementations of our algorithms in the semi-streaming\nsetting, obtaining the first low pass polylogarithmic space and sublinear time\nalgorithms achieving arbitrary approximation factor.",
    "published": "2010-10-21T09:57:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Regularized Risk Minimization by Nesterov's Accelerated Gradient\n  Methods: Algorithmic Extensions and Empirical Studies",
    "authors": [
      "Xinhua Zhang",
      "Ankan Saha",
      "S. V. N. Vishwanathan"
    ],
    "summary": "Nesterov's accelerated gradient methods (AGM) have been successfully applied\nin many machine learning areas. However, their empirical performance on\ntraining max-margin models has been inferior to existing specialized solvers.\nIn this paper, we first extend AGM to strongly convex and composite objective\nfunctions with Bregman style prox-functions. Our unifying framework covers both\nthe $\\infty$-memory and 1-memory styles of AGM, tunes the Lipschiz constant\nadaptively, and bounds the duality gap. Then we demonstrate various ways to\napply this framework of methods to a wide range of machine learning problems.\nEmphasis will be given on their rate of convergence and how to efficiently\ncompute the gradient and optimize the models. The experimental results show\nthat with our extensions AGM outperforms state-of-the-art solvers on max-margin\nmodels.",
    "published": "2010-11-01T23:41:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Importance Weight Aware Updates",
    "authors": [
      "Nikos Karampatziakis",
      "John Langford"
    ],
    "summary": "An importance weight quantifies the relative importance of one example over\nanother, coming up in applications of boosting, asymmetric classification\ncosts, reductions, and active learning. The standard approach for dealing with\nimportance weights in gradient descent is via multiplication of the gradient.\nWe first demonstrate the problems of this approach when importance weights are\nlarge, and argue in favor of more sophisticated ways for dealing with them. We\nthen develop an approach which enjoys an invariance property: that updating\ntwice with importance weight $h$ is equivalent to updating once with importance\nweight $2h$. For many important losses this has a closed form update which\nsatisfies standard regret guarantees when all examples have $h=1$. We also\nbriefly discuss two other reasonable approaches for handling large importance\nweights. Empirically, these approaches yield substantially superior prediction\nwith similar computational performance while reducing the sensitivity of the\nalgorithm to the exact setting of the learning rate. We apply these to online\nactive learning yielding an extraordinarily fast active learning algorithm that\nworks even in the presence of adversarial noise.",
    "published": "2010-11-06T18:40:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Theorem 2.3 in \"Prediction, Learning, and Games\" by Cesa-Bianchi and\n  Lugosi",
    "authors": [
      "Alexey Chernov"
    ],
    "summary": "The note presents a modified proof of a loss bound for the exponentially\nweighted average forecaster with time-varying potential. The regret term of the\nalgorithm is upper-bounded by sqrt{n ln(N)} (uniformly in n), where N is the\nnumber of experts and n is the number of steps.",
    "published": "2010-11-25T18:52:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Estimating Probabilities in Recommendation Systems",
    "authors": [
      "Mingxuan Sun",
      "Guy Lebanon",
      "Paul Kidwell"
    ],
    "summary": "Recommendation systems are emerging as an important business application with\nsignificant economic impact. Currently popular systems include Amazon's book\nrecommendations, Netflix's movie recommendations, and Pandora's music\nrecommendations. In this paper we address the problem of estimating\nprobabilities associated with recommendation system data using non-parametric\nkernel smoothing. In our estimation we interpret missing items as randomly\ncensored observations and obtain efficient computation schemes using\ncombinatorial properties of generating functions. We demonstrate our approach\nwith several case studies involving real world movie recommendation data. The\nresults are comparable with state-of-the-art techniques while also providing\nprobabilistic preference estimates outside the scope of traditional recommender\nsystems.",
    "published": "2010-12-02T17:04:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with\n  Application to Active User Modeling and Hierarchical Reinforcement Learning",
    "authors": [
      "Eric Brochu",
      "Vlad M. Cora",
      "Nando de Freitas"
    ],
    "summary": "We present a tutorial on Bayesian optimization, a method of finding the\nmaximum of expensive cost functions. Bayesian optimization employs the Bayesian\ntechnique of setting a prior over the objective function and combining it with\nevidence to get a posterior function. This permits a utility-based selection of\nthe next observation to make on the objective function, which must take into\naccount both exploration (sampling from areas of high uncertainty) and\nexploitation (sampling areas likely to offer improvement over the current best\nobservation). We also present two detailed extensions of Bayesian optimization,\nwith experiments---active user modelling with preferences, and hierarchical\nreinforcement learning---and a discussion of the pros and cons of Bayesian\noptimization based on our experiences.",
    "published": "2010-12-12T22:53:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Queue-Aware Dynamic Clustering and Power Allocation for Network MIMO\n  Systems via Distributive Stochastic Learning",
    "authors": [
      "Ying Cui",
      "Qingqing Huang",
      "Vincent K. N. Lau"
    ],
    "summary": "In this paper, we propose a two-timescale delay-optimal dynamic clustering\nand power allocation design for downlink network MIMO systems. The dynamic\nclustering control is adaptive to the global queue state information (GQSI)\nonly and computed at the base station controller (BSC) over a longer time\nscale. On the other hand, the power allocations of all the BSs in one cluster\nare adaptive to both intra-cluster channel state information (CCSI) and\nintra-cluster queue state information (CQSI), and computed at the cluster\nmanager (CM) over a shorter time scale. We show that the two-timescale\ndelay-optimal control can be formulated as an infinite-horizon average cost\nConstrained Partially Observed Markov Decision Process (CPOMDP). By exploiting\nthe special problem structure, we shall derive an equivalent Bellman equation\nin terms of Pattern Selection Q-factor to solve the CPOMDP. To address the\ndistributive requirement and the issue of exponential memory requirement and\ncomputational complexity, we approximate the Pattern Selection Q-factor by the\nsum of Per-cluster Potential functions and propose a novel distributive online\nlearning algorithm to estimate the Per-cluster Potential functions (at each CM)\nas well as the Lagrange multipliers (LM) (at each BS). We show that the\nproposed distributive online learning algorithm converges almost surely (with\nprobability 1). By exploiting the birth-death structure of the queue dynamics,\nwe further decompose the Per-cluster Potential function into sum of Per-cluster\nPer-user Potential functions and formulate the instantaneous power allocation\nas a Per-stage QSI-aware Interference Game played among all the CMs. We also\npropose a QSI-aware Simultaneous Iterative Water-filling Algorithm (QSIWFA) and\nshow that it can achieve the Nash Equilibrium (NE).",
    "published": "2010-12-17T13:16:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Survey & Experiment: Towards the Learning Accuracy",
    "authors": [
      "Zeyuan Allen Zhu"
    ],
    "summary": "To attain the best learning accuracy, people move on with difficulties and\nfrustrations. Though one can optimize the empirical objective using a given set\nof samples, its generalization ability to the entire sample distribution\nremains questionable. Even if a fair generalization guarantee is offered, one\nstill wants to know what is to happen if the regularizer is removed, and/or how\nwell the artificial loss (like the hinge loss) relates to the accuracy.\n  For such reason, this report surveys four different trials towards the\nlearning accuracy, embracing the major advances in supervised learning theory\nin the past four years. Starting from the generic setting of learning, the\nfirst two trials introduce the best optimization and generalization bounds for\nconvex learning, and the third trial gets rid of the regularizer. As an\ninnovative attempt, the fourth trial studies the optimization when the\nobjective is exactly the accuracy, in the special case of binary\nclassification. This report also analyzes the last trial through experiments.",
    "published": "2010-12-18T03:25:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Travel Time Estimation Using Floating Car Data",
    "authors": [
      "Raffi Sevlian",
      "Ram Rajagopal"
    ],
    "summary": "This report explores the use of machine learning techniques to accurately\npredict travel times in city streets and highways using floating car data\n(location information of user vehicles on a road network). The aim of this\nreport is twofold, first we present a general architecture of solving this\nproblem, then present and evaluate few techniques on real floating car data\ngathered over a month on a 5 Km highway in New Delhi.",
    "published": "2010-12-20T07:36:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "How I won the \"Chess Ratings - Elo vs the Rest of the World\" Competition",
    "authors": [
      "Yannis Sismanis"
    ],
    "summary": "This article discusses in detail the rating system that won the kaggle\ncompetition \"Chess Ratings: Elo vs the rest of the world\". The competition\nprovided a historical dataset of outcomes for chess games, and aimed to\ndiscover whether novel approaches can predict the outcomes of future games,\nmore accurately than the well-known Elo rating system. The winning rating\nsystem, called Elo++ in the rest of the article, builds upon the Elo rating\nsystem. Like Elo, Elo++ uses a single rating per player and predicts the\noutcome of a game, by using a logistic curve over the difference in ratings of\nthe players. The major component of Elo++ is a regularization technique that\navoids overfitting these ratings. The dataset of chess games and outcomes is\nrelatively small and one has to be careful not to draw \"too many conclusions\"\nout of the limited data. Many approaches tested in the competition showed signs\nof such an overfitting. The leader-board was dominated by attempts that did a\nvery good job on a small test dataset, but couldn't generalize well on the\nprivate hold-out dataset. The Elo++ regularization takes into account the\nnumber of games per player, the recency of these games and the ratings of the\nopponents. Finally, Elo++ employs a stochastic gradient descent scheme for\ntraining the ratings, and uses only two global parameters (white's advantage\nand regularization constant) that are optimized using cross-validation.",
    "published": "2010-12-21T09:11:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "EigenNet: A Bayesian hybrid of generative and conditional models for\n  sparse learning",
    "authors": [
      "Yuan Qi",
      "Feng Yan"
    ],
    "summary": "It is a challenging task to select correlated variables in a high dimensional\nspace. To address this challenge, the elastic net has been developed and\nsuccessfully applied to many applications. Despite its great success, the\nelastic net does not explicitly use correlation information embedded in data to\nselect correlated variables. To overcome this limitation, we present a novel\nBayesian hybrid model, the EigenNet, that uses the eigenstructures of data to\nguide variable selection. Specifically, it integrates a sparse conditional\nclassification model with a generative model capturing variable correlations in\na principled Bayesian framework. We reparameterize the hybrid model in the\neigenspace to avoid overfiting and to increase the computational efficiency of\nits MCMC sampler. Furthermore, we provide an alternative view to the EigenNet\nfrom a regularization perspective: the EigenNet has an adaptive\neigenspace-based composite regularizer, which naturally generalizes the\n$l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and\nreal data show that the EigenNet significantly outperforms the lasso, the\nelastic net, and the Bayesian lasso in terms of prediction accuracy, especially\nwhen the number of training samples is smaller than the number of variables.",
    "published": "2011-02-04T04:40:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Transductive Ordinal Regression",
    "authors": [
      "Chun-Wei Seah",
      "Ivor W. Tsang",
      "Yew-Soon Ong"
    ],
    "summary": "Ordinal regression is commonly formulated as a multi-class problem with\nordinal constraints. The challenge of designing accurate classifiers for\nordinal regression generally increases with the number of classes involved, due\nto the large number of labeled patterns that are needed. The availability of\nordinal class labels, however, is often costly to calibrate or difficult to\nobtain. Unlabeled patterns, on the other hand, often exist in much greater\nabundance and are freely available. To take benefits from the abundance of\nunlabeled patterns, we present a novel transductive learning paradigm for\nordinal regression in this paper, namely Transductive Ordinal Regression (TOR).\nThe key challenge of the present study lies in the precise estimation of both\nthe ordinal class label of the unlabeled data and the decision functions of the\nordinal classes, simultaneously. The core elements of the proposed TOR include\nan objective function that caters to several commonly used loss functions\ncasted in transductive settings, for general ordinal regression. A label\nswapping scheme that facilitates a strictly monotonic decrease in the objective\nfunction value is also introduced. Extensive numerical studies on commonly used\nbenchmark datasets including the real world sentiment prediction problem are\nthen presented to showcase the characteristics and efficacies of the proposed\ntransductive ordinal regression. Further, comparisons to recent\nstate-of-the-art ordinal regression methods demonstrate the introduced\ntransductive learning paradigm for ordinal regression led to the robust and\nimproved performance.",
    "published": "2011-02-14T15:53:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning transformed product distributions",
    "authors": [
      "Constantinos Daskalakis",
      "Ilias Diakonikolas",
      "Rocco A. Servedio"
    ],
    "summary": "We consider the problem of learning an unknown product distribution $X$ over\n$\\{0,1\\}^n$ using samples $f(X)$ where $f$ is a \\emph{known} transformation\nfunction. Each choice of a transformation function $f$ specifies a learning\nproblem in this framework.\n  Information-theoretic arguments show that for every transformation function\n$f$ the corresponding learning problem can be solved to accuracy $\\eps$, using\n$\\tilde{O}(n/\\eps^2)$ examples, by a generic algorithm whose running time may\nbe exponential in $n.$ We show that this learning problem can be\ncomputationally intractable even for constant $\\eps$ and rather simple\ntransformation functions. Moreover, the above sample complexity bound is nearly\noptimal for the general problem, as we give a simple explicit linear\ntransformation function $f(x)=w \\cdot x$ with integer weights $w_i \\leq n$ and\nprove that the corresponding learning problem requires $\\Omega(n)$ samples.\n  As our main positive result we give a highly efficient algorithm for learning\na sum of independent unknown Bernoulli random variables, corresponding to the\ntransformation function $f(x)= \\sum_{i=1}^n x_i$. Our algorithm learns to\n$\\eps$-accuracy in poly$(n)$ time, using a surprising poly$(1/\\eps)$ number of\nsamples that is independent of $n.$ We also give an efficient algorithm that\nuses $\\log n \\cdot \\poly(1/\\eps)$ samples but has running time that is only\n$\\poly(\\log n, 1/\\eps).$",
    "published": "2011-03-03T02:46:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Feature Selection Method for Multivariate Performance Measures",
    "authors": [
      "Qi Mao",
      "Ivor W. Tsang"
    ],
    "summary": "Feature selection with specific multivariate performance measures is the key\nto the success of many applications, such as image retrieval and text\nclassification. The existing feature selection methods are usually designed for\nclassification error. In this paper, we propose a generalized sparse\nregularizer. Based on the proposed regularizer, we present a unified feature\nselection framework for general loss functions. In particular, we study the\nnovel feature selection paradigm by optimizing multivariate performance\nmeasures. The resultant formulation is a challenging problem for\nhigh-dimensional data. Hence, a two-layer cutting plane algorithm is proposed\nto solve this problem, and the convergence is presented. In addition, we adapt\nthe proposed method to optimize multivariate measures for multiple instance\nlearning problems. The analyses by comparing with the state-of-the-art feature\nselection methods show that the proposed method is superior to others.\nExtensive experiments on large-scale and high-dimensional real world datasets\nshow that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing a\nsmall subset of features, and achieves significantly improved performances over\nSVM$^{perf}$ in terms of $F_1$-score.",
    "published": "2011-03-05T07:10:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Parallel Online Learning",
    "authors": [
      "Daniel Hsu",
      "Nikos Karampatziakis",
      "John Langford",
      "Alex Smola"
    ],
    "summary": "In this work we study parallelization of online learning, a core primitive in\nmachine learning. In a parallel environment all known approaches for parallel\nonline learning lead to delayed updates, where the model is updated using\nout-of-date information. In the worst case, or when examples are temporally\ncorrelated, delay can have a very adverse effect on the learning algorithm.\nHere, we analyze and present preliminary empirical results on a set of learning\narchitectures based on a feature sharding approach that present various\ntradeoffs between delay, degree of parallelism, representation power and\nempirical performance.",
    "published": "2011-03-22T04:54:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Gaussian Robust Classification",
    "authors": [
      "Ido Ginodi",
      "Amir Globerson"
    ],
    "summary": "Supervised learning is all about the ability to generalize knowledge.\nSpecifically, the goal of the learning is to train a classifier using training\ndata, in such a way that it will be capable of classifying new unseen data\ncorrectly. In order to acheive this goal, it is important to carefully design\nthe learner, so it will not overfit the training data. The later can is done\nusually by adding a regularization term. The statistical learning theory\nexplains the success of this method by claiming that it restricts the\ncomplexity of the learned model. This explanation, however, is rather abstract\nand does not have a geometric intuition. The generalization error of a\nclassifier may be thought of as correlated with its robustness to perturbations\nof the data: a classifier that copes with disturbance is expected to generalize\nwell. Indeed, Xu et al. [2009] have shown that the SVM formulation is\nequivalent to a robust optimization (RO) formulation, in which an adversary\ndisplaces the training and testing points within a ball of pre-determined\nradius. In this work we explore a different kind of robustness, namely changing\neach data point with a Gaussian cloud centered at the sample. Loss is evaluated\nas the expectation of an underlying loss function on the cloud. This setup fits\nthe fact that in many applications, the data is sampled along with noise. We\ndevelop an RO framework, in which the adversary chooses the covariance of the\nnoise. In our algorithm named GURU, the tuning parameter is a spectral bound on\nthe noise, thus it can be estimated using physical or applicative\nconsiderations. Our experiments show that this framework performs as well as\nSVM and even slightly better in some cases. Generalizations for Mercer kernels\nand for the multiclass case are presented as well. We also show that our\nframework may be further generalized, using the technique of convex perspective\nfunctions.",
    "published": "2011-04-01T19:33:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Meaningful Clustered Forest: an Automatic and Robust Clustering\n  Algorithm",
    "authors": [
      "Mariano Tepper",
      "Pablo Musé",
      "Andrés Almansa"
    ],
    "summary": "We propose a new clustering technique that can be regarded as a numerical\nmethod to compute the proximity gestalt. The method analyzes edge length\nstatistics in the MST of the dataset and provides an a contrario cluster\ndetection criterion. The approach is fully parametric on the chosen distance\nand can detect arbitrarily shaped clusters. The method is also automatic, in\nthe sense that only a single parameter is left to the user. This parameter has\nan intuitive interpretation as it controls the expected number of false\ndetections. We show that the iterative application of our method can (1)\nprovide robustness to noise and (2) solve a masking phenomenon in which a\nhighly populated and salient cluster dominates the scene and inhibits the\ndetection of less-populated, but still salient, clusters.",
    "published": "2011-04-04T19:04:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC learnability versus VC dimension: a footnote to a basic result of\n  statistical learning",
    "authors": [
      "Vladimir Pestov"
    ],
    "summary": "A fundamental result of statistical learnig theory states that a concept\nclass is PAC learnable if and only if it is a uniform Glivenko-Cantelli class\nif and only if the VC dimension of the class is finite. However, the theorem is\nonly valid under special assumptions of measurability of the class, in which\ncase the PAC learnability even becomes consistent. Otherwise, there is a\nclassical example, constructed under the Continuum Hypothesis by Dudley and\nDurst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a\nconcept class of VC dimension one which is neither uniform Glivenko-Cantelli\nnor consistently PAC learnable. We show that, rather surprisingly, under an\nadditional set-theoretic hypothesis which is much milder than the Continuum\nHypothesis (Martin's Axiom), PAC learnability is equivalent to finite VC\ndimension for every concept class.",
    "published": "2011-04-12T01:15:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Temporal Second Difference Traces",
    "authors": [
      "Mitchell Keith Bloch"
    ],
    "summary": "Q-learning is a reliable but inefficient off-policy temporal-difference\nmethod, backing up reward only one step at a time. Replacing traces, using a\nrecency heuristic, are more efficient but less reliable. In this work, we\nintroduce model-free, off-policy temporal difference methods that make better\nuse of experience than Watkins' Q(\\lambda). We introduce both Optimistic\nQ(\\lambda) and the temporal second difference trace (TSDT). TSDT is\nparticularly powerful in deterministic domains. TSDT uses neither recency nor\nfrequency heuristics, storing (s,a,r,s',\\delta) so that off-policy updates can\nbe performed after apparently suboptimal actions have been taken. There are\nadditional advantages when using state abstraction, as in MAXQ. We demonstrate\nthat TSDT does significantly better than both Q-learning and Watkins'\nQ(\\lambda) in a deterministic cliff-walking domain. Results in a noisy\ncliff-walking domain are less advantageous for TSDT, but demonstrate the\nefficacy of Optimistic Q(\\lambda), a replacing trace with some of the\nadvantages of TSDT.",
    "published": "2011-04-24T22:59:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement\n  Learning",
    "authors": [
      "Mitchell Keith Bloch"
    ],
    "summary": "In experimenting with off-policy temporal difference (TD) methods in\nhierarchical reinforcement learning (HRL) systems, we have observed unwanted\non-policy learning under reproducible conditions. Here we present modifications\nto several TD methods that prevent unintentional on-policy learning from\noccurring. These modifications create a tension between exploration and\nlearning. Traditional TD methods require commitment to finishing subtasks\nwithout exploration in order to update Q-values for early actions with high\nprobability. One-step intra-option learning and temporal second difference\ntraces (TSDT) do not suffer from this limitation. We demonstrate that our HRL\nsystem is efficient without commitment to completion of subtasks in a\ncliff-walking domain, contrary to a widespread claim in the literature that it\nis critical for efficiency of learning. Furthermore, decreasing commitment as\nexploration progresses is shown to improve both online performance and the\nresultant policy in the taxicab domain, opening a new avenue for research into\nwhen it is more beneficial to continue with the current subtask or to replan.",
    "published": "2011-04-27T00:58:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Attacking and Defending Covert Channels and Behavioral Models",
    "authors": [
      "Valentino Crespi",
      "George Cybenko",
      "Annarita Giani"
    ],
    "summary": "In this paper we present methods for attacking and defending $k$-gram\nstatistical analysis techniques that are used, for example, in network traffic\nanalysis and covert channel detection. The main new result is our demonstration\nof how to use a behavior's or process' $k$-order statistics to build a\nstochastic process that has those same $k$-order stationary statistics but\npossesses different, deliberately designed, $(k+1)$-order statistics if\ndesired. Such a model realizes a \"complexification\" of the process or behavior\nwhich a defender can use to monitor whether an attacker is shaping the\nbehavior. By deliberately introducing designed $(k+1)$-order behaviors, the\ndefender can check to see if those behaviors are present in the data. We also\ndevelop constructs for source codes that respect the $k$-order statistics of a\nprocess while encoding covert information. One fundamental consequence of these\nresults is that certain types of behavior analyses techniques come down to an\n{\\em arms race} in the sense that the advantage goes to the party that has more\ncomputing resources applied to the problem.",
    "published": "2011-04-27T04:12:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Suboptimal Solution Path Algorithm for Support Vector Machine",
    "authors": [
      "Masayuki Karasuyama",
      "Ichiro Takeuchi"
    ],
    "summary": "We consider a suboptimal solution path algorithm for the Support Vector\nMachine. The solution path algorithm is an effective tool for solving a\nsequence of a parametrized optimization problems in machine learning. The path\nof the solutions provided by this algorithm are very accurate and they satisfy\nthe optimality conditions more strictly than other SVM optimization algorithms.\nIn many machine learning application, however, this strict optimality is often\nunnecessary, and it adversely affects the computational efficiency. Our\nalgorithm can generate the path of suboptimal solutions within an arbitrary\nuser-specified tolerance level. It allows us to control the trade-off between\nthe accuracy of the solution and the computational cost. Moreover, We also show\nthat our suboptimal solutions can be interpreted as the solution of a\n\\emph{perturbed optimization problem} from the original one. We provide some\ntheoretical analyses of our algorithm based on this novel interpretation. The\nexperimental results also demonstrate the effectiveness of our algorithm.",
    "published": "2011-05-03T03:14:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Domain Adaptation: Overfitting and Small Sample Statistics",
    "authors": [
      "Dean Foster",
      "Sham Kakade",
      "Ruslan Salakhutdinov"
    ],
    "summary": "We study the prevalent problem when a test distribution differs from the\ntraining distribution. We consider a setting where our training set consists of\na small number of sample domains, but where we have many samples in each\ndomain. Our goal is to generalize to a new domain. For example, we may want to\nlearn a similarity function using only certain classes of objects, but we\ndesire that this similarity function be applicable to object classes not\npresent in our training sample (e.g. we might seek to learn that \"dogs are\nsimilar to dogs\" even though images of dogs were absent from our training set).\nOur theoretical analysis shows that we can select many more features than\ndomains while avoiding overfitting by utilizing data-dependent variance\nproperties. We present a greedy feature selection algorithm based on using\nT-statistics. Our experiments validate this theory showing that our T-statistic\nbased greedy feature selection is more robust at avoiding overfitting than the\nclassical greedy procedure.",
    "published": "2011-05-04T15:50:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptively Learning the Crowd Kernel",
    "authors": [
      "Omer Tamuz",
      "Ce Liu",
      "Serge Belongie",
      "Ohad Shamir",
      "Adam Tauman Kalai"
    ],
    "summary": "We introduce an algorithm that, given n objects, learns a similarity matrix\nover all n^2 pairs, from crowdsourced data alone. The algorithm samples\nresponses to adaptively chosen triplet-based relative-similarity queries. Each\nquery has the form \"is object 'a' more similar to 'b' or to 'c'?\" and is chosen\nto be maximally informative given the preceding responses. The output is an\nembedding of the objects into Euclidean space (like MDS); we refer to this as\nthe \"crowd kernel.\" SVMs reveal that the crowd kernel captures prominent and\nsubtle features across a number of domains, such as \"is striped\" among neckties\nand \"vowel vs. consonant\" among letters.",
    "published": "2011-05-05T11:03:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Maximal Large Deviation Inequality for Sub-Gaussian Variables",
    "authors": [
      "Dotan Di Castro",
      "Claudio Gentile",
      "Shie Mannor"
    ],
    "summary": "In this short note we prove a maximal concentration lemma for sub-Gaussian\nrandom variables stating that for independent sub-Gaussian random variables we\nhave \\[P<(\\max_{1\\le i\\le N}S_{i}>\\epsilon>)\n\\le\\exp<(-\\frac{1}{N^2}\\sum_{i=1}^{N}\\frac{\\epsilon^{2}}{2\\sigma_{i}^{2}}>), \\]\nwhere $S_i$ is the sum of $i$ zero mean independent sub-Gaussian random\nvariables and $\\sigma_i$ is the variance of the $i$th random variable.",
    "published": "2011-05-12T19:29:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Calibration with Changing Checking Rules and Its Application to\n  Short-Term Trading",
    "authors": [
      "Vladimir Trunov",
      "Vladimir V'yugin"
    ],
    "summary": "We provide a natural learning process in which a financial trader without a\nrisk receives a gain in case when Stock Market is inefficient. In this process,\nthe trader rationally choose his gambles using a prediction made by a\nrandomized calibrated algorithm. Our strategy is based on Dawid's notion of\ncalibration with more general changing checking rules and on some modification\nof Kakade and Foster's randomized algorithm for computing calibrated forecasts.",
    "published": "2011-05-21T17:28:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounding the Fat Shattering Dimension of a Composition Function Class\n  Built Using a Continuous Logic Connective",
    "authors": [
      "Hubert Haoyang Duan"
    ],
    "summary": "We begin this report by describing the Probably Approximately Correct (PAC)\nmodel for learning a concept class, consisting of subsets of a domain, and a\nfunction class, consisting of functions from the domain to the unit interval.\nTwo combinatorial parameters, the Vapnik-Chervonenkis (VC) dimension and its\ngeneralization, the Fat Shattering dimension of scale e, are explained and a\nfew examples of their calculations are given with proofs. We then explain\nSauer's Lemma, which involves the VC dimension and is used to prove the\nequivalence of a concept class being distribution-free PAC learnable and it\nhaving finite VC dimension.\n  As the main new result of our research, we explore the construction of a new\nfunction class, obtained by forming compositions with a continuous logic\nconnective, a uniformly continuous function from the unit hypercube to the unit\ninterval, from a collection of function classes. Vidyasagar had proved that\nsuch a composition function class has finite Fat Shattering dimension of all\nscales if the classes in the original collection do; however, no estimates of\nthe dimension were known. Using results by Mendelson-Vershynin and Talagrand,\nwe bound the Fat Shattering dimension of scale e of this new function class in\nterms of the Fat Shattering dimensions of the collection's classes.\n  We conclude this report by providing a few open questions and future research\ntopics involving the PAC learning model.",
    "published": "2011-05-23T20:04:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning, Stability, and Stochastic Gradient Descent",
    "authors": [
      "Tomaso Poggio",
      "Stephen Voinea",
      "Lorenzo Rosasco"
    ],
    "summary": "In batch learning, stability together with existence and uniqueness of the\nsolution corresponds to well-posedness of Empirical Risk Minimization (ERM)\nmethods; recently, it was proved that CV_loo stability is necessary and\nsufficient for generalization and consistency of ERM. In this note, we\nintroduce CV_on stability, which plays a similar note in online learning. We\nshow that stochastic gradient descent (SDG) with the usual hypotheses is CVon\nstable and we then discuss the implications of CV_on stability for convergence\nof SGD.",
    "published": "2011-05-24T07:58:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-Scale Music Annotation and Retrieval: Learning to Rank in Joint\n  Semantic Spaces",
    "authors": [
      "Jason Weston",
      "Samy Bengio",
      "Philippe Hamel"
    ],
    "summary": "Music prediction tasks range from predicting tags given a song or clip of\naudio, predicting the name of the artist, or predicting related songs given a\nsong, clip, artist name or tag. That is, we are interested in every semantic\nrelationship between the different musical concepts in our database. In\nrealistically sized databases, the number of songs is measured in the hundreds\nof thousands or more, and the number of artists in the tens of thousands or\nmore, providing a considerable challenge to standard machine learning\ntechniques. In this work, we propose a method that scales to such datasets\nwhich attempts to capture the semantic similarities between the database items\nby modeling audio, artist names, and tags in a single low-dimensional semantic\nspace. This choice of space is learnt by optimizing the set of prediction tasks\nof interest jointly using multi-task learning. Our method both outperforms\nbaseline methods and, in comparison to them, is faster and consumes less\nmemory. We then demonstrate how our method learns an interpretable model, where\nthe semantic space captures well the similarities of interest.",
    "published": "2011-05-26T03:41:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Kernel Belief Propagation",
    "authors": [
      "Le Song",
      "Arthur Gretton",
      "Danny Bickson",
      "Yucheng Low",
      "Carlos Guestrin"
    ],
    "summary": "We propose a nonparametric generalization of belief propagation, Kernel\nBelief Propagation (KBP), for pairwise Markov random fields. Messages are\nrepresented as functions in a reproducing kernel Hilbert space (RKHS), and\nmessage updates are simple linear operations in the RKHS. KBP makes none of the\nassumptions commonly required in classical BP algorithms: the variables need\nnot arise from a finite domain or a Gaussian distribution, nor must their\nrelations take any particular parametric form. Rather, the relations between\nvariables are represented implicitly, and are learned nonparametrically from\ntraining data. KBP has the advantage that it may be used on any domain where\nkernels are defined (Rd, strings, groups), even where explicit parametric\nmodels are not known, or closed form expressions for the BP updates do not\nexist. The computational cost of message updates in KBP is polynomial in the\ntraining data size. We also propose a constant time approximate message update\nprocedure by representing messages using a small number of basis functions. In\nexperiments, we apply KBP to image denoising, depth prediction from still\nimages, and protein configuration prediction: KBP is faster than competing\nclassical and nonparametric approaches (by orders of magnitude, in some cases),\nwhile providing significantly more accurate results.",
    "published": "2011-05-27T15:56:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Perceptron with Dynamic Margin",
    "authors": [
      "Constantinos Panagiotakopoulos",
      "Petroula Tsampouka"
    ],
    "summary": "The classical perceptron rule provides a varying upper bound on the maximum\nmargin, namely the length of the current weight vector divided by the total\nnumber of updates up to that time. Requiring that the perceptron updates its\ninternal state whenever the normalized margin of a pattern is found not to\nexceed a certain fraction of this dynamic upper bound we construct a new\napproximate maximum margin classifier called the perceptron with dynamic margin\n(PDM). We demonstrate that PDM converges in a finite number of steps and derive\nan upper bound on them. We also compare experimentally PDM with other\nperceptron-like algorithms and support vector machines on hard margin tasks\ninvolving linear kernels which are equivalent to 2-norm soft margin.",
    "published": "2011-05-30T17:02:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Unified Framework for Approximating and Clustering Data",
    "authors": [
      "Dan Feldman",
      "Michael Langberg"
    ],
    "summary": "Given a set $F$ of $n$ positive functions over a ground set $X$, we consider\nthe problem of computing $x^*$ that minimizes the expression $\\sum_{f\\in\nF}f(x)$, over $x\\in X$. A typical application is \\emph{shape fitting}, where we\nwish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from\na (possibly infinite) family $X$ of shapes. Here, each point $p\\in P$\ncorresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$,\nand we seek a shape $x$ that minimizes the sum of distances from each point in\n$P$. In the $k$-clustering variant, each $x\\in X$ is a tuple of $k$ shapes, and\n$f(x)$ is the distance from $p$ to its closest shape in $x$.\n  Our main result is a unified framework for constructing {\\em coresets} and\n{\\em approximate clustering} for such general sets of functions. To achieve our\nresults, we forge a link between the classic and well defined notion of\n$\\varepsilon$-approximations from the theory of PAC Learning and VC dimension,\nto the relatively new (and not so consistent) paradigm of coresets, which are\nsome kind of \"compressed representation\" of the input set $F$. Using\ntraditional techniques, a coreset usually implies an LTAS (linear time\napproximation scheme) for the corresponding optimization problem, which can be\ncomputed in parallel, via one pass over the data, and using only\npolylogarithmic space (i.e, in the streaming model).\n  We show how to generalize the results of our framework for squared distances\n(as in $k$-mean), distances to the $q$th power, and deterministic\nconstructions.",
    "published": "2011-06-07T15:52:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Max-Margin Stacking and Sparse Regularization for Linear Classifier\n  Combination and Selection",
    "authors": [
      "Mehmet Umut Sen",
      "Hakan Erdogan"
    ],
    "summary": "The main principle of stacked generalization (or Stacking) is using a\nsecond-level generalizer to combine the outputs of base classifiers in an\nensemble. In this paper, we investigate different combination types under the\nstacking framework; namely weighted sum (WS), class-dependent weighted sum\n(CWS) and linear stacked generalization (LSG). For learning the weights, we\npropose using regularized empirical risk minimization with the hinge loss. In\naddition, we propose using group sparsity for regularization to facilitate\nclassifier selection. We performed experiments using two different ensemble\nsetups with differing diversities on 8 real-world datasets. Results show the\npower of regularized learning with the hinge loss function. Using sparse\nregularization, we are able to reduce the number of selected classifiers of the\ndiverse ensemble without sacrificing accuracy. With the non-diverse ensembles,\nwe even gain accuracy on average by using sparse regularization.",
    "published": "2011-06-08T23:03:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reinforcement learning based sensing policy optimization for energy\n  efficient cognitive radio networks",
    "authors": [
      "Jan Oksanen",
      "Jarmo Lundén",
      "Visa Koivunen"
    ],
    "summary": "This paper introduces a machine learning based collaborative multi-band\nspectrum sensing policy for cognitive radios. The proposed sensing policy\nguides secondary users to focus the search of unused radio spectrum to those\nfrequencies that persistently provide them high data rate. The proposed policy\nis based on machine learning, which makes it adaptive with the temporally and\nspatially varying radio spectrum. Furthermore, there is no need for dynamic\nmodeling of the primary activity since it is implicitly learned over time.\nEnergy efficiency is achieved by minimizing the number of assigned sensors per\neach subband under a constraint on miss detection probability. It is important\nto control the missed detections because they cause collisions with primary\ntransmissions and lead to retransmissions at both the primary and secondary\nuser. Simulations show that the proposed machine learning based sensing policy\nimproves the overall throughput of the secondary network and improves the\nenergy efficiency while controlling the miss detection probability.",
    "published": "2011-06-09T10:40:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning the Dependence Graph of Time Series with Latent Factors",
    "authors": [
      "Ali Jalali",
      "Sujay Sanghavi"
    ],
    "summary": "This paper considers the problem of learning, from samples, the dependency\nstructure of a system of linear stochastic differential equations, when some of\nthe variables are latent. In particular, we observe the time evolution of some\nvariables, and never observe other variables; from this, we would like to find\nthe dependency structure between the observed variables - separating out the\nspurious interactions caused by the (marginalizing out of the) latent\nvariables' time series. We develop a new method, based on convex optimization,\nto do so in the case when the number of latent variables is smaller than the\nnumber of observed ones. For the case when the dependency structure between the\nobserved variables is sparse, we theoretically establish a high-dimensional\nscaling result for structure recovery. We verify our theoretical result with\nboth synthetic and real data (from the stock market).",
    "published": "2011-06-09T19:34:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On epsilon-optimality of the pursuit learning algorithm",
    "authors": [
      "Ryan Martin",
      "Omkar Tilak"
    ],
    "summary": "Estimator algorithms in learning automata are useful tools for adaptive,\nreal-time optimization in computer science and engineering applications. This\npaper investigates theoretical convergence properties for a special case of\nestimator algorithms: the pursuit learning algorithm. In this note, we identify\nand fill a gap in existing proofs of probabilistic convergence for pursuit\nlearning. It is tradition to take the pursuit learning tuning parameter to be\nfixed in practical applications, but our proof sheds light on the importance of\na vanishing sequence of tuning parameters in a theoretical convergence\nanalysis.",
    "published": "2011-06-16T21:32:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Decoding finger movements from ECoG signals using switching linear\n  models",
    "authors": [
      "Rémi Flamary",
      "Alain Rakotomamonjy"
    ],
    "summary": "One of the major challenges of ECoG-based Brain-Machine Interfaces is the\nmovement prediction of a human subject. Several methods exist to predict an arm\n2-D trajectory. The fourth BCI Competition gives a dataset in which the aim is\nto predict individual finger movements (5-D trajectory). The difficulty lies in\nthe fact that there is no simple relation between ECoG signals and finger\nmovement. We propose in this paper to decode finger flexions using switching\nmodels. This method permits to simplify the system as it is now described as an\nensemble of linear models depending on an internal state. We show that an\ninteresting accuracy prediction can be obtained by such a model.",
    "published": "2011-06-17T06:53:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large margin filtering for signal sequence labeling",
    "authors": [
      "Rémi Flamary",
      "Benjamin Labbé",
      "Alain Rakotomamonjy"
    ],
    "summary": "Signal Sequence Labeling consists in predicting a sequence of labels given an\nobserved sequence of samples. A naive way is to filter the signal in order to\nreduce the noise and to apply a classification algorithm on the filtered\nsamples. We propose in this paper to jointly learn the filter with the\nclassifier leading to a large margin filtering for classification. This method\nallows to learn the optimal cutoff frequency and phase of the filter that may\nbe different from zero. Two methods are proposed and tested on a toy dataset\nand on a real life BCI dataset from BCI Competition III.",
    "published": "2011-06-17T06:54:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Handling uncertainties in SVM classification",
    "authors": [
      "Emilie Niaf",
      "Rémi Flamary",
      "Carole Lartizien",
      "Stéphane Canu"
    ],
    "summary": "This paper addresses the pattern classification problem arising when\navailable target data include some uncertainty information. Target data\nconsidered here is either qualitative (a class label) or quantitative (an\nestimation of the posterior probability). Our main contribution is a SVM\ninspired formulation of this problem allowing to take into account class label\nthrough a hinge loss as well as probability estimates using epsilon-insensitive\ncost function together with a minimum norm (maximum margin) objective. This\nformulation shows a dual form leading to a quadratic problem and allows the use\nof a representer theorem and associated kernel. The solution provided can be\nused for both decision and posterior probability estimation. Based on empirical\nevidence our method outperforms regular SVM in terms of probability predictions\nand classification performances.",
    "published": "2011-06-17T06:55:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Algorithmic Programming Language Identification",
    "authors": [
      "David Klein",
      "Kyle Murray",
      "Simon Weber"
    ],
    "summary": "Motivated by the amount of code that goes unidentified on the web, we\nintroduce a practical method for algorithmically identifying the programming\nlanguage of source code. Our work is based on supervised learning and\nintelligent statistical features. We also explored, but abandoned, a\ngrammatical approach. In testing, our implementation greatly outperforms that\nof an existing tool that relies on a Bayesian classifier. Code is written in\nPython and available under an MIT license.",
    "published": "2011-06-21T00:37:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods",
    "authors": [
      "Andrew Cotter",
      "Ohad Shamir",
      "Nathan Srebro",
      "Karthik Sridharan"
    ],
    "summary": "Mini-batch algorithms have been proposed as a way to speed-up stochastic\nconvex optimization problems. We study how such algorithms can be improved\nusing accelerated gradient methods. We provide a novel analysis, which shows\nhow standard gradient methods may sometimes be insufficient to obtain a\nsignificant speed-up and propose a novel accelerated gradient algorithm, which\ndeals with this deficiency, enjoys a uniformly superior guarantee and works\nwell in practice.",
    "published": "2011-06-22T20:59:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Potential-Based Shaping and Q-Value Initialization are Equivalent",
    "authors": [
      "E. Wiewiora"
    ],
    "summary": "Shaping has proven to be a powerful but precarious means of improving\nreinforcement learning performance. Ng, Harada, and Russell (1999) proposed the\npotential-based shaping algorithm for adding shaping rewards in a way that\nguarantees the learner will learn optimal behavior. In this note, we prove\ncertain similarities between this shaping algorithm and the initialization step\nrequired for several reinforcement learning algorithms. More specifically, we\nprove that a reinforcement learner with initial Q-values based on the shaping\nalgorithm's potential function make the same updates throughout learning as a\nlearner receiving potential-based shaping rewards. We further prove that under\na broad category of policies, the behavior of these two learners are\nindistinguishable. The comparison provides intuition on the theoretical\nproperties of the shaping algorithm as well as a suggestion for a simpler\nmethod for capturing the algorithm's benefit. In addition, the equivalence\nraises previously unaddressed issues concerning the efficiency of learning with\npotential-based shaping.",
    "published": "2011-06-26T21:07:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "IBSEAD: - A Self-Evolving Self-Obsessed Learning Algorithm for Machine\n  Learning",
    "authors": [
      "Jitesh Dundas",
      "David Chik"
    ],
    "summary": "We present IBSEAD or distributed autonomous entity systems based Interaction\n- a learning algorithm for the computer to self-evolve in a self-obsessed\nmanner. This learning algorithm will present the computer to look at the\ninternal and external environment in series of independent entities, which will\ninteract with each other, with and/or without knowledge of the computer's\nbrain. When a learning algorithm interacts, it does so by detecting and\nunderstanding the entities in the human algorithm. However, the problem with\nthis approach is that the algorithm does not consider the interaction of the\nthird party or unknown entities, which may be interacting with each other.\nThese unknown entities in their interaction with the non-computer entities make\nan effect in the environment that influences the information and the behaviour\nof the computer brain. Such details and the ability to process the dynamic and\nunsettling nature of these interactions are absent in the current learning\nalgorithm such as the decision tree learning algorithm. IBSEAD is able to\nevaluate and consider such algorithms and thus give us a better accuracy in\nsimulation of the highly evolved nature of the human brain. Processes such as\ndreams, imagination and novelty, that exist in humans are not fully simulated\nby the existing learning algorithms. Also, Hidden Markov models (HMM) are\nuseful in finding \"hidden\" entities, which may be known or unknown. However,\nthis model fails to consider the case of unknown entities which maybe unclear\nor unknown. IBSEAD is better because it considers three types of entities-\nknown, unknown and invisible. We present our case with a comparison of existing\nalgorithms in known environments and cases and present the results of the\nexperiments using dry run of the simulated runs of the existing machine\nlearning algorithms versus IBSEAD.",
    "published": "2011-06-30T11:08:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Note on Improved Loss Bounds for Multiple Kernel Learning",
    "authors": [
      "Zakria Hussain",
      "John Shawe-Taylor",
      "Mario Marchand"
    ],
    "summary": "In this paper, we correct an upper bound, presented in~\\cite{hs-11}, on the\ngeneralisation error of classifiers learned through multiple kernel learning.\nThe bound in~\\cite{hs-11} uses Rademacher complexity and has an\\emph{additive}\ndependence on the logarithm of the number of kernels and the margin achieved by\nthe classifier. However, there are some errors in parts of the proof which are\ncorrected in this paper. Unfortunately, the final result turns out to be a risk\nbound which has a \\emph{multiplicative} dependence on the logarithm of the\nnumber of kernels and the margin achieved by the classifier.",
    "published": "2011-06-30T15:03:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "GraphLab: A Distributed Framework for Machine Learning in the Cloud",
    "authors": [
      "Yucheng Low",
      "Joseph Gonzalez",
      "Aapo Kyrola",
      "Danny Bickson",
      "Carlos Guestrin"
    ],
    "summary": "Machine Learning (ML) techniques are indispensable in a wide range of fields.\nUnfortunately, the exponential increase of dataset sizes are rapidly extending\nthe runtime of sequential algorithms and threatening to slow future progress in\nML. With the promise of affordable large-scale parallel computing, Cloud\nsystems offer a viable platform to resolve the computational challenges in ML.\nHowever, designing and implementing efficient, provably correct distributed ML\nalgorithms is often prohibitively challenging. To enable ML researchers to\neasily and efficiently use parallel systems, we introduced the GraphLab\nabstraction which is designed to represent the computational patterns in ML\nalgorithms while permitting efficient parallel and distributed implementations.\nIn this paper we provide a formal description of the GraphLab parallel\nabstraction and present an efficient distributed implementation. We conduct a\ncomprehensive evaluation of GraphLab on three state-of-the-art ML algorithms\nusing real large-scale data and a 64 node EC2 cluster of 512 processors. We\nfind that GraphLab achieves orders of magnitude performance gains over Hadoop\nwhile performing comparably or superior to hand-tuned MPI implementations.",
    "published": "2011-07-05T16:56:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Optimal One Pass Large Scale Learning with Averaged Stochastic\n  Gradient Descent",
    "authors": [
      "Wei Xu"
    ],
    "summary": "For large scale learning problems, it is desirable if we can obtain the\noptimal model parameters by going through the data in only one pass. Polyak and\nJuditsky (1992) showed that asymptotically the test performance of the simple\naverage of the parameters obtained by stochastic gradient descent (SGD) is as\ngood as that of the parameters which minimize the empirical cost. However, to\nour knowledge, despite its optimal asymptotic convergence rate, averaged SGD\n(ASGD) received little attention in recent research on large scale learning.\nOne possible reason is that it may take a prohibitively large number of\ntraining samples for ASGD to reach its asymptotic region for most real\nproblems. In this paper, we present a finite sample analysis for the method of\nPolyak and Juditsky (1992). Our analysis shows that it indeed usually takes a\nhuge number of samples for ASGD to reach its asymptotic region for improperly\nchosen learning rate. More importantly, based on our analysis, we propose a\nsimple way to properly set learning rate so that it takes a reasonable amount\nof data for ASGD to reach its asymptotic region. We compare ASGD using our\nproposed learning rate with other well known algorithms for training large\nscale linear classifiers. The experiments clearly show the superiority of ASGD.",
    "published": "2011-07-13T08:57:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Discovering Knowledge using a Constraint-based Language",
    "authors": [
      "Patrice Boizumault",
      "Bruno Crémilleux",
      "Mehdi Khiari",
      "Samir Loudni",
      "Jean-Philippe Métivier"
    ],
    "summary": "Discovering pattern sets or global patterns is an attractive issue from the\npattern mining community in order to provide useful information. By combining\nlocal patterns satisfying a joint meaning, this approach produces patterns of\nhigher level and thus more useful for the data analyst than the usual local\npatterns, while reducing the number of patterns. In parallel, recent works\ninvestigating relationships between data mining and constraint programming (CP)\nshow that the CP paradigm is a nice framework to model and mine such patterns\nin a declarative and generic way. We present a constraint-based language which\nenables us to define queries addressing patterns sets and global patterns. The\nusefulness of such a declarative approach is highlighted by several examples\ncoming from the clustering based on associations. This language has been\nimplemented in the CP framework.",
    "published": "2011-07-18T12:01:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Universality of Online Mirror Descent",
    "authors": [
      "Nathan Srebro",
      "Karthik Sridharan",
      "Ambuj Tewari"
    ],
    "summary": "We show that for a general class of convex online learning problems, Mirror\nDescent can always achieve a (nearly) optimal regret guarantee.",
    "published": "2011-07-20T19:34:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Divergence of Reinforcement Learning Algorithms with Value-Iteration\n  and Function Approximation",
    "authors": [
      "Michael Fairbank",
      "Eduardo Alonso"
    ],
    "summary": "This paper gives specific divergence examples of value-iteration for several\nmajor Reinforcement Learning and Adaptive Dynamic Programming algorithms, when\nusing a function approximator for the value function. These divergence examples\ndiffer from previous divergence examples in the literature, in that they are\napplicable for a greedy policy, i.e. in a \"value iteration\" scenario. Perhaps\nsurprisingly, with a greedy policy, it is also possible to get divergence for\nthe algorithms TD(1) and Sarsa(1). In addition to these divergences, we also\nachieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and\nGDHP.",
    "published": "2011-07-22T13:05:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Axioms for Rational Reinforcement Learning",
    "authors": [
      "Peter Sunehag",
      "Marcus Hutter"
    ],
    "summary": "We provide a formal, simple and intuitive theory of rational decision making\nincluding sequential decisions that affect the environment. The theory has a\ngeometric flavor, which makes the arguments easy to visualize and understand.\nOur theory is for complete decision makers, which means that they have a\ncomplete set of preferences. Our main result shows that a complete rational\ndecision maker implicitly has a probabilistic model of the environment. We have\na countable version of this result that brings light on the issue of countable\nvs finite additivity by showing how it depends on the geometry of the space\nwhich we have preferences over. This is achieved through fruitfully connecting\nrationality with the Hahn-Banach Theorem. The theory presented here can be\nviewed as a formalization and extension of the betting odds approach to\nprobability of Ramsey and De Finetti.",
    "published": "2011-07-27T16:29:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Automatic Network Reconstruction using ASP",
    "authors": [
      "Max Ostrowski",
      "Torsten Schaub",
      "Markus Durzinsky",
      "Wolfgang Marwan",
      "Annegret Wagler"
    ],
    "summary": "Building biological models by inferring functional dependencies from\nexperimental data is an im- portant issue in Molecular Biology. To relieve the\nbiologist from this traditionally manual process, various approaches have been\nproposed to increase the degree of automation. However, available ap- proaches\noften yield a single model only, rely on specific assumptions, and/or use\ndedicated, heuris- tic algorithms that are intolerant to changing circumstances\nor requirements in the view of the rapid progress made in Biotechnology. Our\naim is to provide a declarative solution to the problem by ap- peal to Answer\nSet Programming (ASP) overcoming these difficulties. We build upon an existing\napproach to Automatic Network Reconstruction proposed by part of the authors.\nThis approach has firm mathematical foundations and is well suited for ASP due\nto its combinatorial flavor providing a characterization of all models\nexplaining a set of experiments. The usage of ASP has several ben- efits over\nthe existing heuristic algorithms. First, it is declarative and thus\ntransparent for biological experts. Second, it is elaboration tolerant and thus\nallows for an easy exploration and incorporation of biological constraints.\nThird, it allows for exploring the entire space of possible models. Finally,\nour approach offers an excellent performance, matching existing,\nspecial-purpose systems.",
    "published": "2011-07-28T10:36:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Feature Extraction for Change-Point Detection using Stationary Subspace\n  Analysis",
    "authors": [
      "Duncan Blythe",
      "Paul von Bünau",
      "Frank Meinecke",
      "Klaus-Robert Müller"
    ],
    "summary": "Detecting changes in high-dimensional time series is difficult because it\ninvolves the comparison of probability densities that need to be estimated from\nfinite samples. In this paper, we present the first feature extraction method\ntailored to change point detection, which is based on an extended version of\nStationary Subspace Analysis. We reduce the dimensionality of the data to the\nmost non-stationary directions, which are most informative for detecting state\nchanges in the time series. In extensive simulations on synthetic data we show\nthat the accuracy of three change point detection algorithms is significantly\nincreased by a prior feature extraction step. These findings are confirmed in\nan application to industrial fault monitoring.",
    "published": "2011-08-11T18:54:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Algorithms for Ridge and Lasso Regression with Partially\n  Observed Attributes",
    "authors": [
      "Elad Hazan",
      "Tomer Koren"
    ],
    "summary": "We consider the most common variants of linear regression, including Ridge,\nLasso and Support-vector regression, in a setting where the learner is allowed\nto observe only a fixed number of attributes of each example at training time.\nWe present simple and efficient algorithms for these problems: for Lasso and\nRidge regression they need the same total number of attributes (up to\nconstants) as do full-information algorithms, for reaching a certain accuracy.\nFor Support-vector regression, we require exponentially less attributes\ncompared to the state of the art. By that, we resolve an open problem recently\nposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to\nbe justified by superior performance compared to the state of the art.",
    "published": "2011-08-23T11:52:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Non-trivial two-armed partial-monitoring games are bandits",
    "authors": [
      "András Antos",
      "Gábor Bartók",
      "Csaba Szepesvári"
    ],
    "summary": "We consider online learning in partial-monitoring games against an oblivious\nadversary. We show that when the number of actions available to the learner is\ntwo and the game is nontrivial then it is reducible to a bandit-like game and\nthus the minimax regret is $\\Theta(\\sqrt{T})$.",
    "published": "2011-08-24T22:38:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Local Component Analysis",
    "authors": [
      "Nicolas Le Roux",
      "Francis Bach"
    ],
    "summary": "Kernel density estimation, a.k.a. Parzen windows, is a popular density\nestimation method, which can be used for outlier detection or clustering. With\nmultivariate data, its performance is heavily reliant on the metric used within\nthe kernel. Most earlier work has focused on learning only the bandwidth of the\nkernel (i.e., a scalar multiplicative factor). In this paper, we propose to\nlearn a full Euclidean metric through an expectation-minimization (EM)\nprocedure, which can be seen as an unsupervised counterpart to neighbourhood\ncomponent analysis (NCA). In order to avoid overfitting with a fully\nnonparametric density estimator in high dimensions, we also consider a\nsemi-parametric Gaussian-Parzen density model, where some of the variables are\nmodelled through a jointly Gaussian density, while others are modelled through\nParzen windows. For these two models, EM leads to simple closed-form updates\nbased on matrix inversions and eigenvalue decompositions. We show empirically\nthat our method leads to density estimators with higher test-likelihoods than\nnatural competing methods, and that the metrics may be used within most\nunsupervised learning techniques that rely on such metrics, such as spectral\nclustering or manifold learning methods. Finally, we present a stochastic\napproximation scheme which allows for the use of this method in a large-scale\nsetting.",
    "published": "2011-09-01T05:28:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Weighted Clustering",
    "authors": [
      "Margareta Ackerman",
      "Shai Ben-David",
      "Simina Brânzei",
      "David Loker"
    ],
    "summary": "One of the most prominent challenges in clustering is \"the user's dilemma,\"\nwhich is the problem of selecting an appropriate clustering algorithm for a\nspecific task. A formal approach for addressing this problem relies on the\nidentification of succinct, user-friendly properties that formally capture when\ncertain clustering methods are preferred over others.\n  Until now these properties focused on advantages of classical Linkage-Based\nalgorithms, failing to identify when other clustering paradigms, such as\npopular center-based methods, are preferable. We present surprisingly simple\nnew properties that delineate the differences between common clustering\nparadigms, which clearly and formally demonstrates advantages of center-based\napproaches for some applications. These properties address how sensitive\nalgorithms are to changes in element frequencies, which we capture in a\ngeneralized setting where every element is associated with a real-valued\nweight.",
    "published": "2011-09-08T20:53:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning From Labeled And Unlabeled Data: An Empirical Study Across\n  Techniques And Domains",
    "authors": [
      "N. V. Chawla",
      "Grigoris Karakoulas"
    ],
    "summary": "There has been increased interest in devising learning techniques that\ncombine unlabeled data with labeled data ? i.e. semi-supervised learning.\nHowever, to the best of our knowledge, no study has been performed across\nvarious techniques and different types and amounts of labeled and unlabeled\ndata. Moreover, most of the published work on semi-supervised learning\ntechniques assumes that the labeled and unlabeled data come from the same\ndistribution. It is possible for the labeling process to be associated with a\nselection bias such that the distributions of data points in the labeled and\nunlabeled sets are different. Not correcting for such bias can result in biased\nfunction approximation with potentially poor performance. In this paper, we\npresent an empirical study of various semi-supervised learning techniques on a\nvariety of datasets. We attempt to answer various questions such as the effect\nof independence or relevance amongst features, the effect of the size of the\nlabeled and unlabeled sets and the effect of noise. We also investigate the\nimpact of sample-selection bias on the semi-supervised learning techniques\nunder study and implement a bivariate probit technique particularly designed to\ncorrect for such bias.",
    "published": "2011-09-09T15:56:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficiency versus Convergence of Boolean Kernels for On-Line Learning\n  Algorithms",
    "authors": [
      "R. Khardon",
      "D. Roth",
      "R. A. Servedio"
    ],
    "summary": "The paper studies machine learning problems where each example is described\nusing a set of Boolean features and where hypotheses are represented by linear\nthreshold elements. One method of increasing the expressiveness of learned\nhypotheses in this context is to expand the feature set to include conjunctions\nof basic features. This can be done explicitly or where possible by using a\nkernel function. Focusing on the well known Perceptron and Winnow algorithms,\nthe paper demonstrates a tradeoff between the computational efficiency with\nwhich the algorithm can be run over the expanded feature space and the\ngeneralization ability of the corresponding learning algorithm. We first\ndescribe several kernel functions which capture either limited forms of\nconjunctions or all conjunctions. We show that these kernels can be used to\nefficiently run the Perceptron algorithm over a feature space of exponentially\nmany conjunctions; however we also show that using such kernels, the Perceptron\nalgorithm can provably make an exponential number of mistakes even when\nlearning simple functions. We then consider the question of whether kernel\nfunctions can analogously be used to run the multiplicative-update Winnow\nalgorithm over an expanded feature space of exponentially many conjunctions.\nKnown upper bounds imply that the Winnow algorithm can learn Disjunctive Normal\nForm (DNF) formulae with a polynomial mistake bound in this setting. However,\nwe prove that it is computationally hard to simulate Winnows behavior for\nlearning DNF over such a feature set. This implies that the kernel functions\nwhich correspond to running Winnow for this problem are not efficiently\ncomputable, and that there is no general construction that can run Winnow with\nkernels.",
    "published": "2011-09-09T20:31:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Risk-Sensitive Reinforcement Learning Applied to Control under\n  Constraints",
    "authors": [
      "P. Geibel",
      "F. Wysotzki"
    ],
    "summary": "In this paper, we consider Markov Decision Processes (MDPs) with error\nstates. Error states are those states entering which is undesirable or\ndangerous. We define the risk with respect to a policy as the probability of\nentering such a state when the policy is pursued. We consider the problem of\nfinding good policies whose risk is smaller than some user-specified threshold,\nand formalize it as a constrained MDP with two criteria. The first criterion\ncorresponds to the value function originally given. We will show that the risk\ncan be formulated as a second criterion function based on a cumulative return,\nwhose definition is independent of the original value function. We present a\nmodel free, heuristic reinforcement learning algorithm that aims at finding\ngood deterministic policies. It is based on weighting the original value\nfunction and the risk. The weight parameter is adapted in order to find a\nfeasible solution for the constrained problem that has a good performance with\nrespect to the value function. The algorithm was successfully applied to the\ncontrol of a feed tank with stochastic inflows that lies upstream of a\ndistillation column. This control task was originally formulated as an optimal\ncontrol problem with chance constraints, and it was solved under certain\nassumptions on the model to obtain an optimal solution. The power of our\nlearning algorithm is that it can be used even when some of these restrictive\nassumptions are relaxed.",
    "published": "2011-09-09T20:32:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bandits with an Edge",
    "authors": [
      "Dotan Di Castro",
      "Claudio Gentile",
      "Shie Mannor"
    ],
    "summary": "We consider a bandit problem over a graph where the rewards are not directly\nobserved. Instead, the decision maker can compare two nodes and receive\n(stochastic) information pertaining to the difference in their value. The graph\nstructure describes the set of possible comparisons. Consequently, comparing\nbetween two nodes that are relatively far requires estimating the difference\nbetween every pair of nodes on the path between them. We analyze this problem\nfrom the perspective of sample complexity: How many queries are needed to find\nan approximately optimal node with probability more than $1-\\delta$ in the PAC\nsetup? We show that the topology of the graph plays a crucial in defining the\nsample complexity: graphs with a low diameter have a much better sample\ncomplexity.",
    "published": "2011-09-11T09:00:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Distributed User Profiling via Spectral Methods",
    "authors": [
      "Dan-Cristian Tomozei",
      "Laurent Massoulié"
    ],
    "summary": "User profiling is a useful primitive for constructing personalised services,\nsuch as content recommendation. In the present paper we investigate the\nfeasibility of user profiling in a distributed setting, with no central\nauthority and only local information exchanges between users. We compute a\nprofile vector for each user (i.e., a low-dimensional vector that characterises\nher taste) via spectral transformation of observed user-produced ratings for\nitems. Our two main contributions follow: i) We consider a low-rank\nprobabilistic model of user taste. More specifically, we consider that users\nand items are partitioned in a constant number of classes, such that users and\nitems within the same class are statistically identical. We prove that without\nprior knowledge of the compositions of the classes, based solely on few random\nobserved ratings (namely $O(N\\log N)$ such ratings for $N$ users), we can\npredict user preference with high probability for unrated items by running a\nlocal vote among users with similar profile vectors. In addition, we provide\nempirical evaluations characterising the way in which spectral profiling\nperformance depends on the dimension of the profile space. Such evaluations are\nperformed on a data set of real user ratings provided by Netflix. ii) We\ndevelop distributed algorithms which provably achieve an embedding of users\ninto a low-dimensional space, based on spectral transformation. These involve\nsimple message passing among users, and provably converge to the desired\nembedding. Our method essentially relies on a novel combination of gossiping\nand the algorithm proposed by Oja and Karhunen.",
    "published": "2011-09-15T11:31:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Topic Models by Belief Propagation",
    "authors": [
      "Jia Zeng",
      "William K. Cheung",
      "Jiming Liu"
    ],
    "summary": "Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model\nfor probabilistic topic modeling, which attracts worldwide interests and\ntouches on many important applications in text mining, computer vision and\ncomputational biology. This paper represents LDA as a factor graph within the\nMarkov random field (MRF) framework, which enables the classic loopy belief\npropagation (BP) algorithm for approximate inference and parameter estimation.\nAlthough two commonly-used approximate inference methods, such as variational\nBayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in\nlearning LDA, the proposed BP is competitive in both speed and accuracy as\nvalidated by encouraging experimental results on four large-scale document data\nsets. Furthermore, the BP algorithm has the potential to become a generic\nlearning scheme for variants of LDA-based topic models. To this end, we show\nhow to learn two typical variants of LDA-based topic models, such as\nauthor-topic models (ATM) and relational topic models (RTM), using BP based on\nthe factor graph representation.",
    "published": "2011-09-15T19:20:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Application of distances between terms for flat and hierarchical data",
    "authors": [
      "Jorge-Alonso Bedoya-Puerta",
      "Jose Hernandez-Orallo"
    ],
    "summary": "In machine learning, distance-based algorithms, and other approaches, use\ninformation that is represented by propositional data. However, this kind of\nrepresentation can be quite restrictive and, in many cases, it requires more\ncomplex structures in order to represent data in a more natural way. Terms are\nthe basis for functional and logic programming representation. Distances\nbetween terms are a useful tool not only to compare terms, but also to\ndetermine the search space in many of these applications. This dissertation\napplies distances between terms, exploiting the features of each distance and\nthe possibility to compare from propositional data types to hierarchical\nrepresentations. The distances between terms are applied through the k-NN\n(k-nearest neighbor) classification algorithm using XML as a common language\nrepresentation. To be able to represent these data in an XML structure and to\ntake advantage of the benefits of distance between terms, it is necessary to\napply some transformations. These transformations allow the conversion of flat\ndata into hierarchical data represented in XML, using some techniques based on\nintuitive associations between the names and values of variables and\nassociations based on attribute similarity.\n  Several experiments with the distances between terms of Nienhuys-Cheng and\nEstruch et al. were performed. In the case of originally propositional data,\nthese distances are compared to the Euclidean distance. In all cases, the\nexperiments were performed with the distance-weighted k-nearest neighbor\nalgorithm, using several exponents for the attraction function (weighted\ndistance). It can be seen that in some cases, the term distances can\nsignificantly improve the results on approaches applied to flat\nrepresentations.",
    "published": "2011-09-23T13:51:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Noise Tolerance under Risk Minimization",
    "authors": [
      "Naresh Manwani",
      "P. S. Sastry"
    ],
    "summary": "In this paper we explore noise tolerant learning of classifiers. We formulate\nthe problem as follows. We assume that there is an ${\\bf unobservable}$\ntraining set which is noise-free. The actual training set given to the learning\nalgorithm is obtained from this ideal data set by corrupting the class label of\neach example. The probability that the class label of an example is corrupted\nis a function of the feature vector of the example. This would account for most\nkinds of noisy data one encounters in practice. We say that a learning method\nis noise tolerant if the classifiers learnt with the ideal noise-free data and\nwith noisy data, both have the same classification accuracy on the noise-free\ndata. In this paper we analyze the noise tolerance properties of risk\nminimization (under different loss functions), which is a generic method for\nlearning classifiers. We show that risk minimization under 0-1 loss function\nhas impressive noise tolerance properties and that under squared error loss is\ntolerant only to uniform noise; risk minimization under other loss functions is\nnot noise tolerant. We conclude the paper with some discussion on implications\nof these theoretical results.",
    "published": "2011-09-24T04:50:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Active Learning with Multiple Views",
    "authors": [
      "C. A. Knoblock",
      "S. Minton",
      "I. Muslea"
    ],
    "summary": "Active learners alleviate the burden of labeling large amounts of data by\ndetecting and asking the user to label only the most informative examples in\nthe domain. We focus here on active learning for multi-view domains, in which\nthere are several disjoint subsets of features (views), each of which is\nsufficient to learn the target concept. In this paper we make several\ncontributions. First, we introduce Co-Testing, which is the first approach to\nmulti-view active learning. Second, we extend the multi-view learning framework\nby also exploiting weak views, which are adequate only for learning a concept\nthat is more general/specific than the target concept. Finally, we empirically\nshow that Co-Testing outperforms existing active learners on a variety of real\nworld domains such as wrapper induction, Web page classification, advertisement\nremoval, and discourse tree parsing.",
    "published": "2011-10-05T18:59:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Augmented Complex Kernel LMS",
    "authors": [
      "Pantelis Bouboulis",
      "Sergios Theodoridis",
      "Michael Mavroforakis"
    ],
    "summary": "Recently, a unified framework for adaptive kernel based signal processing of\ncomplex data was presented by the authors, which, besides offering techniques\nto map the input data to complex Reproducing Kernel Hilbert Spaces, developed a\nsuitable Wirtinger-like Calculus for general Hilbert Spaces. In this short\npaper, the extended Wirtinger's calculus is adopted to derive complex\nkernel-based widely-linear estimation filters. Furthermore, we illuminate\nseveral important characteristics of the widely linear filters. We show that,\nalthough in many cases the gains from adopting widely linear estimation\nfilters, as alternatives to ordinary linear ones, are rudimentary, for the case\nof kernel based widely linear filters significant performance improvements can\nbe obtained.",
    "published": "2011-10-05T19:03:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dynamic Matrix Factorization: A State Space Approach",
    "authors": [
      "John Z. Sun",
      "Kush R. Varshney",
      "Karthik Subbian"
    ],
    "summary": "Matrix factorization from a small number of observed entries has recently\ngarnered much attention as the key ingredient of successful recommendation\nsystems. One unresolved problem in this area is how to adapt current methods to\nhandle changing user preferences over time. Recent proposals to address this\nissue are heuristic in nature and do not fully exploit the time-dependent\nstructure of the problem. As a principled and general temporal formulation, we\npropose a dynamical state space model of matrix factorization. Our proposal\nbuilds upon probabilistic matrix factorization, a Bayesian model with Gaussian\npriors. We utilize results in state tracking, such as the Kalman filter, to\nprovide accurate recommendations in the presence of both process and\nmeasurement noise. We show how system parameters can be learned via\nexpectation-maximization and provide comparisons to current published\ntechniques.",
    "published": "2011-10-10T16:35:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Active Learning Using Smooth Relative Regret Approximations with\n  Applications",
    "authors": [
      "Nir Ailon",
      "Ron Begleiter",
      "Esther Ezra"
    ],
    "summary": "The disagreement coefficient of Hanneke has become a central data independent\ninvariant in proving active learning rates. It has been shown in various ways\nthat a concept class with low complexity together with a bound on the\ndisagreement coefficient at an optimal solution allows active learning rates\nthat are superior to passive learning ones.\n  We present a different tool for pool based active learning which follows from\nthe existence of a certain uniform version of low disagreement coefficient, but\nis not equivalent to it. In fact, we present two fundamental active learning\nproblems of significant interest for which our approach allows nontrivial\nactive learning bounds. However, any general purpose method relying on the\ndisagreement coefficient bounds only fails to guarantee any useful bounds for\nthese problems.\n  The tool we use is based on the learner's ability to compute an estimator of\nthe difference between the loss of any hypotheses and some fixed \"pivotal\"\nhypothesis to within an absolute error of at most $\\eps$ times the",
    "published": "2011-10-10T18:32:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supervised learning of short and high-dimensional temporal sequences for\n  life science measurements",
    "authors": [
      "F. -M. Schleif",
      "A. Gisbrecht",
      "B. Hammer"
    ],
    "summary": "The analysis of physiological processes over time are often given by\nspectrometric or gene expression profiles over time with only few time points\nbut a large number of measured variables. The analysis of such temporal\nsequences is challenging and only few methods have been proposed. The\ninformation can be encoded time independent, by means of classical expression\ndifferences for a single time point or in expression profiles over time.\nAvailable methods are limited to unsupervised and semi-supervised settings. The\npredictive variables can be identified only by means of wrapper or\npost-processing techniques. This is complicated due to the small number of\nsamples for such studies. Here, we present a supervised learning approach,\ntermed Supervised Topographic Mapping Through Time (SGTM-TT). It learns a\nsupervised mapping of the temporal sequences onto a low dimensional grid. We\nutilize a hidden markov model (HMM) to account for the time domain and\nrelevance learning to identify the relevant feature dimensions most predictive\nover time. The learned mapping can be used to visualize the temporal sequences\nand to predict the class of a new sequence. The relevance learning permits the\nidentification of discriminating masses or gen expressions and prunes\ndimensions which are unnecessary for the classification task or encode mainly\nnoise. In this way we obtain a very efficient learning system for temporal\nsequences. The results indicate that using simultaneous supervised learning and\nmetric adaptation significantly improves the prediction accuracy for\nsynthetically and real life data in comparison to the standard techniques. The\ndiscriminating features, identified by relevance learning, compare favorably\nwith the results of alternative methods. Our method permits the visualization\nof the data on a low dimensional grid, highlighting the observed temporal\nstructure.",
    "published": "2011-10-11T16:19:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dynamic Batch Bayesian Optimization",
    "authors": [
      "Javad Azimi",
      "Ali Jalali",
      "Xiaoli Fern"
    ],
    "summary": "Bayesian optimization (BO) algorithms try to optimize an unknown function\nthat is expensive to evaluate using minimum number of evaluations/experiments.\nMost of the proposed algorithms in BO are sequential, where only one experiment\nis selected at each iteration. This method can be time inefficient when each\nexperiment takes a long time and more than one experiment can be ran\nconcurrently. On the other hand, requesting a fix-sized batch of experiments at\neach iteration causes performance inefficiency in BO compared to the sequential\npolicies. In this paper, we present an algorithm that asks a batch of\nexperiments at each time step t where the batch size p_t is dynamically\ndetermined in each step. Our algorithm is based on the observation that the\nsequence of experiments selected by the sequential policy can sometimes be\nalmost independent from each other. Our algorithm identifies such scenarios and\nrequest those experiments at the same time without degrading the performance.\nWe evaluate our proposed method using the Expected Improvement policy and the\nresults show substantial speedup with little impact on the performance in eight\nreal and synthetic benchmarks.",
    "published": "2011-10-14T21:47:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Injecting External Solutions Into CMA-ES",
    "authors": [
      "Nikolaus Hansen"
    ],
    "summary": "This report considers how to inject external candidate solutions into the\nCMA-ES algorithm. The injected solutions might stem from a gradient or a Newton\nstep, a surrogate model optimizer or any other oracle or search mechanism. They\ncan also be the result of a repair mechanism, for example to render infeasible\nsolutions feasible. Only small modifications to the CMA-ES are necessary to\nturn injection into a reliable and effective method: too long steps need to be\ntightly renormalized. The main objective of this report is to reveal this\nsimple mechanism. Depending on the source of the injected solutions,\ninteresting variants of CMA-ES arise. When the best-ever solution is always\n(re-)injected, an elitist variant of CMA-ES with weighted multi-recombination\narises. When \\emph{all} solutions are injected from an \\emph{external} source,\nthe resulting algorithm might be viewed as \\emph{adaptive encoding} with\nstep-size control. In first experiments, injected solutions of very good\nquality lead to a convergence speed twice as fast as on the (simple) sphere\nfunction without injection. This means that we observe an impressive speed-up\non otherwise difficult to solve functions. Single bad injected solutions on the\nother hand do no significant harm.",
    "published": "2011-10-19T04:42:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Data-dependent kernels in nearly-linear time",
    "authors": [
      "Guy Lever",
      "Tom Diethe",
      "John Shawe-Taylor"
    ],
    "summary": "We propose a method to efficiently construct data-dependent kernels which can\nmake use of large quantities of (unlabeled) data. Our construction makes an\napproximation in the standard construction of semi-supervised kernels in\nSindhwani et al. 2005. In typical cases these kernels can be computed in\nnearly-linear time (in the amount of data), improving on the cubic time of the\nstandard construction, enabling large scale semi-supervised learning in a\nvariety of contexts. The methods are validated on semi-supervised and\nunsupervised problems on data sets containing upto 64,000 sample points.",
    "published": "2011-10-20T00:56:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Hierarchical and Topographic Dictionaries with Structured\n  Sparsity",
    "authors": [
      "Julien Mairal",
      "Rodolphe Jenatton",
      "Guillaume Obozinski",
      "Francis Bach"
    ],
    "summary": "Recent work in signal processing and statistics have focused on defining new\nregularization functions, which not only induce sparsity of the solution, but\nalso take into account the structure of the problem. We present in this paper a\nclass of convex penalties introduced in the machine learning community, which\ntake the form of a sum of l_2 and l_infinity-norms over groups of variables.\nThey extend the classical group-sparsity regularization in the sense that the\ngroups possibly overlap, allowing more flexibility in the group design. We\nreview efficient optimization methods to deal with the corresponding inverse\nproblems, and their application to the problem of learning dictionaries of\nnatural image patches: On the one hand, dictionary learning has indeed proven\neffective for various signal processing tasks. On the other hand, structured\nsparsity provides a natural framework for modeling dependencies between\ndictionary elements. We thus consider a structured sparse regularization to\nlearn dictionaries embedded in a particular structure, for instance a tree or a\ntwo-dimensional grid. In the latter case, the results we obtain are similar to\nthe dictionaries produced by topographic independent component analysis.",
    "published": "2011-10-20T09:50:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Wikipedia Edit Number Prediction based on Temporal Dynamics Only",
    "authors": [
      "Dell Zhang"
    ],
    "summary": "In this paper, we describe our approach to the Wikipedia Participation\nChallenge which aims to predict the number of edits a Wikipedia editor will\nmake in the next 5 months. The best submission from our team, \"zeditor\",\nachieved 41.7% improvement over WMF's baseline predictive model and the final\nrank of 3rd place among 96 teams. An interesting characteristic of our approach\nis that only temporal dynamics features (i.e., how the number of edits changes\nin recent periods, etc.) are used in a self-supervised learning framework,\nwhich makes it easy to be generalised to other application domains.",
    "published": "2011-10-23T14:41:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deciding of HMM parameters based on number of critical points for\n  gesture recognition from motion capture data",
    "authors": [
      "Michał Cholewa",
      "Przemysław Głomb"
    ],
    "summary": "This paper presents a method of choosing number of states of a HMM based on\nnumber of critical points of the motion capture data. The choice of Hidden\nMarkov Models(HMM) parameters is crucial for recognizer's performance as it is\nthe first step of the training and cannot be corrected automatically within\nHMM. In this article we define predictor of number of states based on number of\ncritical points of the sequence and test its effectiveness against sample data.",
    "published": "2011-10-28T10:20:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC-Bayes-Bernstein Inequality for Martingales and its Application to\n  Multiarmed Bandits",
    "authors": [
      "Yevgeny Seldin",
      "Nicolò Cesa-Bianchi",
      "Peter Auer",
      "François Laviolette",
      "John Shawe-Taylor"
    ],
    "summary": "We develop a new tool for data-dependent analysis of the\nexploration-exploitation trade-off in learning under limited feedback. Our tool\nis based on two main ingredients. The first ingredient is a new concentration\ninequality that makes it possible to control the concentration of weighted\naverages of multiple (possibly uncountably many) simultaneously evolving and\ninterdependent martingales. The second ingredient is an application of this\ninequality to the exploration-exploitation trade-off via importance weighted\nsampling. We apply the new tool to the stochastic multiarmed bandit problem,\nhowever, the main importance of this paper is the development and understanding\nof the new tool rather than improvement of existing algorithms for stochastic\nmultiarmed bandits. In the follow-up work we demonstrate that the new tool can\nimprove over state-of-the-art in structurally richer problems, such as\nstochastic multiarmed bandits with side information (Seldin et al., 2011a).",
    "published": "2011-10-31T11:36:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Confidence Estimation in Structured Prediction",
    "authors": [
      "Avihai Mejer",
      "Koby Crammer"
    ],
    "summary": "Structured classification tasks such as sequence labeling and dependency\nparsing have seen much interest by the Natural Language Processing and the\nmachine learning communities. Several online learning algorithms were adapted\nfor structured tasks such as Perceptron, Passive- Aggressive and the recently\nintroduced Confidence-Weighted learning . These online algorithms are easy to\nimplement, fast to train and yield state-of-the-art performance. However,\nunlike probabilistic models like Hidden Markov Model and Conditional random\nfields, these methods generate models that output merely a prediction with no\nadditional information regarding confidence in the correctness of the output.\nIn this work we fill the gap proposing few alternatives to compute the\nconfidence in the output of non-probabilistic algorithms.We show how to compute\nconfidence estimates in the prediction such that the confidence reflects the\nprobability that the word is labeled correctly. We then show how to use our\nmethods to detect mislabeled words, trade recall for precision and active\nlearning. We evaluate our methods on four noun-phrase chunking and named entity\nrecognition sequence labeling tasks, and on dependency parsing for 14\nlanguages.",
    "published": "2011-11-06T08:43:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Robust Interactive Learning",
    "authors": [
      "Maria-Florina Balcan",
      "Steve Hanneke"
    ],
    "summary": "In this paper we propose and study a generalization of the standard\nactive-learning model where a more general type of query, class conditional\nquery, is allowed. Such queries have been quite useful in applications, but\nhave been lacking theoretical understanding. In this work, we characterize the\npower of such queries under two well-known noise models. We give nearly tight\nupper and lower bounds on the number of queries needed to learn both for the\ngeneral agnostic setting and for the bounded noise model. We further show that\nour methods can be made adaptive to the (unknown) noise rate, with only\nnegligible loss in query complexity.",
    "published": "2011-11-06T14:01:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Parametrized Stochastic Multi-armed Bandits with Binary Rewards",
    "authors": [
      "Chong Jiang",
      "R. Srikant"
    ],
    "summary": "In this paper, we consider the problem of multi-armed bandits with a large,\npossibly infinite number of correlated arms. We assume that the arms have\nBernoulli distributed rewards, independent across time, where the probabilities\nof success are parametrized by known attribute vectors for each arm, as well as\nan unknown preference vector, each of dimension $n$. For this model, we seek an\nalgorithm with a total regret that is sub-linear in time and independent of the\nnumber of arms. We present such an algorithm, which we call the Two-Phase\nAlgorithm, and analyze its performance. We show upper bounds on the total\nregret which applies uniformly in time, for both the finite and infinite arm\ncases. The asymptotics of the finite arm bound show that for any $f \\in\n\\omega(\\log(T))$, the total regret can be made to be $O(n \\cdot f(T))$. In the\ninfinite arm case, the total regret is $O(\\sqrt{n^3 T})$.",
    "published": "2011-11-18T19:23:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Regression in Metric Spaces via Approximate Lipschitz\n  Extension",
    "authors": [
      "Lee-Ad Gottlieb",
      "Aryeh Kontorovich",
      "Robert Krauthgamer"
    ],
    "summary": "We present a framework for performing efficient regression in general metric\nspaces. Roughly speaking, our regressor predicts the value at a new point by\ncomputing a Lipschitz extension --- the smoothest function consistent with the\nobserved data --- after performing structural risk minimization to avoid\noverfitting. We obtain finite-sample risk bounds with minimal structural and\nnoise assumptions, and a natural speed-precision tradeoff. The offline\n(learning) and online (prediction) stages can be solved by convex programming,\nbut this naive approach has runtime complexity $O(n^3)$, which is prohibitive\nfor large datasets. We design instead a regression algorithm whose speed and\ngeneralization performance depend on the intrinsic dimension of the data, to\nwhich the algorithm adapts. While our main innovation is algorithmic, the\nstatistical results may also be of independent interest.",
    "published": "2011-11-18T20:32:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large Scale Spectral Clustering Using Approximate Commute Time Embedding",
    "authors": [
      "Nguyen Lu Dang Khoa",
      "Sanjay Chawla"
    ],
    "summary": "Spectral clustering is a novel clustering method which can detect complex\nshapes of data clusters. However, it requires the eigen decomposition of the\ngraph Laplacian matrix, which is proportion to $O(n^3)$ and thus is not\nsuitable for large scale systems. Recently, many methods have been proposed to\naccelerate the computational time of spectral clustering. These approximate\nmethods usually involve sampling techniques by which a lot information of the\noriginal data may be lost. In this work, we propose a fast and accurate\nspectral clustering approach using an approximate commute time embedding, which\nis similar to the spectral embedding. The method does not require using any\nsampling technique and computing any eigenvector at all. Instead it uses random\nprojection and a linear time solver to find the approximate embedding. The\nexperiments in several synthetic and real datasets show that the proposed\napproach has better clustering quality and is faster than the state-of-the-art\napproximate spectral clustering methods.",
    "published": "2011-11-19T08:39:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Trading Regret for Efficiency: Online Convex Optimization with Long Term\n  Constraints",
    "authors": [
      "Mehrdad Mahdavi",
      "Rong Jin",
      "Tianbao Yang"
    ],
    "summary": "In this paper we propose a framework for solving constrained online convex\noptimization problem. Our motivation stems from the observation that most\nalgorithms proposed for online convex optimization require a projection onto\nthe convex set $\\mathcal{K}$ from which the decisions are made. While for\nsimple shapes (e.g. Euclidean ball) the projection is straightforward, for\narbitrary complex sets this is the main computational challenge and may be\ninefficient in practice. In this paper, we consider an alternative online\nconvex optimization problem. Instead of requiring decisions belong to\n$\\mathcal{K}$ for all rounds, we only require that the constraints which define\nthe set $\\mathcal{K}$ be satisfied in the long run. We show that our framework\ncan be utilized to solve a relaxed version of online learning with side\nconstraints addressed in \\cite{DBLP:conf/colt/MannorT06} and\n\\cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an online\nconvex-concave optimization problem, we propose an efficient algorithm which\nachieves $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret bound and\n$\\tilde{\\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then we\nmodify the algorithm in order to guarantee that the constraints are satisfied\nin the long run. This gain is achieved at the price of getting\n$\\tilde{\\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based on\nthe Mirror Prox method \\citep{nemirovski-2005-prox} to solve variational\ninequalities which achieves $\\tilde{\\mathcal{\\mathcal{O}}}(T^{2/3})$ bound for\nboth regret and the violation of constraints when the domain $\\K$ can be\ndescribed by a finite number of linear constraints. Finally, we extend the\nresult to the setting where we only have partial access to the convex set\n$\\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the same\nbounds in expectation as our first algorithm.",
    "published": "2011-11-25T18:51:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Regret Bound by Variation for Online Convex Optimization",
    "authors": [
      "Tianbao Yang",
      "Mehrdad Mahdavi",
      "Rong Jin",
      "Shenghuo Zhu"
    ],
    "summary": "In citep{Hazan-2008-extract}, the authors showed that the regret of online\nlinear optimization can be bounded by the total variation of the cost vectors.\nIn this paper, we extend this result to general online convex optimization. We\nfirst analyze the limitations of the algorithm in \\citep{Hazan-2008-extract}\nwhen applied it to online convex optimization. We then present two algorithms\nfor online convex optimization whose regrets are bounded by the variation of\ncost functions. We finally consider the bandit setting, and present a\nrandomized algorithm for online bandit convex optimization with a\nvariation-based regret bound. We show that the regret bound for online bandit\nconvex optimization is optimal when the variation of cost functions is\nindependent of the number of trials.",
    "published": "2011-11-28T03:50:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning in embodied action-perception loops through exploration",
    "authors": [
      "Daniel Y. Little",
      "Friedrich T. Sommer"
    ],
    "summary": "Although exploratory behaviors are ubiquitous in the animal kingdom, their\ncomputational underpinnings are still largely unknown. Behavioral Psychology\nhas identified learning as a primary drive underlying many exploratory\nbehaviors. Exploration is seen as a means for an animal to gather sensory data\nuseful for reducing its ignorance about the environment. While related problems\nhave been addressed in Data Mining and Reinforcement Learning, the\ncomputational modeling of learning-driven exploration by embodied agents is\nlargely unrepresented.\n  Here, we propose a computational theory for learning-driven exploration based\non the concept of missing information that allows an agent to identify\ninformative actions using Bayesian inference. We demonstrate that when\nembodiment constraints are high, agents must actively coordinate their actions\nto learn efficiently. Compared to earlier approaches, our exploration policy\nyields more efficient learning across a range of worlds with diverse\nstructures. The improved learning in turn affords greater success in general\ntasks including navigation and reward gathering. We conclude by discussing how\nthe proposed theory relates to previous information-theoretic objectives of\nbehavior, such as predictive information and the free energy principle, and how\nit might contribute to a general theory of exploratory behavior.",
    "published": "2011-12-06T00:13:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Identity for Kernel Ridge Regression",
    "authors": [
      "Fedor Zhdanov",
      "Yuri Kalnishkan"
    ],
    "summary": "This paper derives an identity connecting the square loss of ridge regression\nin on-line mode with the loss of the retrospectively best regressor. Some\ncorollaries about the properties of the cumulative loss of on-line ridge\nregression are also obtained.",
    "published": "2011-12-06T20:15:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bipartite ranking algorithm for classification and survival analysis",
    "authors": [
      "Marina Sapir"
    ],
    "summary": "Unsupervised aggregation of independently built univariate predictors is\nexplored as an alternative regularization approach for noisy, sparse datasets.\nBipartite ranking algorithm Smooth Rank implementing this approach is\nintroduced. The advantages of this algorithm are demonstrated on two types of\nproblems. First, Smooth Rank is applied to two-class problems from bio-medical\nfield, where ranking is often preferable to classification. In comparison\nagainst SVMs with radial and linear kernels, Smooth Rank had the best\nperformance on 8 out of 12 benchmark benchmarks. The second area of application\nis survival analysis, which is reduced here to bipartite ranking in a way which\nallows one to use commonly accepted measures of methods performance. In\ncomparison of Smooth Rank with Cox PH regression and CoxPath methods, Smooth\nRank proved to be the best on 9 out of 10 benchmark datasets.",
    "published": "2011-12-08T21:33:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Analysis and Extension of Arc-Cosine Kernels for Large Margin\n  Classification",
    "authors": [
      "Youngmin Cho",
      "Lawrence K. Saul"
    ],
    "summary": "We investigate a recently proposed family of positive-definite kernels that\nmimic the computation in large neural networks. We examine the properties of\nthese kernels using tools from differential geometry; specifically, we analyze\nthe geometry of surfaces in Hilbert space that are induced by these kernels.\nWhen this geometry is described by a Riemannian manifold, we derive results for\nthe metric, curvature, and volume element. Interestingly, though, we find that\nthe simplest kernel in this family does not admit such an interpretation. We\nexplore two variations of these kernels that mimic computation in neural\nnetworks with different activation functions. We experiment with these new\nkernels on several data sets and highlight their general trends in performance\nfor classification.",
    "published": "2011-12-16T05:21:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Nonnegative Matrix Factorization for Semi-supervised Dimensionality\n  Reduction",
    "authors": [
      "Youngmin Cho",
      "Lawrence K. Saul"
    ],
    "summary": "We show how to incorporate information from labeled examples into nonnegative\nmatrix factorization (NMF), a popular unsupervised learning algorithm for\ndimensionality reduction. In addition to mapping the data into a space of lower\ndimensionality, our approach aims to preserve the nonnegative components of the\ndata that are important for classification. We identify these components from\nthe support vectors of large-margin classifiers and derive iterative updates to\npreserve them in a semi-supervised version of NMF. These updates have a simple\nmultiplicative form like their unsupervised counterparts; they are also\nguaranteed at each iteration to decrease their loss function---a weighted sum\nof I-divergences that captures the trade-off between unsupervised and\nsupervised learning. We evaluate these updates for dimensionality reduction\nwhen they are used as a precursor to linear classification. In this role, we\nfind that they yield much better performance than their unsupervised\ncounterparts. We also find one unexpected benefit of the low dimensional\nrepresentations discovered by our approach: often they yield more accurate\nclassifiers than both ordinary and transductive SVMs trained in the original\ninput space.",
    "published": "2011-12-16T05:33:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering and Latent Semantic Indexing Aspects of the Nonnegative\n  Matrix Factorization",
    "authors": [
      "Andri Mirzal"
    ],
    "summary": "This paper provides a theoretical support for clustering aspect of the\nnonnegative matrix factorization (NMF). By utilizing the Karush-Kuhn-Tucker\noptimality conditions, we show that NMF objective is equivalent to graph\nclustering objective, so clustering aspect of the NMF has a solid\njustification. Different from previous approaches which usually discard the\nnonnegativity constraints, our approach guarantees the stationary point being\nused in deriving the equivalence is located on the feasible region in the\nnonnegative orthant. Additionally, since clustering capability of a matrix\ndecomposition technique can sometimes imply its latent semantic indexing (LSI)\naspect, we will also evaluate LSI aspect of the NMF by showing its capability\nin solving the synonymy and polysemy problems in synthetic datasets. And more\nextensive evaluation will be conducted by comparing LSI performances of the NMF\nand the singular value decomposition (SVD), the standard LSI method, using some\nstandard datasets.",
    "published": "2011-12-17T03:57:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evaluation of Performance Measures for Classifiers Comparison",
    "authors": [
      "Vincent Labatut",
      "Hocine Cherifi"
    ],
    "summary": "The selection of the best classification algorithm for a given dataset is a\nvery widespread problem, occuring each time one has to choose a classifier to\nsolve a real-world problem. It is also a complex task with many important\nmethodological decisions to make. Among those, one of the most crucial is the\nchoice of an appropriate measure in order to properly assess the classification\nperformance and rank the algorithms. In this article, we focus on this specific\ntask. We present the most popular measures and compare their behavior through\ndiscrimination plots. We then discuss their properties from a more theoretical\nperspective. It turns out several of them are equivalent for classifiers\ncomparison purposes. Futhermore. they can also lead to interpretation problems.\nAmong the numerous measures proposed over the years, it appears that the\nclassical overall success rate and marginal rates are the more suitable for\nclassifier comparison task.",
    "published": "2011-12-18T08:02:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Modeling transition dynamics in MDPs with RKHS embeddings of conditional\n  distributions",
    "authors": [
      "Steffen Grünewälder",
      "Luca Baldassarre",
      "Massimiliano Pontil",
      "Arthur Gretton",
      "Guy Lever"
    ],
    "summary": "We propose a new, nonparametric approach to estimating the value function in\nreinforcement learning. This approach makes use of a recently developed\nrepresentation of conditional distributions as functions in a reproducing\nkernel Hilbert space. Such representations bypass the need for estimating\ntransition probabilities, and apply to any domain on which kernels can be\ndefined. Our approach avoids the need to approximate intractable integrals\nsince expectations are represented as RKHS inner products whose computation has\nlinear complexity in the sample size. Thus, we can efficiently perform value\nfunction estimation in a wide variety of settings, including finite state\nspaces, continuous states spaces, and partially observable tasks where only\nsensor measurements are available. A second advantage of the approach is that\nwe learn the conditional distribution representation from a training sample,\nand do not require an exhaustive exploration of the state space. We prove\nconvergence of our approach either to the optimal policy, or to the closest\nprojection of the optimal policy in our model class, under reasonable\nassumptions. In experiments, we demonstrate the performance of our algorithm on\na learning task in a continuous state space (the under-actuated pendulum), and\non a navigation problem where only images from a sensor are observed. We\ncompare with least-squares policy iteration where a Gaussian process is used\nfor value function estimation. Our algorithm achieves better performance in\nboth tasks.",
    "published": "2011-12-20T15:21:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Combining One-Class Classifiers via Meta-Learning",
    "authors": [
      "Eitan Menahem",
      "Lior Rokach",
      "Yuval Elovici"
    ],
    "summary": "Selecting the best classifier among the available ones is a difficult task,\nespecially when only instances of one class exist. In this work we examine the\nnotion of combining one-class classifiers as an alternative for selecting the\nbest classifier. In particular, we propose two new one-class classification\nperformance measures to weigh classifiers and show that a simple ensemble that\nimplements these measures can outperform the most popular one-class ensembles.\nFurthermore, we propose a new one-class ensemble scheme, TUPSO, which uses\nmeta-learning to combine one-class classifiers. Our experiments demonstrate the\nsuperiority of TUPSO over all other tested ensembles and show that the TUPSO\nperformance is statistically indistinguishable from that of the hypothetical\nbest classifier.",
    "published": "2011-12-22T08:07:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Building high-level features using large scale unsupervised learning",
    "authors": [
      "Quoc V. Le",
      "Marc'Aurelio Ranzato",
      "Rajat Monga",
      "Matthieu Devin",
      "Kai Chen",
      "Greg S. Corrado",
      "Jeff Dean",
      "Andrew Y. Ng"
    ],
    "summary": "We consider the problem of building high-level, class-specific feature\ndetectors from only unlabeled data. For example, is it possible to learn a face\ndetector using only unlabeled images? To answer this, we train a 9-layered\nlocally connected sparse autoencoder with pooling and local contrast\nnormalization on a large dataset of images (the model has 1 billion\nconnections, the dataset has 10 million 200x200 pixel images downloaded from\nthe Internet). We train this network using model parallelism and asynchronous\nSGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to\nwhat appears to be a widely-held intuition, our experimental results reveal\nthat it is possible to train a face detector without having to label images as\ncontaining a face or not. Control experiments show that this feature detector\nis robust not only to translation but also to scaling and out-of-plane\nrotation. We also find that the same network is sensitive to other high-level\nconcepts such as cat faces and human bodies. Starting with these learned\nfeatures, we trained our network to obtain 15.8% accuracy in recognizing 20,000\nobject categories from ImageNet, a leap of 70% relative improvement over the\nprevious state-of-the-art.",
    "published": "2011-12-29T00:26:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Two-Manifold Problems",
    "authors": [
      "Byron Boots",
      "Geoffrey J. Gordon"
    ],
    "summary": "Recently, there has been much interest in spectral approaches to learning\nmanifolds---so-called kernel eigenmap methods. These methods have had some\nsuccesses, but their applicability is limited because they are not robust to\nnoise. To address this limitation, we look at two-manifold problems, in which\nwe simultaneously reconstruct two related manifolds, each representing a\ndifferent view of the same data. By solving these interconnected learning\nproblems together and allowing information to flow between them, two-manifold\nalgorithms are able to succeed where a non-integrated approach would fail: each\nview allows us to suppress noise in the other, reducing bias in the same way\nthat an instrumental variable allows us to remove bias in a {linear}\ndimensionality reduction problem. We propose a class of algorithms for\ntwo-manifold problems, based on spectral decomposition of cross-covariance\noperators in Hilbert space. Finally, we discuss situations where two-manifold\nproblems are useful, and demonstrate that solving a two-manifold problem can\naid in learning a nonlinear dynamical system from limited data.",
    "published": "2011-12-29T19:52:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "T-Learning",
    "authors": [
      "Vincent Graziano",
      "Faustino Gomez",
      "Mark Ring",
      "Juergen Schmidhuber"
    ],
    "summary": "Traditional Reinforcement Learning (RL) has focused on problems involving\nmany states and few actions, such as simple grid worlds. Most real world\nproblems, however, are of the opposite type, Involving Few relevant states and\nmany actions. For example, to return home from a conference, humans identify\nonly few subgoal states such as lobby, taxi, airport etc. Each valid behavior\nconnecting two such states can be viewed as an action, and there are trillions\nof them. Assuming the subgoal identification problem is already solved, the\nquality of any RL method---in real-world settings---depends less on how well it\nscales with the number of states than on how well it scales with the number of\nactions. This is where our new method T-Learning excels, by evaluating the\nrelatively few possible transits from one state to another in a\npolicy-independent way, rather than a huge number of state-action pairs, or\nstates in traditional policy-dependent ways. Illustrative experiments\ndemonstrate that performance improvements of T-Learning over Q-learning can be\narbitrarily large.",
    "published": "2011-12-31T17:29:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Topic Modeling Toolbox Using Belief Propagation",
    "authors": [
      "Jia Zeng"
    ],
    "summary": "Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model\nfor probabilistic topic modeling, which attracts worldwide interests and\ntouches on many important applications in text mining, computer vision and\ncomputational biology. This paper introduces a topic modeling toolbox (TMBP)\nbased on the belief propagation (BP) algorithms. TMBP toolbox is implemented by\nMEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing\ntopic modeling packages, the novelty of this toolbox lies in the BP algorithms\nfor learning LDA-based topic models. The current version includes BP algorithms\nfor latent Dirichlet allocation (LDA), author-topic models (ATM), relational\ntopic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project\nand more BP-based algorithms for various topic models will be added in the near\nfuture. Interested users may also extend BP algorithms for learning more\ncomplicated topic models. The source codes are freely available under the GNU\nGeneral Public Licence, Version 1.0 at https://mloss.org/software/view/399/.",
    "published": "2012-01-04T07:07:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Customers Behavior Modeling by Semi-Supervised Learning in Customer\n  Relationship Management",
    "authors": [
      "Siavash Emtiyaz",
      "MohammadReza Keyvanpour"
    ],
    "summary": "Leveraging the power of increasing amounts of data to analyze customer base\nfor attracting and retaining the most valuable customers is a major problem\nfacing companies in this information age. Data mining technologies extract\nhidden information and knowledge from large data stored in databases or data\nwarehouses, thereby supporting the corporate decision making process. CRM uses\ndata mining (one of the elements of CRM) techniques to interact with customers.\nThis study investigates the use of a technique, semi-supervised learning, for\nthe management and analysis of customer-related data warehouse and information.\nThe idea of semi-supervised learning is to learn not only from the labeled\ntraining data, but to exploit also the structural information in additionally\navailable unlabeled data. The proposed semi-supervised method is a model by\nmeans of a feed-forward neural network trained by a back propagation algorithm\n(multi-layer perceptron) in order to predict the category of an unknown\ncustomer (potential customers). In addition, this technique can be used with\nRapid Miner tools for both labeled and unlabeled data.",
    "published": "2012-01-08T23:59:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Automatic Detection of Diabetes Diagnosis using Feature Weighted Support\n  Vector Machines based on Mutual Information and Modified Cuckoo Search",
    "authors": [
      "Davar Giveki",
      "Hamid Salimi",
      "GholamReza Bahmanyar",
      "Younes Khademian"
    ],
    "summary": "Diabetes is a major health problem in both developing and developed countries\nand its incidence is rising dramatically. In this study, we investigate a novel\nautomatic approach to diagnose Diabetes disease based on Feature Weighted\nSupport Vector Machines (FW-SVMs) and Modified Cuckoo Search (MCS). The\nproposed model consists of three stages: Firstly, PCA is applied to select an\noptimal subset of features out of set of all the features. Secondly, Mutual\nInformation is employed to construct the FWSVM by weighting different features\nbased on their degree of importance. Finally, since parameter selection plays a\nvital role in classification accuracy of SVMs, MCS is applied to select the\nbest parameter values. The proposed MI-MCS-FWSVM method obtains 93.58% accuracy\non UCI dataset. The experimental results demonstrate that our method\noutperforms the previous methods by not only giving more accurate results but\nalso significantly speeding up the classification procedure.",
    "published": "2012-01-10T11:03:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Low-Rank Kernel Learning for Regression",
    "authors": [
      "Pierre Machart",
      "Thomas Peel",
      "Liva Ralaivola",
      "Sandrine Anthoine",
      "Hervé Glotin"
    ],
    "summary": "We present a novel approach to learn a kernel-based regression function. It\nis based on the useof conical combinations of data-based parameterized kernels\nand on a new stochastic convex optimization procedure of which we establish\nconvergence guarantees. The overall learning procedure has the nice properties\nthat a) the learned conical combination is automatically designed to perform\nthe regression task at hand and b) the updates implicated by the optimization\nprocedure are quite inexpensive. In order to shed light on the appositeness of\nour learning strategy, we present empirical results from experiments conducted\non various benchmark datasets.",
    "published": "2012-01-11T21:03:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Acoustical Quality Assessment of the Classroom Environment",
    "authors": [
      "Marian George",
      "Moustafa Youssef"
    ],
    "summary": "Teaching is one of the most important factors affecting any education system.\nMany research efforts have been conducted to facilitate the presentation modes\nused by instructors in classrooms as well as provide means for students to\nreview lectures through web browsers. Other studies have been made to provide\nacoustical design recommendations for classrooms like room size and\nreverberation times. However, using acoustical features of classrooms as a way\nto provide education systems with feedback about the learning process was not\nthoroughly investigated in any of these studies. We propose a system that\nextracts different sound features of students and instructors, and then uses\nmachine learning techniques to evaluate the acoustical quality of any learning\nenvironment. We infer conclusions about the students' satisfaction with the\nquality of lectures. Using classifiers instead of surveys and other subjective\nways of measures can facilitate and speed such experiments which enables us to\nperform them continuously. We believe our system enables education systems to\ncontinuously review and improve their teaching strategies and acoustical\nquality of classrooms.",
    "published": "2012-01-13T17:46:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Efficient Primal-Dual Prox Method for Non-Smooth Optimization",
    "authors": [
      "Tianbao Yang",
      "Mehrdad Mahdavi",
      "Rong Jin",
      "Shenghuo Zhu"
    ],
    "summary": "We study the non-smooth optimization problems in machine learning, where both\nthe loss function and the regularizer are non-smooth functions. Previous\nstudies on efficient empirical loss minimization assume either a smooth loss\nfunction or a strongly convex regularizer, making them unsuitable for\nnon-smooth optimization. We develop a simple yet efficient method for a family\nof non-smooth optimization problems where the dual form of the loss function is\nbilinear in primal and dual variables. We cast a non-smooth optimization\nproblem into a minimax optimization problem, and develop a primal dual prox\nmethod that solves the minimax optimization problem at a rate of $O(1/T)$\n{assuming that the proximal step can be efficiently solved}, significantly\nfaster than a standard subgradient descent method that has an $O(1/\\sqrt{T})$\nconvergence rate. Our empirical study verifies the efficiency of the proposed\nmethod for various non-smooth optimization problems that arise ubiquitously in\nmachine learning by comparing it to the state-of-the-art first order methods.",
    "published": "2012-01-24T04:09:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Comparison Between Data Mining Prediction Algorithms for Fault\n  Detection(Case study: Ahanpishegan co.)",
    "authors": [
      "Golriz Amooee",
      "Behrouz Minaei-Bidgoli",
      "Malihe Bagheri-Dehnavi"
    ],
    "summary": "In the current competitive world, industrial companies seek to manufacture\nproducts of higher quality which can be achieved by increasing reliability,\nmaintainability and thus the availability of products. On the other hand,\nimprovement in products lifecycle is necessary for achieving high reliability.\nTypically, maintenance activities are aimed to reduce failures of industrial\nmachinery and minimize the consequences of such failures. So the industrial\ncompanies try to improve their efficiency by using different fault detection\ntechniques. One strategy is to process and analyze previous generated data to\npredict future failures. The purpose of this paper is to detect wasted parts\nusing different data mining algorithms and compare the accuracy of these\nalgorithms. A combination of thermal and physical characteristics has been used\nand the algorithms were implemented on Ahanpishegan's current data to estimate\nthe availability of its produced parts.\n  Keywords: Data Mining, Fault Detection, Availability, Prediction Algorithms.",
    "published": "2012-01-29T16:23:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Active Learning of Custering with Side Information Using $\\eps$-Smooth\n  Relative Regret Approximations",
    "authors": [
      "Nir Ailon",
      "Ron Begleiter"
    ],
    "summary": "Clustering is considered a non-supervised learning setting, in which the goal\nis to partition a collection of data points into disjoint clusters. Often a\nbound $k$ on the number of clusters is given or assumed by the practitioner.\nMany versions of this problem have been defined, most notably $k$-means and\n$k$-median.\n  An underlying problem with the unsupervised nature of clustering it that of\ndetermining a similarity function. One approach for alleviating this difficulty\nis known as clustering with side information, alternatively, semi-supervised\nclustering. Here, the practitioner incorporates side information in the form of\n\"must be clustered\" or \"must be separated\" labels for data point pairs. Each\nsuch piece of information comes at a \"query cost\" (often involving human\nresponse solicitation). The collection of labels is then incorporated in the\nusual clustering algorithm as either strict or as soft constraints, possibly\nadding a pairwise constraint penalty function to the chosen clustering\nobjective.\n  Our work is mostly related to clustering with side information. We ask how to\nchoose the pairs of data points. Our analysis gives rise to a method provably\nbetter than simply choosing them uniformly at random. Roughly speaking, we show\nthat the distribution must be biased so as more weight is placed on pairs\nincident to elements in smaller clusters in some optimal solution. Of course we\ndo not know the optimal solution, hence we don't know the bias. Using the\nrecently introduced method of $\\eps$-smooth relative regret approximations of\nAilon, Begleiter and Ezra, we can show an iterative process that improves both\nthe clustering and the bias in tandem. The process provably converges to the\noptimal solution faster (in terms of query cost) than an algorithm selecting\npairs uniformly.",
    "published": "2012-01-31T07:46:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Contextual Bandit Learning with Predictable Rewards",
    "authors": [
      "Alekh Agarwal",
      "Miroslav Dudík",
      "Satyen Kale",
      "John Langford",
      "Robert E. Schapire"
    ],
    "summary": "Contextual bandit learning is a reinforcement learning problem where the\nlearner repeatedly receives a set of features (context), takes an action and\nreceives a reward based on the action and context. We consider this problem\nunder a realizability assumption: there exists a function in a (known) function\nclass, always capable of predicting the expected reward, given the action and\ncontext. Under this assumption, we show three things. We present a new\nalgorithm---Regressor Elimination--- with a regret similar to the agnostic\nsetting (i.e. in the absence of realizability assumption). We prove a new lower\nbound showing no algorithm can achieve superior performance in the worst case\neven with the realizability assumption. However, we do show that for any set of\npolicies (mapping contexts to actions), there is a distribution over rewards\n(given context) such that our new algorithm has constant regret unlike the\nprevious approaches.",
    "published": "2012-02-07T02:27:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Performance of Maximum Likelihood Inverse Reinforcement Learning",
    "authors": [
      "Héctor Ratia",
      "Luis Montesano",
      "Ruben Martinez-Cantin"
    ],
    "summary": "Inverse reinforcement learning (IRL) addresses the problem of recovering a\ntask description given a demonstration of the optimal policy used to solve such\na task. The optimal policy is usually provided by an expert or teacher, making\nIRL specially suitable for the problem of apprenticeship learning. The task\ndescription is encoded in the form of a reward function of a Markov decision\nprocess (MDP). Several algorithms have been proposed to find the reward\nfunction corresponding to a set of demonstrations. One of the algorithms that\nhas provided best results in different applications is a gradient method to\noptimize a policy squared error criterion. On a parallel line of research,\nother authors have presented recently a gradient approximation of the maximum\nlikelihood estimate of the reward signal. In general, both approaches\napproximate the gradient estimate and the criteria at different stages to make\nthe algorithm tractable and efficient. In this work, we provide a detailed\ndescription of the different methods to highlight differences in terms of\nreward estimation, policy similarity and computational costs. We also provide\nexperimental results to evaluate the differences in performance of the methods.",
    "published": "2012-02-07T23:14:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PAC Bounds for Discounted MDPs",
    "authors": [
      "Tor Lattimore",
      "Marcus Hutter"
    ],
    "summary": "We study upper and lower bounds on the sample-complexity of learning\nnear-optimal behaviour in finite-state discounted Markov Decision Processes\n(MDPs). For the upper bound we make the assumption that each action leads to at\nmost two possible next-states and prove a new bound for a UCRL-style algorithm\non the number of time-steps when it is not Probably Approximately Correct\n(PAC). The new lower bound strengthens previous work by being both more general\n(it applies to all policies) and tighter. The upper and lower bounds match up\nto logarithmic factors.",
    "published": "2012-02-17T11:59:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Confusion Matrix Stability Bounds for Multiclass Classification",
    "authors": [
      "Pierre Machart",
      "Liva Ralaivola"
    ],
    "summary": "In this paper, we provide new theoretical results on the generalization\nproperties of learning algorithms for multiclass classification problems. The\noriginality of our work is that we propose to use the confusion matrix of a\nclassifier as a measure of its quality; our contribution is in the line of work\nwhich attempts to set up and study the statistical properties of new evaluation\nmeasures such as, e.g. ROC curves. In the confusion-based learning framework we\npropose, we claim that a targetted objective is to minimize the size of the\nconfusion matrix C, measured through its operator norm ||C||. We derive\ngeneralization bounds on the (size of the) confusion matrix in an extended\nframework of uniform stability, adapted to the case of matrix valued loss.\nPivotal to our study is a very recent matrix concentration inequality that\ngeneralizes McDiarmid's inequality. As an illustration of the relevance of our\ntheoretical results, we show how two SVM learning procedures can be proved to\nbe confusion-friendly. To the best of our knowledge, the present paper is the\nfirst that focuses on the confusion matrix from a theoretical point of view.",
    "published": "2012-02-28T14:03:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Application of Gist SVM in Cancer Detection",
    "authors": [
      "S. Aruna",
      "S. P. Rajagopalan",
      "L. V. Nandakishore"
    ],
    "summary": "In this paper, we study the application of GIST SVM in disease prediction\n(detection of cancer). Pattern classification problems can be effectively\nsolved by Support vector machines. Here we propose a classifier which can\ndifferentiate patients having benign and malignant cancer cells. To improve the\naccuracy of classification, we propose to determine the optimal size of the\ntraining set and perform feature selection. To find the optimal size of the\ntraining set, different sizes of training sets are experimented and the one\nwith highest classification rate is selected. The optimal features are selected\nthrough their F-Scores.",
    "published": "2012-03-01T14:40:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Necessity of Irrelevant Variables",
    "authors": [
      "David P. Helmbold",
      "Philip M. Long"
    ],
    "summary": "This work explores the effects of relevant and irrelevant boolean variables\non the accuracy of classifiers. The analysis uses the assumption that the\nvariables are conditionally independent given the class, and focuses on a\nnatural family of learning algorithms for such sources when the relevant\nvariables have a small advantage over random guessing. The main result is that\nalgorithms relying predominately on irrelevant variables have error\nprobabilities that quickly go to 0 in situations where algorithms that limit\nthe use of irrelevant variables have errors bounded below by a positive\nconstant. We also show that accurate learning is possible even when there are\nso few examples that one cannot determine with high confidence whether or not\nany individual variable is relevant.",
    "published": "2012-03-12T17:17:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Data Mining: A Prediction for Performance Improvement of Engineering\n  Students using Classification",
    "authors": [
      "Surjeet Kumar Yadav",
      "Saurabh Pal"
    ],
    "summary": "Now-a-days the amount of data stored in educational database increasing\nrapidly. These databases contain hidden information for improvement of\nstudents' performance. Educational data mining is used to study the data\navailable in the educational field and bring out the hidden knowledge from it.\nClassification methods like decision trees, Bayesian network etc can be applied\non the educational data for predicting the student's performance in\nexamination. This prediction will help to identify the weak students and help\nthem to score better marks. The C4.5, ID3 and CART decision tree algorithms are\napplied on engineering student's data to predict their performance in the final\nexam. The outcome of the decision tree predicted the number of students who are\nlikely to pass, fail or promoted to next year. The results provide steps to\nimprove the performance of the students who were predicted to fail or promoted.\nAfter the declaration of the results in the final examination the marks\nobtained by the students are fed into the system and the results were analyzed\nfor the next session. The comparative analysis of the results states that the\nprediction has helped the weaker students to improve and brought out betterment\nin the result.",
    "published": "2012-03-17T02:06:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Mixture Methods Based on Bregman Divergences",
    "authors": [
      "Mehmet A. Donmez",
      "Huseyin A. Inan",
      "Suleyman S. Kozat"
    ],
    "summary": "We investigate adaptive mixture methods that linearly combine outputs of $m$\nconstituent filters running in parallel to model a desired signal. We use\n\"Bregman divergences\" and obtain certain multiplicative updates to train the\nlinear combination weights under an affine constraint or without any\nconstraints. We use unnormalized relative entropy and relative entropy to\ndefine two different Bregman divergences that produce an unnormalized\nexponentiated gradient update and a normalized exponentiated gradient update on\nthe mixture weights, respectively. We then carry out the mean and the\nmean-square transient analysis of these adaptive algorithms when they are used\nto combine outputs of $m$ constituent filters. We illustrate the accuracy of\nour results and demonstrate the effectiveness of these updates for sparse\nmixture systems.",
    "published": "2012-03-20T21:32:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Very Short Literature Survey From Supervised Learning To Surrogate\n  Modeling",
    "authors": [
      "Altay Brusan"
    ],
    "summary": "The past century was era of linear systems. Either systems (especially\nindustrial ones) were simple (quasi)linear or linear approximations were\naccurate enough. In addition, just at the ending decades of the century\nprofusion of computing devices were available, before then due to lack of\ncomputational resources it was not easy to evaluate available nonlinear system\nstudies. At the moment both these two conditions changed, systems are highly\ncomplex and also pervasive amount of computation strength is cheap and easy to\nachieve. For recent era, a new branch of supervised learning well known as\nsurrogate modeling (meta-modeling, surface modeling) has been devised which\naimed at answering new needs of modeling realm. This short literature survey is\non to introduce surrogate modeling to whom is familiar with the concepts of\nsupervised learning. Necessity, challenges and visions of the topic are\nconsidered.",
    "published": "2012-03-21T17:29:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Credal Classification based on AODE and compression coefficients",
    "authors": [
      "Giorgio Corani",
      "Alessandro Antonucci"
    ],
    "summary": "Bayesian model averaging (BMA) is an approach to average over alternative\nmodels; yet, it usually gets excessively concentrated around the single most\nprobable model, therefore achieving only sub-optimal classification\nperformance. The compression-based approach (Boulle, 2007) overcomes this\nproblem, averaging over the different models by applying a logarithmic\nsmoothing over the models' posterior probabilities. This approach has shown\nexcellent performances when applied to ensembles of naive Bayes classifiers.\nAODE is another ensemble of models with high performance (Webb, 2005), based on\na collection of non-naive classifiers (called SPODE) whose probabilistic\npredictions are aggregated by simple arithmetic mean. Aggregating the SPODEs\nvia BMA rather than by arithmetic mean deteriorates the performance; instead,\nwe aggregate the SPODEs via the compression coefficients and we show that the\nresulting classifier obtains a slight but consistent improvement over AODE.\nHowever, an important issue in any Bayesian ensemble of models is the\narbitrariness in the choice of the prior over the models. We address this\nproblem by the paradigm of credal classification, namely by substituting the\nunique prior with a set of priors. Credal classifier automatically recognize\nthe prior-dependent instances, namely the instances whose most probable class\nvaries, when different priors are considered; in these cases, credal\nclassifiers remain reliable by returning a set of classes rather than a single\nclass. We thus develop the credal version of both the BMA-based and the\ncompression-based ensemble of SPODEs, substituting the single prior over the\nmodels by a set of priors. Experiments show that both credal classifiers\nprovide higher classification reliability than their determinate counterparts;\nmoreover the compression-based credal classifier compares favorably to previous\ncredal classifiers.",
    "published": "2012-03-26T16:25:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Minimax Classifier for Uncertain Costs",
    "authors": [
      "Rui Wang",
      "Ke Tang"
    ],
    "summary": "Many studies on the cost-sensitive learning assumed that a unique cost matrix\nis known for a problem. However, this assumption may not hold for many\nreal-world problems. For example, a classifier might need to be applied in\nseveral circumstances, each of which associates with a different cost matrix.\nOr, different human experts have different opinions about the costs for a given\nproblem. Motivated by these facts, this study aims to seek the minimax\nclassifier over multiple cost matrices. In summary, we theoretically proved\nthat, no matter how many cost matrices are involved, the minimax problem can be\ntackled by solving a number of standard cost-sensitive problems and\nsub-problems that involve only two cost matrices. As a result, a general\nframework for achieving minimax classifier over multiple cost matrices is\nsuggested and justified by preliminary empirical studies.",
    "published": "2012-05-02T12:38:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Greedy Multiple Instance Learning via Codebook Learning and Nearest\n  Neighbor Voting",
    "authors": [
      "Gang Chen",
      "Jason Corso"
    ],
    "summary": "Multiple instance learning (MIL) has attracted great attention recently in\nmachine learning community. However, most MIL algorithms are very slow and\ncannot be applied to large datasets. In this paper, we propose a greedy\nstrategy to speed up the multiple instance learning process. Our contribution\nis two fold. First, we propose a density ratio model, and show that maximizing\na density ratio function is the low bound of the DD model under certain\nconditions. Secondly, we make use of a histogram ratio between positive bags\nand negative bags to represent the density ratio function and find codebooks\nseparately for positive bags and negative bags by a greedy strategy. For\ntesting, we make use of a nearest neighbor strategy to classify new bags. We\ntest our method on both small benchmark datasets and the large TRECVID MED11\ndataset. The experimental results show that our method yields comparable\naccuracy to the current state of the art, while being up to at least one order\nof magnitude faster.",
    "published": "2012-05-03T04:09:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix\n  Factorization with Automatic Regularization Parameters Determination",
    "authors": [
      "Andri Mirzal"
    ],
    "summary": "We present a converged algorithm for Tikhonov regularized nonnegative matrix\nfactorization (NMF). We specially choose this regularization because it is\nknown that Tikhonov regularized least square (LS) is the more preferable form\nin solving linear inverse problems than the conventional LS. Because an NMF\nproblem can be decomposed into LS subproblems, it can be expected that Tikhonov\nregularized NMF will be the more appropriate approach in solving NMF problems.\nThe algorithm is derived using additive update rules which have been shown to\nhave convergence guarantee. We equip the algorithm with a mechanism to\nautomatically determine the regularization parameters based on the L-curve, a\nwell-known concept in the inverse problems community, but is rather unknown in\nthe NMF research. The introduction of this algorithm thus solves two inherent\nproblems in Tikhonov regularized NMF algorithm research, i.e., convergence\nguarantee and regularization parameters determination.",
    "published": "2012-05-10T03:31:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Constrained Regret Minimization",
    "authors": [
      "Mehrdad Mahdavi",
      "Tianbao Yang",
      "Rong Jin"
    ],
    "summary": "Online learning constitutes a mathematical and compelling framework to\nanalyze sequential decision making problems in adversarial environments. The\nlearner repeatedly chooses an action, the environment responds with an outcome,\nand then the learner receives a reward for the played action. The goal of the\nlearner is to maximize his total reward. However, there are situations in\nwhich, in addition to maximizing the cumulative reward, there are some\nadditional constraints on the sequence of decisions that must be satisfied on\naverage by the learner. In this paper we study an extension to the online\nlearning where the learner aims to maximize the total reward given that some\nadditional constraints need to be satisfied. By leveraging on the theory of\nLagrangian method in constrained optimization, we propose Lagrangian\nexponentially weighted average (LEWA) algorithm, which is a primal-dual variant\nof the well known exponentially weighted average algorithm, to efficiently\nsolve constrained online decision making problems. Using novel theoretical\nanalysis, we establish the regret and the violation of the constraint bounds in\nfull information and bandit feedback models.",
    "published": "2012-05-08T23:06:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Uniqueness Theorem for Clustering",
    "authors": [
      "Reza Bosagh Zadeh",
      "Shai Ben-David"
    ],
    "summary": "Despite the widespread use of Clustering, there is distressingly little\ngeneral theory of clustering available. Questions like \"What distinguishes a\nclustering of data from other data partitioning?\", \"Are there any principles\ngoverning all clustering paradigms?\", \"How should a user choose an appropriate\nclustering algorithm for a particular task?\", etc. are almost completely\nunanswered by the existing body of clustering literature. We consider an\naxiomatic approach to the theory of Clustering. We adopt the framework of\nKleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, we\nsidestep his impossibility result and arrive at a consistent set of axioms. We\nsuggest to extend these axioms, aiming to provide an axiomatic taxonomy of\nclustering paradigms. Such a taxonomy should provide users some guidance\nconcerning the choice of the appropriate clustering paradigm for a given task.\nThe main result of this paper is a set of abstract properties that characterize\nthe Single-Linkage clustering function. This characterization result provides\nnew insight into the properties of desired data groupings that make\nSingle-Linkage the appropriate choice. We conclude by considering a taxonomy of\nclustering functions based on abstract properties that each satisfies.",
    "published": "2012-05-09T18:48:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier",
    "authors": [
      "Jin Yu",
      "S. V. N. Vishwanatan",
      "Jian Zhang"
    ],
    "summary": "A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X =\nx) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has\nbeen shown that Support Vector Machines (SVMs) in the limit are quantile\nclassifiers with t = 1/2 . In this paper, we show that by using asymmetric cost\nof misclassification SVMs can be appropriately extended to recover, in the\nlimit, the quantile binary classifier for any t. We then present a principled\nalgorithm to solve the extended SVM classifier for all values of t\nsimultaneously. This has two implications: First, one can recover the entire\nconditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build\na risk-agnostic SVM classifier where the cost of misclassification need not be\nknown apriori. Preliminary numerical experiments show the effectiveness of the\nproposed algorithm.",
    "published": "2012-05-09T18:46:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Probabilistic Structured Predictors",
    "authors": [
      "Shankar Vembu",
      "Thomas Gartner",
      "Mario Boley"
    ],
    "summary": "We consider MAP estimators for structured prediction with exponential family\nmodels. In particular, we concentrate on the case that efficient algorithms for\nuniform sampling from the output space exist. We show that under this\nassumption (i) exact computation of the partition function remains a hard\nproblem, and (ii) the partition function and the gradient of the log partition\nfunction can be approximated efficiently. Our main result is an approximation\nscheme for the partition function based on Markov Chain Monte Carlo theory. We\nalso show that the efficient uniform sampling assumption holds in several\napplication settings that are of importance in machine learning.",
    "published": "2012-05-09T18:36:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in\n  Weakly Communicating MDPs",
    "authors": [
      "Peter L. Bartlett",
      "Ambuj Tewari"
    ],
    "summary": "We provide an algorithm that achieves the optimal regret rate in an unknown\nweakly communicating Markov Decision Process (MDP). The algorithm proceeds in\nepisodes where, in each episode, it picks a policy using regularization based\non the span of the optimal bias vector. For an MDP with S states and A actions\nwhose optimal bias vector has span bounded by H, we show a regret bound of\n~O(HSpAT). We also relate the span to various diameter-like quantities\nassociated with the MDP, demonstrating how our results improve on previous\nregret bounds.",
    "published": "2012-05-09T14:47:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning",
    "authors": [
      "John Asmuth",
      "Lihong Li",
      "Michael L. Littman",
      "Ali Nouri",
      "David Wingate"
    ],
    "summary": "We present a modular approach to reinforcement learning that uses a Bayesian\nrepresentation of the uncertainty over models. The approach, BOSS (Best of\nSampled Set), drives exploration by sampling multiple models from the posterior\nand selecting actions optimistically. It extends previous work by providing a\nrule for deciding when to resample and how to combine the models. We show that\nour algorithm achieves nearoptimal reward with high probability with a sample\ncomplexity that is low relative to the speed at which the posterior\ndistribution converges during learning. We demonstrate that BOSS performs quite\nfavorably compared to state-of-the-art reinforcement-learning approaches and\nillustrate its flexibility by pairing it with a non-parametric model that\ngeneralizes across states.",
    "published": "2012-05-09T14:42:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Decoupling Exploration and Exploitation in Multi-Armed Bandits",
    "authors": [
      "Orly Avner",
      "Shie Mannor",
      "Ohad Shamir"
    ],
    "summary": "We consider a multi-armed bandit problem where the decision maker can explore\nand exploit different arms at every round. The exploited arm adds to the\ndecision maker's cumulative reward (without necessarily observing the reward)\nwhile the explored arm reveals its value. We devise algorithms for this setup\nand show that the dependence on the number of arms, k, can be much better than\nthe standard square root of k dependence, depending on the behavior of the\narms' reward sequences. For the important case of piecewise stationary\nstochastic bandits, we show a significant improvement over existing algorithms.\nOur algorithms are based on a non-uniform sampling policy, which we show is\nessential to the success of any algorithm in the adversarial setup. Finally, we\nshow some simulation results on an ultra-wide band channel selection inspired\nsetting indicating the applicability of our algorithms.",
    "published": "2012-05-13T15:11:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Normalized Maximum Likelihood Coding for Exponential Family with Its\n  Applications to Optimal Clustering",
    "authors": [
      "So Hirai",
      "Kenji Yamanishi"
    ],
    "summary": "We are concerned with the issue of how to calculate the normalized maximum\nlikelihood (NML) code-length. There is a problem that the normalization term of\nthe NML code-length may diverge when it is continuous and unbounded and a\nstraightforward computation of it is highly expensive when the data domain is\nfinite . In previous works it has been investigated how to calculate the NML\ncode-length for specific types of distributions. We first propose a general\nmethod for computing the NML code-length for the exponential family. Then we\nspecifically focus on Gaussian mixture model (GMM), and propose a new efficient\nmethod for computing the NML to them. We develop it by generalizing Rissanen's\nre-normalizing technique. Then we apply this method to the clustering issue, in\nwhich a clustering structure is modeled using a GMM, and the main task is to\nestimate the optimal number of clusters on the basis of the NML code-length. We\ndemonstrate using artificial data sets the superiority of the NML-based\nclustering over other criteria such as AIC, BIC in terms of the data size\nrequired for high accuracy rate to be achieved.",
    "published": "2012-05-16T03:54:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Visualization of features of a series of measurements with\n  one-dimensional cellular structure",
    "authors": [
      "D. V. Lande"
    ],
    "summary": "This paper describes the method of visualization of periodic constituents and\ninstability areas in series of measurements, being based on the algorithm of\nsmoothing out and concept of one-dimensional cellular automata. A method can be\nused at the analysis of temporal series, related to the volumes of thematic\npublications in web-space.",
    "published": "2012-05-19T08:16:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning",
    "authors": [
      "Constantinos Panagiotakopoulos",
      "Petroula Tsampouka"
    ],
    "summary": "We introduce into the classical perceptron algorithm with margin a mechanism\nthat shrinks the current weight vector as a first step of the update. If the\nshrinking factor is constant the resulting algorithm may be regarded as a\nmargin-error-driven version of NORMA with constant learning rate. In this case\nwe show that the allowed strength of shrinking depends on the value of the\nmaximum margin. We also consider variable shrinking factors for which there is\nno such dependence. In both cases we obtain new generalizations of the\nperceptron with margin able to provably attain in a finite number of steps any\ndesirable approximation of the maximal margin hyperplane. The new approximate\nmaximum margin classifiers appear experimentally to be very competitive in\n2-norm soft margin tasks involving linear kernels.",
    "published": "2012-05-21T19:19:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Safe Exploration in Markov Decision Processes",
    "authors": [
      "Teodor Mihai Moldovan",
      "Pieter Abbeel"
    ],
    "summary": "In environments with uncertain dynamics exploration is necessary to learn how\nto perform well. Existing reinforcement learning algorithms provide strong\nexploration guarantees, but they tend to rely on an ergodicity assumption. The\nessence of ergodicity is that any state is eventually reachable from any other\nstate by following a suitable policy. This assumption allows for exploration\nalgorithms that operate by simply favoring states that have rarely been visited\nbefore. For most physical systems this assumption is impractical as the systems\nwould break before any reasonable exploration has taken place, i.e., most\nphysical systems don't satisfy the ergodicity assumption. In this paper we\naddress the need for safe exploration methods in Markov decision processes. We\nfirst propose a general formulation of safety through ergodicity. We show that\nimposing safety by restricting attention to the resulting set of guaranteed\nsafe policies is NP-hard. We then present an efficient algorithm for guaranteed\nsafe, but potentially suboptimal, exploration. At the core is an optimization\nformulation in which the constraints restrict attention to a subset of the\nguaranteed safe policies and the objective favors exploration policies. Our\nframework is compatible with the majority of previously proposed exploration\nmethods, which rely on an exploration bonus. Our experiments, which include a\nMartian terrain exploration problem, show that our method is able to explore\nbetter than classical exploration methods.",
    "published": "2012-05-22T06:02:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Off-Policy Actor-Critic",
    "authors": [
      "Thomas Degris",
      "Martha White",
      "Richard S. Sutton"
    ],
    "summary": "This paper presents the first actor-critic algorithm for off-policy\nreinforcement learning. Our algorithm is online and incremental, and its\nper-time-step complexity scales linearly with the number of learned weights.\nPrevious work on actor-critic algorithms is limited to the on-policy setting\nand does not take advantage of the recent advances in off-policy gradient\ntemporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable\na target policy to be learned while following and obtaining data from another\n(behavior) policy. For many problems, however, actor-critic methods are more\npractical than action value methods (like Greedy-GQ) because they explicitly\nrepresent the policy; consequently, the policy can be stochastic and utilize a\nlarge action space. In this paper, we illustrate how to practically combine the\ngenerality and learning potential of off-policy learning with the flexibility\nin action selection given by actor-critic methods. We derive an incremental,\nlinear time and space complexity algorithm that includes eligibility traces,\nprove convergence under assumptions similar to previous off-policy algorithms,\nand empirically show better or comparable performance to existing algorithms on\nstandard reinforcement-learning benchmark problems.",
    "published": "2012-05-22T08:36:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiclass Learning Approaches: A Theoretical Comparison with\n  Implications",
    "authors": [
      "Amit Daniely",
      "Sivan Sabato",
      "Shai Shalev Shwartz"
    ],
    "summary": "We theoretically analyze and compare the following five popular multiclass\nclassification methods: One vs. All, All Pairs, Tree-based classifiers, Error\nCorrecting Output Codes (ECOC) with randomly generated code matrices, and\nMulticlass SVM. In the first four methods, the classification is based on a\nreduction to binary classification. We consider the case where the binary\nclassifier comes from a class of VC dimension $d$, and in particular from the\nclass of halfspaces over $\\reals^d$. We analyze both the estimation error and\nthe approximation error of these methods. Our analysis reveals interesting\nconclusions of practical relevance, regarding the success of the different\napproaches under various conditions. Our proof technique employs tools from VC\ntheory to analyze the \\emph{approximation error} of hypothesis classes. This is\nin sharp contrast to most, if not all, previous uses of VC theory, which only\ndeal with estimation error.",
    "published": "2012-05-29T17:40:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Optimization Framework for Semi-Supervised and Transfer Learning\n  using Multiple Classifiers and Clusterers",
    "authors": [
      "Ayan Acharya",
      "Eduardo R. Hruschka",
      "Joydeep Ghosh",
      "Sreangsu Acharyya"
    ],
    "summary": "Unsupervised models can provide supplementary soft constraints to help\nclassify new, \"target\" data since similar instances in the target set are more\nlikely to share the same class label. Such models can also help detect possible\ndifferences between training and target distributions, which is useful in\napplications where concept drift may take place, as in transfer learning\nsettings. This paper describes a general optimization framework that takes as\ninput class membership estimates from existing classifiers learnt on previously\nencountered \"source\" data, as well as a similarity matrix from a cluster\nensemble operating solely on the target data to be classified, and yields a\nconsensus labeling of the target data. This framework admits a wide range of\nloss functions and classification/clustering methods. It exploits properties of\nBregman divergences in conjunction with Legendre duality to yield a principled\nand scalable approach. A variety of experiments show that the proposed\nframework can yield results substantially superior to those provided by popular\ntransductive learning techniques or by naively applying classifiers learnt on\nthe original task to the target data.",
    "published": "2012-04-20T01:58:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction\n  of Lung Cancer Survivability",
    "authors": [
      "George Dimitoglou",
      "James A. Adams",
      "Carol M. Jim"
    ],
    "summary": "Numerous data mining techniques have been developed to extract information\nand identify patterns and predict trends from large data sets. In this study,\ntwo classification techniques, the J48 implementation of the C4.5 algorithm and\na Naive Bayes classifier are applied to predict lung cancer survivability from\nan extensive data set with fifteen years of patient records. The purpose of the\nproject is to verify the predictive effectiveness of the two techniques on\nreal, historical data. Besides the performance outcome that renders J48\nmarginally better than the Naive Bayes technique, there is a detailed\ndescription of the data and the required pre-processing activities. The\nperformance results confirm expectations while some of the issues that appeared\nduring experimentation, underscore the value of having domain-specific\nunderstanding to leverage any domain-specific characteristics inherent in the\ndata.",
    "published": "2012-06-06T04:56:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cumulative Step-size Adaptation on Linear Functions: Technical Report",
    "authors": [
      "Alexandre Adrien Chotard",
      "Anne Auger",
      "Nikolaus Hansen"
    ],
    "summary": "The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,\nwhere the step size is adapted measuring the length of a so-called cumulative\npath. The cumulative path is a combination of the previous steps realized by\nthe algorithm, where the importance of each step decreases with time. This\narticle studies the CSA-ES on composites of strictly increasing with affine\nlinear functions through the investigation of its underlying Markov chains.\nRigorous results on the change and the variation of the step size are derived\nwith and without cumulation. The step-size diverges geometrically fast in most\ncases. Furthermore, the influence of the cumulation parameter is studied.",
    "published": "2012-06-06T13:03:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet\n  Allocation",
    "authors": [
      "Jian-feng Yan",
      "Zhi-Qiang Liu",
      "Yang Gao",
      "Jia Zeng"
    ],
    "summary": "This paper presents a novel communication-efficient parallel belief\npropagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).\nBased on the synchronous belief propagation (BP) algorithm, we first develop a\nparallel belief propagation (PBP) algorithm on the parallel architecture.\nBecause the extensive communication delay often causes a low efficiency of\nparallel topic modeling, we further use Zipf's law to reduce the total\ncommunication cost in PBP. Extensive experiments on different data sets\ndemonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces\nmore than 80% communication cost than the state-of-the-art parallel Gibbs\nsampling (PGS) algorithm.",
    "published": "2012-06-11T13:00:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustered Bandits",
    "authors": [
      "Loc Bui",
      "Ramesh Johari",
      "Shie Mannor"
    ],
    "summary": "We consider a multi-armed bandit setting that is inspired by real-world\napplications in e-commerce. In our setting, there are a few types of users,\neach with a specific response to the different arms. When a user enters the\nsystem, his type is unknown to the decision maker. The decision maker can\neither treat each user separately ignoring the previously observed users, or\ncan attempt to take advantage of knowing that only few types exist and cluster\nthe users according to their response to the arms. We devise algorithms that\ncombine the usual exploration-exploitation tradeoff with clustering of users\nand demonstrate the value of clustering. In the process of developing\nalgorithms for the clustered setting, we propose and analyze simple algorithms\nfor the setup where a decision maker knows that a user belongs to one of few\ntypes, but does not know which one.",
    "published": "2012-06-19T10:26:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exact Soft Confidence-Weighted Learning",
    "authors": [
      "Jialei Wang",
      "Peilin Zhao",
      "Steven C. H. Hoi"
    ],
    "summary": "In this paper, we propose a new Soft Confidence-Weighted (SCW) online\nlearning scheme, which enables the conventional confidence-weighted learning\nmethod to handle non-separable cases. Unlike the previous confidence-weighted\nlearning algorithms, the proposed soft confidence-weighted learning method\nenjoys all the four salient properties: (i) large margin training, (ii)\nconfidence weighting, (iii) capability to handle non-separable data, and (iv)\nadaptive margin. Our experimental results show that the proposed SCW algorithms\nsignificantly outperform the original CW algorithm. When comparing with a\nvariety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we\nfound that SCW generally achieves better or at least comparable predictive\naccuracy, but enjoys significant advantage of computational efficiency (i.e.,\nsmaller number of updates and lower time cost).",
    "published": "2012-06-18T15:00:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Inductive Kernel Low-rank Decomposition with Priors: A Generalized\n  Nystrom Method",
    "authors": [
      "Kai Zhang",
      "Liang Lan",
      "Jun Liu",
      "andreas Rauber",
      "Fabian Moerchen"
    ],
    "summary": "Low-rank matrix decomposition has gained great popularity recently in scaling\nup kernel methods to large amounts of data. However, some limitations could\nprevent them from working effectively in certain domains. For example, many\nexisting approaches are intrinsically unsupervised, which does not incorporate\nside information (e.g., class labels) to produce task specific decompositions;\nalso, they typically work \"transductively\", i.e., the factorization does not\ngeneralize to new samples, so the complete factorization needs to be recomputed\nwhen new samples become available. To solve these problems, in this paper we\npropose an\"inductive\"-flavored method for low-rank kernel decomposition with\npriors. We achieve this by generalizing the Nystr\\\"om method in a novel way. On\nthe one hand, our approach employs a highly flexible, nonparametric structure\nthat allows us to generalize the low-rank factors to arbitrarily new samples;\non the other hand, it has linear time and space complexities, which can be\norders of magnitudes faster than existing approaches and renders great\nefficiency in learning a low-rank kernel decomposition. Empirical results\ndemonstrate the efficacy and efficiency of the proposed method.",
    "published": "2012-06-18T15:04:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Path Integral Policy Improvement with Covariance Matrix Adaptation",
    "authors": [
      "Freek Stulp",
      "Olivier Sigaud"
    ],
    "summary": "There has been a recent focus in reinforcement learning on addressing\ncontinuous state and action problems by optimizing parameterized policies. PI2\nis a recent example of this approach. It combines a derivation from first\nprinciples of stochastic optimal control with tools from statistical estimation\ntheory. In this paper, we consider PI2 as a member of the wider family of\nmethods which share the concept of probability-weighted averaging to\niteratively update parameters to optimize a cost function. We compare PI2 to\nother members of the same family - Cross-Entropy Methods and CMAES - at the\nconceptual level and in terms of performance. The comparison suggests the\nderivation of a novel algorithm which we call PI2-CMA for \"Path Integral Policy\nImprovement with Covariance Matrix Adaptation\". PI2-CMA's main advantage is\nthat it determines the magnitude of the exploration noise automatically.",
    "published": "2012-06-18T15:05:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimizing F-measure: A Tale of Two Approaches",
    "authors": [
      "Ye Nan",
      "Kian Ming Chai",
      "Wee Sun Lee",
      "Hai Leong Chieu"
    ],
    "summary": "F-measures are popular performance metrics, particularly for tasks with\nimbalanced data sets. Algorithms for learning to maximize F-measures follow two\napproaches: the empirical utility maximization (EUM) approach learns a\nclassifier having optimal performance on training data, while the\ndecision-theoretic approach learns a probabilistic model and then predicts\nlabels with maximum expected F-measure. In this paper, we investigate the\ntheoretical justifications and connections for these two approaches, and we\nstudy the conditions under which one approach is preferable to the other using\nsynthetic and real datasets. Given accurate models, our results suggest that\nthe two approaches are asymptotically equivalent given large training and test\nsets. Nevertheless, empirically, the EUM approach appears to be more robust\nagainst model misspecification, and given a good model, the decision-theoretic\napproach appears to be better for handling rare classes and a common domain\nadaptation scenario.",
    "published": "2012-06-18T15:07:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiple Kernel Learning from Noisy Labels by Stochastic Programming",
    "authors": [
      "Tianbao Yang",
      "Mehrdad Mahdavi",
      "Rong Jin",
      "Lijun Zhang",
      "Yang Zhou"
    ],
    "summary": "We study the problem of multiple kernel learning from noisy labels. This is\nin contrast to most of the previous studies on multiple kernel learning that\nmainly focus on developing efficient algorithms and assume perfectly labeled\ntraining examples. Directly applying the existing multiple kernel learning\nalgorithms to noisily labeled examples often leads to suboptimal performance\ndue to the incorrect class assignments. We address this challenge by casting\nmultiple kernel learning from noisy labels into a stochastic programming\nproblem, and presenting a minimax formulation. We develop an efficient\nalgorithm for solving the related convex-concave optimization problem with a\nfast convergence rate of $O(1/T)$ where $T$ is the number of iterations.\nEmpirical studies on UCI data sets verify both the effectiveness of the\nproposed framework and the efficiency of the proposed optimization algorithm.",
    "published": "2012-06-18T15:08:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Decomposed Learning for Structured Prediction",
    "authors": [
      "Rajhans Samdani",
      "Dan Roth"
    ],
    "summary": "Structured prediction is the cornerstone of several machine learning\napplications. Unfortunately, in structured prediction settings with expressive\ninter-variable interactions, exact inference-based learning algorithms, e.g.\nStructural SVM, are often intractable. We present a new way, Decomposed\nLearning (DecL), which performs efficient learning by restricting the inference\nstep to a limited part of the structured spaces. We provide characterizations\nbased on the structure, target parameters, and gold labels, under which DecL is\nequivalent to exact learning. We then show that in real world settings, where\nour theoretical assumptions may not completely hold, DecL-based algorithms are\nsignificantly more efficient and as accurate as exact learning.",
    "published": "2012-06-18T15:08:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Two-Manifold Problems with Applications to Nonlinear System\n  Identification",
    "authors": [
      "Byron Boots",
      "Geoff Gordon"
    ],
    "summary": "Recently, there has been much interest in spectral approaches to learning\nmanifolds---so-called kernel eigenmap methods. These methods have had some\nsuccesses, but their applicability is limited because they are not robust to\nnoise. To address this limitation, we look at two-manifold problems, in which\nwe simultaneously reconstruct two related manifolds, each representing a\ndifferent view of the same data. By solving these interconnected learning\nproblems together, two-manifold algorithms are able to succeed where a\nnon-integrated approach would fail: each view allows us to suppress noise in\nthe other, reducing bias. We propose a class of algorithms for two-manifold\nproblems, based on spectral decomposition of cross-covariance operators in\nHilbert space, and discuss when two-manifold problems are useful. Finally, we\ndemonstrate that solving a two-manifold problem can aid in learning a nonlinear\ndynamical system from limited data.",
    "published": "2012-06-18T15:23:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Modelling transition dynamics in MDPs with RKHS embeddings",
    "authors": [
      "Steffen Grunewalder",
      "Guy Lever",
      "Luca Baldassarre",
      "Massi Pontil",
      "Arthur Gretton"
    ],
    "summary": "We propose a new, nonparametric approach to learning and representing\ntransition dynamics in Markov decision processes (MDPs), which can be combined\neasily with dynamic programming methods for policy optimisation and value\nestimation. This approach makes use of a recently developed representation of\nconditional distributions as \\emph{embeddings} in a reproducing kernel Hilbert\nspace (RKHS). Such representations bypass the need for estimating transition\nprobabilities or densities, and apply to any domain on which kernels can be\ndefined. This avoids the need to calculate intractable integrals, since\nexpectations are represented as RKHS inner products whose computation has\nlinear complexity in the number of points used to represent the embedding. We\nprovide guarantees for the proposed applications in MDPs: in the context of a\nvalue iteration algorithm, we prove convergence to either the optimal policy,\nor to the closest projection of the optimal policy in our model class (an\nRKHS), under reasonable assumptions. In experiments, we investigate a learning\ntask in a typical classical control setting (the under-actuated pendulum), and\non a navigation problem where only images from a sensor are observed. For\npolicy optimisation we compare with least-squares policy iteration where a\nGaussian process is used for value function estimation. For value estimation we\nalso compare to the NPDP method. Our approach achieves better performance in\nall experiments.",
    "published": "2012-06-18T15:25:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning with Augmented Features for Heterogeneous Domain Adaptation",
    "authors": [
      "Lixin Duan",
      "Dong Xu",
      "Ivor Tsang"
    ],
    "summary": "We propose a new learning method for heterogeneous domain adaptation (HDA),\nin which the data from the source domain and the target domain are represented\nby heterogeneous features with different dimensions. Using two different\nprojection matrices, we first transform the data from two domains into a common\nsubspace in order to measure the similarity between the data from two domains.\nWe then propose two new feature mapping functions to augment the transformed\ndata with their original features and zeros. The existing learning methods\n(e.g., SVM and SVR) can be readily incorporated with our newly proposed\naugmented feature representations to effectively utilize the data from both\ndomains for HDA. Using the hinge loss function in SVM as an example, we\nintroduce the detailed objective function in our method called Heterogeneous\nFeature Augmentation (HFA) for a linear case and also describe its\nkernelization in order to efficiently cope with the data with very high\ndimensions. Moreover, we also develop an alternating optimization algorithm to\neffectively solve the nontrivial optimization problem in our HFA method.\nComprehensive experiments on two benchmark datasets clearly demonstrate that\nHFA outperforms the existing HDA methods.",
    "published": "2012-06-18T15:28:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Marginalized Denoising Autoencoders for Domain Adaptation",
    "authors": [
      "Minmin Chen",
      "Zhixiang Xu",
      "Kilian Weinberger",
      "Fei Sha"
    ],
    "summary": "Stacked denoising autoencoders (SDAs) have been successfully used to learn\nnew representations for domain adaptation. Recently, they have attained record\naccuracy on standard benchmark tasks of sentiment analysis across different\ntext domains. SDAs learn robust data representations by reconstruction,\nrecovering original features from data that are artificially corrupted with\nnoise. In this paper, we propose marginalized SDA (mSDA) that addresses two\ncrucial limitations of SDAs: high computational cost and lack of scalability to\nhigh-dimensional features. In contrast to SDAs, our approach of mSDA\nmarginalizes noise and thus does not require stochastic gradient descent or\nother optimization algorithms to learn parameters ? in fact, they are computed\nin closed-form. Consequently, mSDA, which can be implemented in only 20 lines\nof MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.\nFurthermore, the representations learnt by mSDA are as effective as the\ntraditional SDAs, attaining almost identical accuracies in benchmark tasks.",
    "published": "2012-06-18T15:40:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dynamic Pricing under Finite Space Demand Uncertainty: A Multi-Armed\n  Bandit with Dependent Arms",
    "authors": [
      "Pouya Tehrani",
      "Yixuan Zhai",
      "Qing Zhao"
    ],
    "summary": "We consider a dynamic pricing problem under unknown demand models. In this\nproblem a seller offers prices to a stream of customers and observes either\nsuccess or failure in each sale attempt. The underlying demand model is unknown\nto the seller and can take one of N possible forms. In this paper, we show that\nthis problem can be formulated as a multi-armed bandit with dependent arms. We\npropose a dynamic pricing policy based on the likelihood ratio test. We show\nthat the proposed policy achieves complete learning, i.e., it offers a bounded\nregret where regret is defined as the revenue loss with respect to the case\nwith a known demand model. This is in sharp contrast with the logarithmic\ngrowing regret in multi-armed bandit with independent arms.",
    "published": "2012-06-23T00:36:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Practical recommendations for gradient-based training of deep\n  architectures",
    "authors": [
      "Yoshua Bengio"
    ],
    "summary": "Learning algorithms related to artificial neural networks and in particular\nfor Deep Learning may seem to involve many bells and whistles, called\nhyper-parameters. This chapter is meant as a practical guide with\nrecommendations for some of the most commonly used hyper-parameters, in\nparticular in the context of learning algorithms based on back-propagated\ngradient and gradient-based optimization. It also discusses how to deal with\nthe fact that more interesting results can be obtained when allowing one to\nadjust many hyper-parameters. Overall, it describes elements of the practice\nused to successfully and efficiently train and debug large-scale and often deep\nmulti-layer neural networks. It closes with open questions about the training\ndifficulties observed with deeper architectures.",
    "published": "2012-06-24T19:17:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Representation Learning: A Review and New Perspectives",
    "authors": [
      "Yoshua Bengio",
      "Aaron Courville",
      "Pascal Vincent"
    ],
    "summary": "The success of machine learning algorithms generally depends on data\nrepresentation, and we hypothesize that this is because different\nrepresentations can entangle and hide more or less the different explanatory\nfactors of variation behind the data. Although specific domain knowledge can be\nused to help design representations, learning with generic priors can also be\nused, and the quest for AI is motivating the design of more powerful\nrepresentation-learning algorithms implementing such priors. This paper reviews\nrecent work in the area of unsupervised feature learning and deep learning,\ncovering advances in probabilistic models, auto-encoders, manifold learning,\nand deep networks. This motivates longer-term unanswered questions about the\nappropriate objectives for learning good representations, for computing\nrepresentations (i.e., inference), and the geometrical connections between\nrepresentation learning, density estimation and manifold learning.",
    "published": "2012-06-24T20:51:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph Based Classification Methods Using Inaccurate External Classifier\n  Information",
    "authors": [
      "Sundararajan Sellamanickam",
      "Sathiya Keerthi Selvaraj"
    ],
    "summary": "In this paper we consider the problem of collectively classifying entities\nwhere relational information is available across the entities. In practice\ninaccurate class distribution for each entity is often available from another\n(external) classifier. For example this distribution could come from a\nclassifier built using content features or a simple dictionary. Given the\nrelational and inaccurate external classifier information, we consider two\ngraph based settings in which the problem of collective classification can be\nsolved. In the first setting the class distribution is used to fix labels to a\nsubset of nodes and the labels for the remaining nodes are obtained like in a\ntransductive setting. In the other setting the class distributions of all nodes\nare used to define the fitting function part of a graph regularized objective\nfunction. We define a generalized objective function that handles both the\nsettings. Methods like harmonic Gaussian field and local-global consistency\n(LGC) reported in the literature can be seen as special cases. We extend the\nLGC and weighted vote relational neighbor classification (WvRN) methods to\nsupport usage of external classifier information. We also propose an efficient\nleast squares regularization (LSR) based method and relate it to information\nregularization methods. All the methods are evaluated on several benchmark and\nreal world datasets. Considering together speed, robustness and accuracy,\nexperimental results indicate that the LSR and WvRN-extension methods perform\nbetter than other methods.",
    "published": "2012-06-26T08:29:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Neighborhoods for Metric Learning",
    "authors": [
      "Jun Wang",
      "Adam Woznica",
      "Alexandros Kalousis"
    ],
    "summary": "Metric learning methods have been shown to perform well on different learning\ntasks. Many of them rely on target neighborhood relationships that are computed\nin the original feature space and remain fixed throughout learning. As a\nresult, the learned metric reflects the original neighborhood relations. We\npropose a novel formulation of the metric learning problem in which, in\naddition to the metric, the target neighborhood relations are also learned in a\ntwo-step iterative approach. The new formulation can be seen as a\ngeneralization of many existing metric learning methods. The formulation\nincludes a target neighbor assignment rule that assigns different numbers of\nneighbors to instances according to their quality; `high quality' instances get\nmore neighbors. We experiment with two of its instantiations that correspond to\nthe metric learning algorithms LMNN and MCML and compare it to other metric\nlearning methods on a number of datasets. The experimental results show\nstate-of-the-art performance and provide evidence that learning the\nneighborhood relations does improve predictive performance.",
    "published": "2012-06-28T18:57:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Multilabel Classification and Ranking with Partial Feedback",
    "authors": [
      "Claudio Gentile",
      "Francesco Orabona"
    ],
    "summary": "We present a novel multilabel/ranking algorithm working in partial\ninformation settings. The algorithm is based on 2nd-order descent methods, and\nrelies on upper-confidence bounds to trade-off exploration and exploitation. We\nanalyze this algorithm in a partial adversarial setting, where covariates can\nbe adversarial, but multilabel probabilities are ruled by (generalized) linear\nmodels. We show O(T^{1/2} log T) regret bounds, which improve in several ways\non the existing results. We test the effectiveness of our upper-confidence\nscheme by contrasting against full-information baselines on real-world\nmultilabel datasets, often obtaining comparable performance.",
    "published": "2012-06-30T23:07:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hybrid Template Update System for Unimodal Biometric Systems",
    "authors": [
      "Romain Giot",
      "Christophe Rosenberger",
      "Bernadette Dorizzi"
    ],
    "summary": "Semi-supervised template update systems allow to automatically take into\naccount the intra-class variability of the biometric data over time. Such\nsystems can be inefficient by including too many impostor's samples or skipping\ntoo many genuine's samples. In the first case, the biometric reference drifts\nfrom the real biometric data and attracts more often impostors. In the second\ncase, the biometric reference does not evolve quickly enough and also\nprogressively drifts from the real biometric data. We propose a hybrid system\nusing several biometric sub-references in order to increase per- formance of\nself-update systems by reducing the previously cited errors. The proposition is\nvalidated for a keystroke- dynamics authentication system (this modality\nsuffers of high variability over time) on two consequent datasets from the\nstate of the art.",
    "published": "2012-07-03T19:12:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Web-Based Benchmark for Keystroke Dynamics Biometric Systems: A\n  Statistical Analysis",
    "authors": [
      "Romain Giot",
      "Mohamad El-Abed",
      "Christophe Rosenberger"
    ],
    "summary": "Most keystroke dynamics studies have been evaluated using a specific kind of\ndataset in which users type an imposed login and password. Moreover, these\nstudies are optimistics since most of them use different acquisition protocols,\nprivate datasets, controlled environment, etc. In order to enhance the accuracy\nof keystroke dynamics' performance, the main contribution of this paper is\ntwofold. First, we provide a new kind of dataset in which users have typed both\nan imposed and a chosen pairs of logins and passwords. In addition, the\nkeystroke dynamics samples are collected in a web-based uncontrolled\nenvironment (OS, keyboards, browser, etc.). Such kind of dataset is important\nsince it provides us more realistic results of keystroke dynamics' performance\nin comparison to the literature (controlled environment, etc.). Second, we\npresent a statistical analysis of well known assertions such as the\nrelationship between performance and password size, impact of fusion schemes on\nsystem overall performance, and others such as the relationship between\nperformance and entropy. We put into obviousness in this paper some new results\non keystroke dynamics in realistic conditions.",
    "published": "2012-07-03T19:12:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Accuracy Measures for the Comparison of Classifiers",
    "authors": [
      "Vincent Labatut",
      "Hocine Cherifi"
    ],
    "summary": "The selection of the best classification algorithm for a given dataset is a\nvery widespread problem. It is also a complex one, in the sense it requires to\nmake several important methodological choices. Among them, in this work we\nfocus on the measure used to assess the classification performance and rank the\nalgorithms. We present the most popular measures and discuss their properties.\nDespite the numerous measures proposed over the years, many of them turn out to\nbe equivalent in this specific case, to have interpretation problems, or to be\nunsuitable for our purpose. Consequently, classic overall success rate or\nmarginal rates should be preferred for this specific task.",
    "published": "2012-07-16T08:49:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Better Mixing via Deep Representations",
    "authors": [
      "Yoshua Bengio",
      "Grégoire Mesnil",
      "Yann Dauphin",
      "Salah Rifai"
    ],
    "summary": "It has previously been hypothesized, and supported with some experimental\nevidence, that deeper representations, when well trained, tend to do a better\njob at disentangling the underlying factors of variation. We study the\nfollowing related conjecture: better representations, in the sense of better\ndisentangling, can be exploited to produce faster-mixing Markov chains.\nConsequently, mixing would be more efficient at higher levels of\nrepresentation. To better understand why and how this is happening, we propose\na secondary conjecture: the higher-level samples fill more uniformly the space\nthey occupy and the high-density manifolds tend to unfold when represented at\nhigher levels. The paper discusses these hypotheses and tests them\nexperimentally through visualization and measurements of mixing and\ninterpolating between samples.",
    "published": "2012-07-18T16:07:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics\n  for Pediatric Cardiology",
    "authors": [
      "Thomas Perry",
      "Hongyuan Zha",
      "Patricio Frias",
      "Dadan Zeng",
      "Mark Braunstein"
    ],
    "summary": "Electronic health records contain rich textual data which possess critical\npredictive information for machine-learning based diagnostic aids. However many\ntraditional machine learning methods fail to simultaneously integrate both\nvector space data and text. We present a supervised method using Laplacian\neigenmaps to augment existing machine-learning methods with low-dimensional\nrepresentations of textual predictors which preserve the local similarities.\nThe proposed implementation performs alternating optimization using gradient\ndescent. For the evaluation we applied our method to over 2,000 patient records\nfrom a large single-center pediatric cardiology practice to predict if patients\nwere diagnosed with cardiac disease. Our method was compared with latent\nsemantic indexing, latent Dirichlet allocation, and local Fisher discriminant\nanalysis. The results were assessed using AUC, MCC, specificity, and\nsensitivity. Results indicate supervised Laplacian eigenmaps was the highest\nperforming method in our study, achieving 0.782 and 0.374 for AUC and MCC\nrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over the\nbaseline which excluded textual data and a 2.69% and 5.35% increase in AUC and\nMCC respectively over unsupervised Laplacian eigenmaps. This method allows many\nexisting machine learning predictors to effectively and efficiently utilize the\npotential of textual predictors.",
    "published": "2012-07-27T08:47:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "APRIL: Active Preference-learning based Reinforcement Learning",
    "authors": [
      "Riad Akrour",
      "Marc Schoenauer",
      "Michèle Sebag"
    ],
    "summary": "This paper focuses on reinforcement learning (RL) with limited prior\nknowledge. In the domain of swarm robotics for instance, the expert can hardly\ndesign a reward function or demonstrate the target behavior, forbidding the use\nof both standard RL and inverse reinforcement learning. Although with a limited\nexpertise, the human expert is still often able to emit preferences and rank\nthe agent demonstrations. Earlier work has presented an iterative\npreference-based RL framework: expert preferences are exploited to learn an\napproximate policy return, thus enabling the agent to achieve direct policy\nsearch. Iteratively, the agent selects a new candidate policy and demonstrates\nit; the expert ranks the new demonstration comparatively to the previous best\none; the expert's ranking feedback enables the agent to refine the approximate\npolicy return, and the process is iterated. In this paper, preference-based\nreinforcement learning is combined with active ranking in order to decrease the\nnumber of ranking queries to the expert needed to yield a satisfactory policy.\nExperiments on the mountain car and the cancer treatment testbeds witness that\na couple of dozen rankings enable to learn a competent policy.",
    "published": "2012-08-05T06:34:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Data Selection for Semi-Supervised Learning",
    "authors": [
      "Shafigh Parsazad",
      "Ehsan Saboori",
      "Amin Allahyar"
    ],
    "summary": "The real challenge in pattern recognition task and machine learning process\nis to train a discriminator using labeled data and use it to distinguish\nbetween future data as accurate as possible. However, most of the problems in\nthe real world have numerous data, which labeling them is a cumbersome or even\nan impossible matter. Semi-supervised learning is one approach to overcome\nthese types of problems. It uses only a small set of labeled with the company\nof huge remain and unlabeled data to train the discriminator. In\nsemi-supervised learning, it is very essential that which data is labeled and\ndepend on position of data it effectiveness changes. In this paper, we proposed\nan evolutionary approach called Artificial Immune System (AIS) to determine\nwhich data is better to be labeled to get the high quality data. The\nexperimental results represent the effectiveness of this algorithm in finding\nthese data points.",
    "published": "2012-08-07T01:31:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Guess Who Rated This Movie: Identifying Users Through Subspace\n  Clustering",
    "authors": [
      "Amy Zhang",
      "Nadia Fawaz",
      "Stratis Ioannidis",
      "Andrea Montanari"
    ],
    "summary": "It is often the case that, within an online recommender system, multiple\nusers share a common account. Can such shared accounts be identified solely on\nthe basis of the user- provided ratings? Once a shared account is identified,\ncan the different users sharing it be identified as well? Whenever such user\nidentification is feasible, it opens the way to possible improvements in\npersonalized recommendations, but also raises privacy concerns. We develop a\nmodel for composite accounts based on unions of linear subspaces, and use\nsubspace clustering for carrying out the identification task. We show that a\nsignificant fraction of such accounts is identifiable in a reliable manner, and\nillustrate potential uses for personalized recommendation.",
    "published": "2012-08-07T23:21:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Metric Learning across Heterogeneous Domains by Respectively Aligning\n  Both Priors and Posteriors",
    "authors": [
      "Qiang Qian",
      "Songcan Chen"
    ],
    "summary": "In this paper, we attempts to learn a single metric across two heterogeneous\ndomains where source domain is fully labeled and has many samples while target\ndomain has only a few labeled samples but abundant unlabeled samples. To the\nbest of our knowledge, this task is seldom touched. The proposed learning model\nhas a simple underlying motivation: all the samples in both the source and the\ntarget domains are mapped into a common space, where both their priors\nP(sample)s and their posteriors P(label|sample)s are forced to be respectively\naligned as much as possible. We show that the two mappings, from both the\nsource domain and the target domain to the common space, can be reparameterized\ninto a single positive semi-definite(PSD) matrix. Then we develop an efficient\nBregman Projection algorithm to optimize the PDS matrix over which a LogDet\nfunction is used to regularize. Furthermore, we also show that this model can\nbe easily kernelized and verify its effectiveness in crosslanguage retrieval\ntask and cross-domain object recognition task.",
    "published": "2012-08-09T07:14:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Margin Distribution Controlled Boosting",
    "authors": [
      "Guangxu Guo",
      "Songcan Chen"
    ],
    "summary": "Schapire's margin theory provides a theoretical explanation to the success of\nboosting-type methods and manifests that a good margin distribution (MD) of\ntraining samples is essential for generalization. However the statement that a\nMD is good is vague, consequently, many recently developed algorithms try to\ngenerate a MD in their goodness senses for boosting generalization. Unlike\ntheir indirect control over MD, in this paper, we propose an alternative\nboosting algorithm termed Margin distribution Controlled Boosting (MCBoost)\nwhich directly controls the MD by introducing and optimizing a key adjustable\nmargin parameter. MCBoost's optimization implementation adopts the column\ngeneration technique to ensure fast convergence and small number of weak\nclassifiers involved in the final MCBooster. We empirically demonstrate: 1)\nAdaBoost is actually also a MD controlled algorithm and its iteration number\nacts as a parameter controlling the distribution and 2) the generalization\nperformance of MCBoost evaluated on UCI benchmark datasets is validated better\nthan those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost.",
    "published": "2012-08-09T08:53:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Inverse Reinforcement Learning with Gaussian Process",
    "authors": [
      "Qifeng Qiao",
      "Peter A. Beling"
    ],
    "summary": "We present new algorithms for inverse reinforcement learning (IRL, or inverse\noptimal control) in convex optimization settings. We argue that finite-space\nIRL can be posed as a convex quadratic program under a Bayesian inference\nframework with the objective of maximum a posterior estimation. To deal with\nproblems in large or even infinite state space, we propose a Gaussian process\nmodel and use preference graphs to represent observations of decision\ntrajectories. Our method is distinguished from other approaches to IRL in that\nit makes no assumptions about the form of the reward function and yet it\nretains the promise of computationally manageable implementations for potential\nreal-world applications. In comparison with an establish algorithm on\nsmall-scale numerical problems, our method demonstrated better accuracy in\napprenticeship learning and a more robust dependence on the number of\nobservations.",
    "published": "2012-08-10T08:36:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Active Learning of Halfspaces: an Aggressive Approach",
    "authors": [
      "Alon Gonen",
      "Sivan Sabato",
      "Shai Shalev-Shwartz"
    ],
    "summary": "We study pool-based active learning of half-spaces. We revisit the aggressive\napproach for active learning in the realizable case, and show that it can be\nmade efficient and practical, while also having theoretical guarantees under\nreasonable assumptions. We further show, both theoretically and experimentally,\nthat it can be preferable to mellow approaches. Our efficient aggressive active\nlearner of half-spaces has formal approximation guarantees that hold when the\npool is separable with a margin. While our analysis is focused on the\nrealizable setting, we show that a simple heuristic allows using the same\nalgorithm successfully for pools with low error as well. We further compare the\naggressive approach to the mellow approach, and prove that there are cases in\nwhich the aggressive approach results in significantly better label complexity\ncompared to the mellow approach. We demonstrate experimentally that substantial\nimprovements in label complexity can be achieved using the aggressive approach,\nfor both realizable and low-error settings.",
    "published": "2012-08-17T09:49:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Auto-WEKA: Combined Selection and Hyperparameter Optimization of\n  Classification Algorithms",
    "authors": [
      "Chris Thornton",
      "Frank Hutter",
      "Holger H. Hoos",
      "Kevin Leyton-Brown"
    ],
    "summary": "Many different machine learning algorithms exist; taking into account each\nalgorithm's hyperparameters, there is a staggeringly large number of possible\nalternatives overall. We consider the problem of simultaneously selecting a\nlearning algorithm and setting its hyperparameters, going beyond previous work\nthat addresses these issues in isolation. We show that this problem can be\naddressed by a fully automated approach, leveraging recent innovations in\nBayesian optimization. Specifically, we consider a wide range of feature\nselection techniques (combining 3 search and 8 evaluator methods) and all\nclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10\nmeta-methods, 27 base classifiers, and hyperparameter settings for each\nclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup\n09, variants of the MNIST dataset and CIFAR-10, we show classification\nperformance often much better than using standard selection/hyperparameter\noptimization methods. We hope that our approach will help non-expert users to\nmore effectively identify machine learning algorithms and hyperparameter\nsettings appropriate to their applications, and hence to achieve improved\nperformance.",
    "published": "2012-08-18T02:14:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector\n  Fields",
    "authors": [
      "Nivan Ferreira",
      "James T. Klosowski",
      "Carlos Scheidegger",
      "Claudio Silva"
    ],
    "summary": "Scientists study trajectory data to understand trends in movement patterns,\nsuch as human mobility for traffic analysis and urban planning. There is a\npressing need for scalable and efficient techniques for analyzing this data and\ndiscovering the underlying patterns. In this paper, we introduce a novel\ntechnique which we call vector-field $k$-means.\n  The central idea of our approach is to use vector fields to induce a\nsimilarity notion between trajectories. Other clustering algorithms seek a\nrepresentative trajectory that best describes each cluster, much like $k$-means\nidentifies a representative \"center\" for each cluster. Vector-field $k$-means,\non the other hand, recognizes that in all but the simplest examples, no single\ntrajectory adequately describes a cluster. Our approach is based on the premise\nthat movement trends in trajectory data can be modeled as flows within multiple\nvector fields, and the vector field itself is what defines each of the\nclusters. We also show how vector-field $k$-means connects techniques for\nscalar field design on meshes and $k$-means clustering.\n  We present an algorithm that finds a locally optimal clustering of\ntrajectories into vector fields, and demonstrate how vector-field $k$-means can\nbe used to mine patterns from trajectory data. We present experimental evidence\nof its effectiveness and efficiency using several datasets, including\nhistorical hurricane data, GPS tracks of people and vehicles, and anonymous\ncall records from a large phone company. We compare our results to previous\ntrajectory clustering techniques, and find that our algorithm performs faster\nin practice than the current state-of-the-art in trajectory clustering, in some\nexamples by a large margin.",
    "published": "2012-08-28T21:51:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Link Prediction via Generalized Coupled Tensor Factorisation",
    "authors": [
      "Beyza Ermiş",
      "Evrim Acar",
      "A. Taylan Cemgil"
    ],
    "summary": "This study deals with the missing link prediction problem: the problem of\npredicting the existence of missing connections between entities of interest.\nWe address link prediction using coupled analysis of relational datasets\nrepresented as heterogeneous data, i.e., datasets in the form of matrices and\nhigher-order tensors. We propose to use an approach based on probabilistic\ninterpretation of tensor factorisation models, i.e., Generalised Coupled Tensor\nFactorisation, which can simultaneously fit a large class of tensor models to\nhigher-order tensors/matrices with com- mon latent factors using different loss\nfunctions. Numerical experiments demonstrate that joint analysis of data from\nmultiple sources via coupled factorisation improves the link prediction\nperformance and the selection of right loss function and tensor model is\ncrucial for accurately predicting missing links.",
    "published": "2012-08-30T16:48:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improving the K-means algorithm using improved downhill simplex search",
    "authors": [
      "Ehsan Saboori",
      "Shafigh Parsazad",
      "Anoosheh Sadeghi"
    ],
    "summary": "The k-means algorithm is one of the well-known and most popular clustering\nalgorithms. K-means seeks an optimal partition of the data by minimizing the\nsum of squared error with an iterative optimization procedure, which belongs to\nthe category of hill climbing algorithms. As we know hill climbing searches are\nfamous for converging to local optimums. Since k-means can converge to a local\noptimum, different initial points generally lead to different convergence\ncancroids, which makes it important to start with a reasonable initial\npartition in order to achieve high quality clustering solutions. However, in\ntheory, there exist no efficient and universal methods for determining such\ninitial partitions. In this paper we tried to find an optimum initial\npartitioning for k-means algorithm. To achieve this goal we proposed a new\nimproved version of downhill simplex search, and then we used it in order to\nfind an optimal result for clustering approach and then compare this algorithm\nwith Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved Genetic\nK-Means (IGKM) and k-means algorithms.",
    "published": "2012-09-05T03:02:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Structuring Relevant Feature Sets with Multiple Model Learning",
    "authors": [
      "Jun Wang",
      "Alexandros Kalousis"
    ],
    "summary": "Feature selection is one of the most prominent learning tasks, especially in\nhigh-dimensional datasets in which the goal is to understand the mechanisms\nthat underly the learning dataset. However most of them typically deliver just\na flat set of relevant features and provide no further information on what kind\nof structures, e.g. feature groupings, might underly the set of relevant\nfeatures. In this paper we propose a new learning paradigm in which our goal is\nto uncover the structures that underly the set of relevant features for a given\nlearning problem. We uncover two types of features sets, non-replaceable\nfeatures that contain important information about the target variable and\ncannot be replaced by other features, and functionally similar features sets\nthat can be used interchangeably in learned models, given the presence of the\nnon-replaceable features, with no change in the predictive performance. To do\nso we propose a new learning algorithm that learns a number of disjoint models\nusing a model disjointness regularization constraint together with a constraint\non the predictive agreement of the disjoint models. We explore the behavior of\nour approach on a number of high-dimensional datasets, and show that, as\nexpected by their construction, these satisfy a number of properties. Namely,\nmodel disjointness, a high predictive agreement, and a similar predictive\nperformance to models learned on the full set of relevant features. The ability\nto structure the set of relevant features in such a manner can become a\nvaluable tool in different applications of scientific knowledge discovery.",
    "published": "2012-09-05T10:08:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost\n  Matrices",
    "authors": [
      "Rui Wang",
      "Ke Tang"
    ],
    "summary": "Cost-sensitive learning relies on the availability of a known and fixed cost\nmatrix. However, in some scenarios, the cost matrix is uncertain during\ntraining, and re-train a classifier after the cost matrix is specified would\nnot be an option. For binary classification, this issue can be successfully\naddressed by methods maximizing the Area Under the ROC Curve (AUC) metric.\nSince the AUC can measure performance of base classifiers independent of cost\nduring training, and a larger AUC is more likely to lead to a smaller total\ncost in testing using the threshold moving method. As an extension of AUC to\nmulti-class problems, MAUC has attracted lots of attentions and been widely\nused. Although MAUC also measures performance of base classifiers independent\nof cost, it is unclear whether a larger MAUC of classifiers is more likely to\nlead to a smaller total cost. In fact, it is also unclear what kinds of\npost-processing methods should be used in multi-class problems to convert base\nclassifiers into discrete classifiers such that the total cost is as small as\npossible. In the paper, we empirically explore the relationship between MAUC\nand the total cost of classifiers by applying two categories of post-processing\nmethods. Our results suggest that a larger MAUC is also beneficial.\nInterestingly, simple calibration methods that convert the output matrix into\nposterior probabilities perform better than existing sophisticated post\nre-optimization methods.",
    "published": "2012-09-09T14:11:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Performance Evaluation of Predictive Classifiers For Knowledge Discovery\n  From Engineering Materials Data Sets",
    "authors": [
      "Hemanth K. S Doreswamy"
    ],
    "summary": "In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are\nsuccessively applied on materials informatics to classify the engineering\nmaterials into different classes for the selection of materials that suit the\ninput design specifications. Here, the classifiers are analyzed individually\nand their performance evaluation is analyzed with confusion matrix predictive\nparameters and standard measures, the classification results are analyzed on\ndifferent class of materials. Comparison of classifiers has found that naive\nBayesian classifier is more accurate and better than the C4.5 DTC. The\nknowledge discovered by the naive bayesian classifier can be employed for\ndecision making in materials selection in manufacturing industries.",
    "published": "2012-09-12T05:28:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Conditional validity of inductive conformal predictors",
    "authors": [
      "Vladimir Vovk"
    ],
    "summary": "Conformal predictors are set predictors that are automatically valid in the\nsense of having coverage probability equal to or exceeding a given confidence\nlevel. Inductive conformal predictors are a computationally efficient version\nof conformal predictors satisfying the same property of validity. However,\ninductive conformal predictors have been only known to control unconditional\ncoverage probability. This paper explores various versions of conditional\nvalidity and various ways to achieve them using inductive conformal predictors\nand their modifications.",
    "published": "2012-09-12T17:39:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improving Energy Efficiency in Femtocell Networks: A Hierarchical\n  Reinforcement Learning Framework",
    "authors": [
      "Xianfu Chen",
      "Honggang Zhang",
      "Tao Chen",
      "Mika Lasanen"
    ],
    "summary": "This paper investigates energy efficiency for two-tier femtocell networks\nthrough combining game theory and stochastic learning. With the Stackelberg\ngame formulation, a hierarchical reinforcement learning framework is applied to\nstudy the joint average utility maximization of macrocells and femtocells\nsubject to the minimum signal-to-interference-plus-noise-ratio requirements.\nThe macrocells behave as the leaders and the femtocells are followers during\nthe learning procedure. At each time step, the leaders commit to dynamic\nstrategies based on the best responses of the followers, while the followers\ncompete against each other with no further information but the leaders'\nstrategy information. In this paper, we propose two learning algorithms to\nschedule each cell's stochastic power levels, leading by the macrocells.\nNumerical experiments are presented to validate the proposed studies and show\nthat the two learning algorithms substantially improve the energy efficiency of\nthe femtocell networks.",
    "published": "2012-09-13T06:47:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Parametric Local Metric Learning for Nearest Neighbor Classification",
    "authors": [
      "Jun Wang",
      "Adam Woznica",
      "Alexandros Kalousis"
    ],
    "summary": "We study the problem of learning local metrics for nearest neighbor\nclassification. Most previous works on local metric learning learn a number of\nlocal unrelated metrics. While this \"independence\" approach delivers an\nincreased flexibility its downside is the considerable risk of overfitting. We\npresent a new parametric local metric learning method in which we learn a\nsmooth metric matrix function over the data manifold. Using an approximation\nerror bound of the metric matrix function we learn local metrics as linear\ncombinations of basis metrics defined on anchor points over different regions\nof the instance space. We constrain the metric matrix function by imposing on\nthe linear combinations manifold regularization which makes the learned metric\nmatrix function vary smoothly along the geodesics of the data manifold. Our\nmetric learning method has excellent performance both in terms of predictive\npower and scalability. We experimented with several large-scale classification\nproblems, tens of thousands of instances, and compared it with several state of\nthe art metric learning methods, both global and local, as well as to SVM with\nautomatic kernel selection, all of which it outperforms in a significant\nmanner.",
    "published": "2012-09-13T22:47:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Randomized Model Generation for Shapelet-Based Time Series\n  Classification",
    "authors": [
      "Daniel Gordon",
      "Danny Hendler",
      "Lior Rokach"
    ],
    "summary": "Time series classification is a field which has drawn much attention over the\npast decade. A new approach for classification of time series uses\nclassification trees based on shapelets. A shapelet is a subsequence extracted\nfrom one of the time series in the dataset. A disadvantage of this approach is\nthe time required for building the shapelet-based classification tree. The\nsearch for the best shapelet requires examining all subsequences of all lengths\nfrom all time series in the training set.\n  A key goal of this work was to find an evaluation order of the shapelets\nspace which enables fast convergence to an accurate model. The comparative\nanalysis we conducted clearly indicates that a random evaluation order yields\nthe best results. Our empirical analysis of the distribution of high-quality\nshapelets within the shapelets space provides insights into why randomized\nshapelets sampling is superior to alternative evaluation orders.\n  We present an algorithm for randomized model generation for shapelet-based\nclassification that converges extremely quickly to a model with surprisingly\nhigh accuracy after evaluating only an exceedingly small fraction of the\nshapelets space.",
    "published": "2012-09-23T07:50:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Ultrahigh Dimensional Feature Selection for Big Data",
    "authors": [
      "Mingkui Tan",
      "Ivor W. Tsang",
      "Li Wang"
    ],
    "summary": "In this paper, we present a new adaptive feature scaling scheme for\nultrahigh-dimensional feature selection on Big Data. To solve this problem\neffectively, we first reformulate it as a convex semi-infinite programming\n(SIP) problem and then propose an efficient \\emph{feature generating paradigm}.\nIn contrast with traditional gradient-based approaches that conduct\noptimization on all input features, the proposed method iteratively activates a\ngroup of features and solves a sequence of multiple kernel learning (MKL)\nsubproblems of much reduced scale. To further speed up the training, we propose\nto solve the MKL subproblems in their primal forms through a modified\naccelerated proximal gradient approach. Due to such an optimization scheme,\nsome efficient cache techniques are also developed. The feature generating\nparadigm can guarantee that the solution converges globally under mild\nconditions and achieve lower feature selection bias. Moreover, the proposed\nmethod can tackle two challenging tasks in feature selection: 1) group-based\nfeature selection with complex structures and 2) nonlinear feature selection\nwith explicit feature mappings. Comprehensive experiments on a wide range of\nsynthetic and real-world datasets containing tens of million data points with\n$O(10^{14})$ features demonstrate the competitive performance of the proposed\nmethod over state-of-the-art feature selection methods in terms of\ngeneralization performance and training efficiency.",
    "published": "2012-09-24T13:23:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "BPRS: Belief Propagation Based Iterative Recommender System",
    "authors": [
      "Erman Ayday",
      "Arash Einolghozati",
      "Faramarz Fekri"
    ],
    "summary": "In this paper we introduce the first application of the Belief Propagation\n(BP) algorithm in the design of recommender systems. We formulate the\nrecommendation problem as an inference problem and aim to compute the marginal\nprobability distributions of the variables which represent the ratings to be\npredicted. However, computing these marginal probability functions is\ncomputationally prohibitive for large-scale systems. Therefore, we utilize the\nBP algorithm to efficiently compute these functions. Recommendations for each\nactive user are then iteratively computed by probabilistic message passing. As\nopposed to the previous recommender algorithms, BPRS does not require solving\nthe recommendation problem for all the users if it wishes to update the\nrecommendations for only a single active. Further, BPRS computes the\nrecommendations for each user with linear complexity and without requiring a\ntraining period. Via computer simulations (using the 100K MovieLens dataset),\nwe verify that BPRS iteratively reduces the error in the predicted ratings of\nthe users until it converges. Finally, we confirm that BPRS is comparable to\nthe state of art methods such as Correlation-based neighborhood model (CorNgbr)\nand Singular Value Decomposition (SVD) in terms of rating and precision\naccuracy. Therefore, we believe that the BP-based recommendation algorithm is a\nnew promising approach which offers a significant advantage on scalability\nwhile providing competitive accuracy for the recommender systems.",
    "published": "2012-09-24T16:59:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "More Is Better: Large Scale Partially-supervised Sentiment\n  Classification - Appendix",
    "authors": [
      "Yoav Haimovitch",
      "Koby Crammer",
      "Shie Mannor"
    ],
    "summary": "We describe a bootstrapping algorithm to learn from partially labeled data,\nand the results of an empirical study for using it to improve performance of\nsentiment classification using up to 15 million unlabeled Amazon product\nreviews. Our experiments cover semi-supervised learning, domain adaptation and\nweakly supervised learning. In some cases our methods were able to reduce test\nerror by more than half using such large amount of data.\n  NOTICE: This is only the supplementary material.",
    "published": "2012-09-27T18:57:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Deterministic Analysis of an Online Convex Mixture of Expert\n  Algorithms",
    "authors": [
      "Mehmet A. Donmez",
      "Sait Tunc",
      "Suleyman S. Kozat"
    ],
    "summary": "We analyze an online learning algorithm that adaptively combines outputs of\ntwo constituent algorithms (or the experts) running in parallel to model an\nunknown desired signal. This online learning algorithm is shown to achieve (and\nin some cases outperform) the mean-square error (MSE) performance of the best\nconstituent algorithm in the mixture in the steady-state. However, the MSE\nanalysis of this algorithm in the literature uses approximations and relies on\nstatistical models on the underlying signals and systems. Hence, such an\nanalysis may not be useful or valid for signals generated by various real life\nsystems that show high degrees of nonstationarity, limit cycles and, in many\ncases, that are even chaotic. In this paper, we produce results in an\nindividual sequence manner. In particular, we relate the time-accumulated\nsquared estimation error of this online algorithm at any time over any interval\nto the time accumulated squared estimation error of the optimal convex mixture\nof the constituent algorithms directly tuned to the underlying signal in a\ndeterministic sense without any statistical assumptions. In this sense, our\nanalysis provides the transient, steady-state and tracking behavior of this\nalgorithm in a strong sense without any approximations in the derivations or\nstatistical assumptions on the underlying signals such that our results are\nguaranteed to hold. We illustrate the introduced results through examples.",
    "published": "2012-09-28T01:46:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extension of TSVM to Multi-Class and Hierarchical Text Classification\n  Problems With General Losses",
    "authors": [
      "Sathiya Keerthi Selvaraj",
      "Sundararajan Sellamanickam",
      "Shirish Shevade"
    ],
    "summary": "Transductive SVM (TSVM) is a well known semi-supervised large margin learning\nmethod for binary text classification. In this paper we extend this method to\nmulti-class and hierarchical classification problems. We point out that the\ndetermination of labels of unlabeled examples with fixed classifier weights is\na linear programming problem. We devise an efficient technique for solving it.\nThe method is applicable to general loss functions. We demonstrate the value of\nthe new method using large margin loss on a number of multi-class and\nhierarchical classification datasets. For maxent loss we show empirically that\nour method is better than expectation regularization/constraint and posterior\nregularization methods, and competitive with the version of entropy\nregularization method which uses label constraints.",
    "published": "2012-11-01T15:52:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "K-Plane Regression",
    "authors": [
      "Naresh Manwani",
      "P. S. Sastry"
    ],
    "summary": "In this paper, we present a novel algorithm for piecewise linear regression\nwhich can learn continuous as well as discontinuous piecewise linear functions.\nThe main idea is to repeatedly partition the data and learn a liner model in in\neach partition. While a simple algorithm incorporating this idea does not work\nwell, an interesting modification results in a good algorithm. The proposed\nalgorithm is similar in spirit to $k$-means clustering algorithm. We show that\nour algorithm can also be viewed as an EM algorithm for maximum likelihood\nestimation of parameters under a reasonable probability model. We empirically\ndemonstrate the effectiveness of our approach by comparing its performance with\nthe state of art regression learning algorithms on some real world datasets.",
    "published": "2012-11-07T10:57:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Algorithm for Missing Values Imputation in Categorical Data with Use of\n  Association Rules",
    "authors": [
      "Jiří Kaiser"
    ],
    "summary": "This paper presents algorithm for missing values imputation in categorical\ndata. The algorithm is based on using association rules and is presented in\nthree variants. Experimental shows better accuracy of missing values imputation\nusing the algorithm then using most common attribute value.",
    "published": "2012-11-08T09:22:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization",
    "authors": [
      "Matthew Streeter",
      "H. Brendan McMahan"
    ],
    "summary": "Some of the most compelling applications of online convex optimization,\nincluding online prediction and classification, are unconstrained: the natural\nfeasible set is R^n. Existing algorithms fail to achieve sub-linear regret in\nthis setting unless constraints on the comparator point x^* are known in\nadvance. We present algorithms that, without such prior knowledge, offer\nnear-optimal regret bounds with respect to any choice of x^*. In particular,\nregret with respect to x^* = 0 is constant. We then prove lower bounds showing\nthat our guarantees are near-optimal in this setting.",
    "published": "2012-11-09T22:13:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Recovering the Optimal Solution by Dual Random Projection",
    "authors": [
      "Lijun Zhang",
      "Mehrdad Mahdavi",
      "Rong Jin",
      "Tianbao Yang",
      "Shenghuo Zhu"
    ],
    "summary": "Random projection has been widely used in data classification. It maps\nhigh-dimensional data into a low-dimensional subspace in order to reduce the\ncomputational cost in solving the related optimization problem. While previous\nstudies are focused on analyzing the classification performance of using random\nprojection, in this work, we consider the recovery problem, i.e., how to\naccurately recover the optimal solution to the original optimization problem in\nthe high-dimensional space based on the solution learned from the subspace\nspanned by random projections. We present a simple algorithm, termed Dual\nRandom Projection, that uses the dual solution of the low-dimensional\noptimization problem to recover the optimal solution to the original problem.\nOur theoretical analysis shows that with a high probability, the proposed\nalgorithm is able to accurately recover the optimal solution to the original\nproblem, provided that the data matrix is of low rank or can be well\napproximated by a low rank matrix.",
    "published": "2012-11-13T16:39:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the difficulty of training Recurrent Neural Networks",
    "authors": [
      "Razvan Pascanu",
      "Tomas Mikolov",
      "Yoshua Bengio"
    ],
    "summary": "There are two widely known issues with properly training Recurrent Neural\nNetworks, the vanishing and the exploding gradient problems detailed in Bengio\net al. (1994). In this paper we attempt to improve the understanding of the\nunderlying issues by exploring these problems from an analytical, a geometric\nand a dynamical systems perspective. Our analysis is used to justify a simple\nyet effective solution. We propose a gradient norm clipping strategy to deal\nwith exploding gradients and a soft constraint for the vanishing gradients\nproblem. We validate empirically our hypothesis and proposed solutions in the\nexperimental section.",
    "published": "2012-11-21T15:40:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Approach of Improving Students Academic Performance by using k means\n  clustering algorithm and Decision tree",
    "authors": [
      "Md. Hedayetul Islam Shovon",
      "Mahfuza Haque"
    ],
    "summary": "Improving students academic performance is not an easy task for the academic\ncommunity of higher learning. The academic performance of engineering and\nscience students during their first year at university is a turning point in\ntheir educational path and usually encroaches on their General Point\nAverage,GPA in a decisive manner. The students evaluation factors like class\nquizzes mid and final exam assignment lab work are studied. It is recommended\nthat all these correlated information should be conveyed to the class teacher\nbefore the conduction of final exam. This study will help the teachers to\nreduce the drop out ratio to a significant level and improve the performance of\nstudents. In this paper, we present a hybrid procedure based on Decision Tree\nof Data mining method and Data Clustering that enables academicians to predict\nstudents GPA and based on that instructor can take necessary step to improve\nstudent academic performance.",
    "published": "2012-11-09T09:54:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Target Regression via Input Space Expansion: Treating Targets as\n  Inputs",
    "authors": [
      "Eleftherios Spyromitros-Xioufis",
      "Grigorios Tsoumakas",
      "William Groves",
      "Ioannis Vlahavas"
    ],
    "summary": "In many practical applications of supervised learning the task involves the\nprediction of multiple target variables from a common set of input variables.\nWhen the prediction targets are binary the task is called multi-label\nclassification, while when the targets are continuous the task is called\nmulti-target regression. In both tasks, target variables often exhibit\nstatistical dependencies and exploiting them in order to improve predictive\naccuracy is a core challenge. A family of multi-label classification methods\naddress this challenge by building a separate model for each target on an\nexpanded input space where other targets are treated as additional input\nvariables. Despite the success of these methods in the multi-label\nclassification domain, their applicability and effectiveness in multi-target\nregression has not been studied until now. In this paper, we introduce two new\nmethods for multi-target regression, called Stacked Single-Target and Ensemble\nof Regressor Chains, by adapting two popular multi-label classification methods\nof this family. Furthermore, we highlight an inherent problem of these methods\n- a discrepancy of the values of the additional input variables between\ntraining and prediction - and develop extensions that use out-of-sample\nestimates of the target variables during training in order to tackle this\nproblem. The results of an extensive experimental evaluation carried out on a\nlarge and diverse collection of datasets show that, when the discrepancy is\nappropriately mitigated, the proposed methods attain consistent improvements\nover the independent regressions baseline. Moreover, two versions of Ensemble\nof Regression Chains perform significantly better than four state-of-the-art\nmethods including regularization-based multi-task learning methods and a\nmulti-objective random forest approach.",
    "published": "2012-11-28T11:42:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Advances in Optimizing Recurrent Networks",
    "authors": [
      "Yoshua Bengio",
      "Nicolas Boulanger-Lewandowski",
      "Razvan Pascanu"
    ],
    "summary": "After a more than decade-long period of relatively little research activity\nin the area of recurrent neural networks, several new developments will be\nreviewed here that have allowed substantial progress both in understanding and\nin technical solutions towards more efficient training of recurrent networks.\nThese advances have been motivated by and related to the optimization issues\nsurrounding deep learning. Although recurrent networks are extremely powerful\nin what they can in principle represent in terms of modelling sequences,their\ntraining is plagued by two aspects of the same issue regarding the learning of\nlong-term dependencies. Experiments reported here evaluate the use of clipping\ngradients, spanning longer time ranges with leaky integration, advanced\nmomentum techniques, using more powerful output probability models, and\nencouraging sparser gradients to help symmetry breaking and credit assignment.\nThe experiments are performed on text and music data and show off the combined\neffects of these techniques in generally improving both training and test\nerror.",
    "published": "2012-12-04T23:25:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "High-dimensional sequence transduction",
    "authors": [
      "Nicolas Boulanger-Lewandowski",
      "Yoshua Bengio",
      "Pascal Vincent"
    ],
    "summary": "We investigate the problem of transforming an input sequence into a\nhigh-dimensional output sequence in order to transcribe polyphonic audio music\ninto symbolic notation. We introduce a probabilistic model based on a recurrent\nneural network that is able to learn realistic output distributions given the\ninput and we devise an efficient algorithm to search for the global mode of\nthat distribution. The resulting method produces musically plausible\ntranscriptions even under high levels of noise and drastically outperforms\nprevious state-of-the-art approaches on five datasets of synthesized sounds and\nreal recordings, approximately halving the test error rate.",
    "published": "2012-12-09T23:28:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cost-Sensitive Feature Selection of Data with Errors",
    "authors": [
      "Hong Zhao",
      "Fan Min",
      "William Zhu"
    ],
    "summary": "In data mining applications, feature selection is an essential process since\nit reduces a model's complexity. The cost of obtaining the feature values must\nbe taken into consideration in many domains. In this paper, we study the\ncost-sensitive feature selection problem on numerical data with measurement\nerrors, test costs and misclassification costs. The major contributions of this\npaper are four-fold. First, a new data model is built to address test costs and\nmisclassification costs as well as error boundaries. Second, a covering-based\nrough set with measurement errors is constructed. Given a confidence interval,\nthe neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in a\nthree-dimension space, etc. Third, a new cost-sensitive feature selection\nproblem is defined on this covering-based rough set. Fourth, both backtracking\nand heuristic algorithms are proposed to deal with this new problem. The\nalgorithms are tested on six UCI (University of California - Irvine) data sets.\nExperimental results show that (1) the pruning techniques of the backtracking\nalgorithm help reducing the number of operations significantly, and (2) the\nheuristic algorithm usually obtains optimal results. This study is a step\ntoward realistic applications of cost-sensitive learning.",
    "published": "2012-12-13T14:31:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning efficient sparse and low rank models",
    "authors": [
      "Pablo Sprechmann",
      "Alex M. Bronstein",
      "Guillermo Sapiro"
    ],
    "summary": "Parsimony, including sparsity and low rank, has been shown to successfully\nmodel data in numerous machine learning and signal processing tasks.\nTraditionally, such modeling approaches rely on an iterative algorithm that\nminimizes an objective function with parsimony-promoting terms. The inherently\nsequential structure and data-dependent complexity and latency of iterative\noptimization constitute a major limitation in many applications requiring\nreal-time performance or involving large-scale data. Another limitation\nencountered by these modeling techniques is the difficulty of their inclusion\nin discriminative learning scenarios. In this work, we propose to move the\nemphasis from the model to the pursuit algorithm, and develop a process-centric\nview of parsimonious modeling, in which a learned deterministic\nfixed-complexity pursuit process is used in lieu of iterative optimization. We\nshow a principled way to construct learnable pursuit process architectures for\nstructured sparse and robust low rank models, derived from the iteration of\nproximal descent algorithms. These architectures learn to approximate the exact\nparsimonious representation at a fraction of the complexity of the standard\noptimization methods. We also show that appropriate training regimes allow to\nnaturally extend parsimonious models to discriminative settings.\nState-of-the-art results are demonstrated on several challenging problems in\nimage and audio processing with several orders of magnitude speedup compared to\nthe exact optimization algorithms.",
    "published": "2012-12-14T22:50:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Analysis of Large-scale Traffic Dynamics using Non-negative Tensor\n  Factorization",
    "authors": [
      "Yufei Han",
      "Fabien Moutarde"
    ],
    "summary": "In this paper, we present our work on clustering and prediction of temporal\ndynamics of global congestion configurations in large-scale road networks.\nInstead of looking into temporal traffic state variation of individual links,\nor of small areas, we focus on spatial congestion configurations of the whole\nnetwork. In our work, we aim at describing the typical temporal dynamic\npatterns of this network-level traffic state and achieving long-term prediction\nof the large-scale traffic dynamics, in a unified data-mining framework. To\nthis end, we formulate this joint task using Non-negative Tensor Factorization\n(NTF), which has been shown to be a useful decomposition tools for multivariate\ndata sequences. Clustering and prediction are performed based on the compact\ntensor factorization results. Experiments on large-scale simulated data\nillustrate the interest of our method with promising results for long-term\nforecast of traffic evolution.",
    "published": "2012-12-18T20:17:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hybrid Fuzzy-ART based K-Means Clustering Methodology to Cellular\n  Manufacturing Using Operational Time",
    "authors": [
      "Sourav Sengupta",
      "Tamal Ghosh",
      "Pranab K Dan",
      "Manojit Chattopadhyay"
    ],
    "summary": "This paper presents a new hybrid Fuzzy-ART based K-Means Clustering technique\nto solve the part machine grouping problem in cellular manufacturing systems\nconsidering operational time. The performance of the proposed technique is\ntested with problems from open literature and the results are compared to the\nexisting clustering models such as simple K-means algorithm and modified ART1\nalgorithm using an efficient modified performance measure known as modified\ngrouping efficiency (MGE) as found in the literature. The results support the\nbetter performance of the proposed algorithm. The Novelty of this study lies in\nthe simple and efficient methodology to produce quick solutions for shop floor\nmanagers with least computational efforts and time.",
    "published": "2012-12-20T15:53:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "ADADELTA: An Adaptive Learning Rate Method",
    "authors": [
      "Matthew D. Zeiler"
    ],
    "summary": "We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment.",
    "published": "2012-12-22T15:46:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Tangent Bundle Manifold Learning via Grassmann&Stiefel Eigenmaps",
    "authors": [
      "Alexander V. Bernstein",
      "Alexander P. Kuleshov"
    ],
    "summary": "One of the ultimate goals of Manifold Learning (ML) is to reconstruct an\nunknown nonlinear low-dimensional manifold embedded in a high-dimensional\nobservation space by a given set of data points from the manifold. We derive a\nlocal lower bound for the maximum reconstruction error in a small neighborhood\nof an arbitrary point. The lower bound is defined in terms of the distance\nbetween tangent spaces to the original manifold and the estimated manifold at\nthe considered point and reconstructed point, respectively. We propose an\namplification of the ML, called Tangent Bundle ML, in which the proximity not\nonly between the original manifold and its estimator but also between their\ntangent spaces is required. We present a new algorithm that solves this problem\nand gives a new solution for the ML also.",
    "published": "2012-12-25T12:12:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Novel Design Specification Distance(DSD) Based K-Mean Clustering\n  Performace Evluation on Engineering Materials Database",
    "authors": [
      "Doreswamy",
      "K. S. Hemanth"
    ],
    "summary": "Organizing data into semantically more meaningful is one of the fundamental\nmodes of understanding and learning. Cluster analysis is a formal study of\nmethods for understanding and algorithm for learning. K-mean clustering\nalgorithm is one of the most fundamental and simple clustering algorithms. When\nthere is no prior knowledge about the distribution of data sets, K-mean is the\nfirst choice for clustering with an initial number of clusters. In this paper a\nnovel distance metric called Design Specification (DS) distance measure\nfunction is integrated with K-mean clustering algorithm to improve cluster\naccuracy. The K-means algorithm with proposed distance measure maximizes the\ncluster accuracy to 99.98% at P = 1.525, which is determined through the\niterative procedure. The performance of Design Specification (DS) distance\nmeasure function with K - mean algorithm is compared with the performances of\nother standard distance functions such as Euclidian, squared Euclidean, City\nBlock, and Chebshew similarity measures deployed with K-mean algorithm.The\nproposed method is evaluated on the engineering materials database. The\nexperiments on cluster analysis and the outlier profiling show that these is an\nexcellent improvement in the performance of the proposed method.",
    "published": "2013-01-02T07:13:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Risk-Aversion in Multi-armed Bandits",
    "authors": [
      "Amir Sani",
      "Alessandro Lazaric",
      "Rémi Munos"
    ],
    "summary": "Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and\nultimately maximize the expected reward. Nonetheless, in many practical\nproblems, maximizing the expected reward is not the most desirable objective.\nIn this paper, we introduce a novel setting based on the principle of\nrisk-aversion where the objective is to compete against the arm with the best\nrisk-return trade-off. This setting proves to be intrinsically more difficult\nthan the standard multi-arm bandit setting due in part to an exploration risk\nwhich introduces a regret associated to the variability of an algorithm. Using\nvariance as a measure of risk, we introduce two new algorithms, investigate\ntheir theoretical guarantees, and report preliminary empirical results.",
    "published": "2013-01-09T18:02:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Error Correction in Learning using SVMs",
    "authors": [
      "Srivatsan Laxman",
      "Sushil Mittal",
      "Ramarathnam Venkatesan"
    ],
    "summary": "This paper is concerned with learning binary classifiers under adversarial\nlabel-noise. We introduce the problem of error-correction in learning where the\ngoal is to recover the original clean data from a label-manipulated version of\nit, given (i) no constraints on the adversary other than an upper-bound on the\nnumber of errors, and (ii) some regularity properties for the original data. We\npresent a simple and practical error-correction algorithm called SubSVMs that\nlearns individual SVMs on several small-size (log-size), class-balanced, random\nsubsets of the data and then reclassifies the training points using a majority\nvote. Our analysis reveals the need for the two main ingredients of SubSVMs,\nnamely class-balanced sampling and subsampled bagging. Experimental results on\nsynthetic as well as benchmark UCI data demonstrate the effectiveness of our\napproach. In addition to noise-tolerance, log-size subsampled bagging also\nyields significant run-time benefits over standard SVMs.",
    "published": "2013-01-10T00:47:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning to Optimize Via Posterior Sampling",
    "authors": [
      "Daniel Russo",
      "Benjamin Van Roy"
    ],
    "summary": "This paper considers the use of a simple posterior sampling algorithm to\nbalance between exploration and exploitation when learning to optimize actions\nsuch as in multi-armed bandit problems. The algorithm, also known as Thompson\nSampling, offers significant advantages over the popular upper confidence bound\n(UCB) approach, and can be applied to problems with finite or infinite action\nspaces and complicated relationships among action rewards. We make two\ntheoretical contributions. The first establishes a connection between posterior\nsampling and UCB algorithms. This result lets us convert regret bounds\ndeveloped for UCB algorithms into Bayesian regret bounds for posterior\nsampling. Our second theoretical contribution is a Bayesian regret bound for\nposterior sampling that applies broadly and can be specialized to many model\nclasses. This bound depends on a new notion we refer to as the eluder\ndimension, which measures the degree of dependence among action rewards.\nCompared to UCB algorithm Bayesian regret bounds for specific model classes,\nour general bound matches the best available for linear models and is stronger\nthan the best available for generalized linear models. Further, our analysis\nprovides insight into performance advantages of posterior sampling, which are\nhighlighted through simulation results that demonstrate performance surpassing\nrecently proposed UCB algorithms.",
    "published": "2013-01-11T21:24:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Learning of Domain-invariant Image Representations",
    "authors": [
      "Judy Hoffman",
      "Erik Rodner",
      "Jeff Donahue",
      "Trevor Darrell",
      "Kate Saenko"
    ],
    "summary": "We present an algorithm that learns representations which explicitly\ncompensate for domain mismatch and which can be efficiently realized as linear\nclassifiers. Specifically, we form a linear transformation that maps features\nfrom the target (test) domain to the source (training) domain as part of\ntraining the classifier. We optimize both the transformation and classifier\nparameters jointly, and introduce an efficient cost function based on\nmisclassification loss. Our method combines several features previously\nunavailable in a single algorithm: multi-class adaptation through\nrepresentation learning, ability to map across heterogeneous feature spaces,\nand scalability to large datasets. We present experiments on several image\ndatasets that demonstrate improved accuracy and computational advantages\ncompared to previous approaches.",
    "published": "2013-01-15T04:39:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Feature grouping from spatially constrained multiplicative interaction",
    "authors": [
      "Felix Bauer",
      "Roland Memisevic"
    ],
    "summary": "We present a feature learning model that learns to encode relationships\nbetween images. The model is defined as a Gated Boltzmann Machine, which is\nconstrained such that hidden units that are nearby in space can gate each\nother's connections. We show how frequency/orientation \"columns\" as well as\ntopographic filter maps follow naturally from training the model on image\npairs. The model also helps explain why square-pooling models yield feature\ngroups with similar grouping properties. Experimental results on synthetic\nimage transformations show that spatially constrained gating is an effective\nway to reduce the number of parameters and thereby to regularize a\ntransformation-learning model.",
    "published": "2013-01-15T16:06:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Semantic Matching Energy Function for Learning with Multi-relational\n  Data",
    "authors": [
      "Xavier Glorot",
      "Antoine Bordes",
      "Jason Weston",
      "Yoshua Bengio"
    ],
    "summary": "Large-scale relational learning becomes crucial for handling the huge amounts\nof structured data generated daily in many application domains ranging from\ncomputational biology or information retrieval, to natural language processing.\nIn this paper, we present a new neural network architecture designed to embed\nmulti-relational graphs into a flexible continuous vector space in which the\noriginal data is kept and enhanced. The network is trained to encode the\nsemantics of these graphs in order to assign high probabilities to plausible\ncomponents. We empirically show that it reaches competitive performance in link\nprediction on standard datasets from the literature.",
    "published": "2013-01-15T20:52:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "How good is the Electricity benchmark for evaluating concept drift\n  adaptation",
    "authors": [
      "Indre Zliobaite"
    ],
    "summary": "In this correspondence, we will point out a problem with testing adaptive\nclassifiers on autocorrelated data. In such a case random change alarms may\nboost the accuracy figures. Hence, we cannot be sure if the adaptation is\nworking well.",
    "published": "2013-01-15T22:51:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Features with Structure-Adapting Multi-view Exponential Family\n  Harmoniums",
    "authors": [
      "Yoonseop Kang",
      "Seungjin Choi"
    ],
    "summary": "We proposea graphical model for multi-view feature extraction that\nautomatically adapts its structure to achieve better representation of data\ndistribution. The proposed model, structure-adapting multi-view harmonium\n(SA-MVH) has switch parameters that control the connection between hidden nodes\nand input views, and learn the switch parameter while training. Numerical\nexperiments on synthetic and a real-world dataset demonstrate the useful\nbehavior of the SA-MVH, compared to existing multi-view feature extraction\nmethods.",
    "published": "2013-01-16T01:07:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Saturating Auto-Encoders",
    "authors": [
      "Rostislav Goroshin",
      "Yann LeCun"
    ],
    "summary": "We introduce a simple new regularizer for auto-encoders whose hidden-unit\nactivation functions contain at least one zero-gradient (saturated) region.\nThis regularizer explicitly encourages activations in the saturated region(s)\nof the corresponding activation function. We call these Saturating\nAuto-Encoders (SATAE). We show that the saturation regularizer explicitly\nlimits the SATAE's ability to reconstruct inputs which are not near the data\nmanifold. Furthermore, we show that a wide variety of features can be learned\nwhen different activation functions are used. Finally, connections are\nestablished with the Contractive and Sparse Auto-Encoders.",
    "published": "2013-01-16T04:07:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Behavior Pattern Recognition using A New Representation Model",
    "authors": [
      "Qifeng Qiao",
      "Peter A. Beling"
    ],
    "summary": "We study the use of inverse reinforcement learning (IRL) as a tool for the\nrecognition of agents' behavior on the basis of observation of their sequential\ndecision behavior interacting with the environment. We model the problem faced\nby the agents as a Markov decision process (MDP) and model the observed\nbehavior of the agents in terms of forward planning for the MDP. We use IRL to\nlearn reward functions and then use these reward functions as the basis for\nclustering or classification models. Experimental studies with GridWorld, a\nnavigation problem, and the secretary problem, an optimal stopping problem,\nsuggest reward vectors found from IRL can be a good basis for behavior pattern\nrecognition problems. Empirical comparisons of our method with several existing\nIRL algorithms and with direct methods that use feature statistics observed in\nstate-action space suggest it may be superior for recognition problems.",
    "published": "2013-01-16T09:01:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Switched linear encoding with rectified linear autoencoders",
    "authors": [
      "Leif Johnson",
      "Craig Corcoran"
    ],
    "summary": "Several recent results in machine learning have established formal\nconnections between autoencoders---artificial neural network models that\nattempt to reproduce their inputs---and other coding models like sparse coding\nand K-means. This paper explores in depth an autoencoder model that is\nconstructed using rectified linear activations on its hidden units. Our\nanalysis builds on recent results to further unify the world of sparse linear\ncoding models. We provide an intuitive interpretation of the behavior of these\ncoding models and demonstrate this intuition using small, artificial datasets\nwith known distributions.",
    "published": "2013-01-16T17:04:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Output Kernels for Multi-Task Problems",
    "authors": [
      "Francesco Dinuzzo"
    ],
    "summary": "Simultaneously solving multiple related learning tasks is beneficial under a\nvariety of circumstances, but the prior knowledge necessary to correctly model\ntask relationships is rarely available in practice. In this paper, we develop a\nnovel kernel-based multi-task learning technique that automatically reveals\nstructural inter-task relationships. Building over the framework of output\nkernel learning (OKL), we introduce a method that jointly learns multiple\nfunctions and a low-rank multi-task kernel by solving a non-convex\nregularization problem. Optimization is carried out via a block coordinate\ndescent strategy, where each subproblem is solved using suitable conjugate\ngradient (CG) type iterative methods for linear operator equations. The\neffectiveness of the proposed approach is demonstrated on pharmacological and\ncollaborative filtering data.",
    "published": "2013-01-16T20:16:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "See the Tree Through the Lines: The Shazoo Algorithm -- Full Version --",
    "authors": [
      "Fabio Vitale",
      "Nicolo Cesa-Bianchi",
      "Claudio Gentile",
      "Giovanni Zappella"
    ],
    "summary": "Predicting the nodes of a given graph is a fascinating theoretical problem\nwith applications in several domains. Since graph sparsification via spanning\ntrees retains enough information while making the task much easier, trees are\nan important special case of this problem. Although it is known how to predict\nthe nodes of an unweighted tree in a nearly optimal way, in the weighted case a\nfully satisfactory algorithm is not available yet. We fill this hole and\nintroduce an efficient node predictor, Shazoo, which is nearly optimal on any\nweighted tree. Moreover, we show that Shazoo can be viewed as a common\nnontrivial generalization of both previous approaches for unweighted trees and\nweighted lines. Experiments on real-world datasets confirm that Shazoo performs\nwell in that it fully exploits the structure of the input tree, and gets very\nclose to (and sometimes better than) less scalable energy minimization methods.",
    "published": "2013-01-22T11:59:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Weighted Last-Step Min-Max Algorithm with Improved Sub-Logarithmic\n  Regret",
    "authors": [
      "Edward Moroshko",
      "Koby Crammer"
    ],
    "summary": "In online learning the performance of an algorithm is typically compared to\nthe performance of a fixed function from some class, with a quantity called\nregret. Forster proposed a last-step min-max algorithm which was somewhat\nsimpler than the algorithm of Vovk, yet with the same regret. In fact the\nalgorithm he analyzed assumed that the choices of the adversary are bounded,\nyielding artificially only the two extreme cases. We fix this problem by\nweighing the examples in such a way that the min-max problem will be well\ndefined, and provide analysis with logarithmic regret that may have better\nmultiplicative factor than both bounds of Forster and Vovk. We also derive a\nnew bound that may be sub-logarithmic, as a recent bound of Orabona et.al, but\nmay have better multiplicative factor. Finally, we analyze the algorithm in a\nweak-type of non-stationary setting, and show a bound that is sub-linear if the\nnon-stationarity is sub-linear as well.",
    "published": "2013-01-25T15:09:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hierarchical Data Representation Model - Multi-layer NMF",
    "authors": [
      "Hyun Ah Song",
      "Soo-Young Lee"
    ],
    "summary": "In this paper, we propose a data representation model that demonstrates\nhierarchical feature learning using nsNMF. We extend unit algorithm into\nseveral layers. Experiments with document and image data successfully\ndiscovered feature hierarchies. We also prove that proposed method results in\nmuch better classification and reconstruction performance, especially for small\nnumber of features. feature hierarchies.",
    "published": "2013-01-27T04:51:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering-Based Matrix Factorization",
    "authors": [
      "Nima Mirbakhsh",
      "Charles X. Ling"
    ],
    "summary": "Recommender systems are emerging technologies that nowadays can be found in\nmany applications such as Amazon, Netflix, and so on. These systems help users\nto find relevant information, recommendations, and their preferred items.\nSlightly improvement of the accuracy of these recommenders can highly affect\nthe quality of recommendations. Matrix Factorization is a popular method in\nRecommendation Systems showing promising results in accuracy and complexity. In\nthis paper we propose an extension of matrix factorization which adds general\nneighborhood information on the recommendation model. Users and items are\nclustered into different categories to see how these categories share\npreferences. We then employ these shared interests of categories in a fusion by\nBiased Matrix Factorization to achieve more accurate recommendations. This is a\ncomplement for the current neighborhood aware matrix factorization models which\nrely on using direct neighborhood information of users and items. The proposed\nmodel is tested on two well-known recommendation system datasets: Movielens100k\nand Netflix. Our experiment shows applying the general latent features of\ncategories into factorized recommender models improves the accuracy of\nrecommendations. The current neighborhood-aware models need a great number of\nneighbors to acheive good accuracies. To the best of our knowledge, the\nproposed model is better than or comparable with the current neighborhood-aware\nmodels when they consider fewer number of neighbors.",
    "published": "2013-01-28T20:01:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A game-theoretic framework for classifier ensembles using weighted\n  majority voting with local accuracy estimates",
    "authors": [
      "Harris V. Georgiou",
      "Michael E. Mavroforakis"
    ],
    "summary": "In this paper, a novel approach for the optimal combination of binary\nclassifiers is proposed. The classifier combination problem is approached from\na Game Theory perspective. The proposed framework of adapted weighted majority\nrules (WMR) is tested against common rank-based, Bayesian and simple majority\nmodels, as well as two soft-output averaging rules. Experiments with ensembles\nof Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) and\nweighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate that\nthis new adaptive WMR model, employing local accuracy estimators and the\nanalytically computed optimal weights outperform all the other simple\ncombination rules.",
    "published": "2013-02-03T22:12:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "RandomBoost: Simplified Multi-class Boosting through Randomization",
    "authors": [
      "Sakrapee Paisitkriangkrai",
      "Chunhua Shen",
      "Qinfeng Shi",
      "Anton van den Hengel"
    ],
    "summary": "We propose a novel boosting approach to multi-class classification problems,\nin which multiple classes are distinguished by a set of random projection\nmatrices in essence. The approach uses random projections to alleviate the\nproliferation of binary classifiers typically required to perform multi-class\nclassification. The result is a multi-class classifier with a single\nvector-valued parameter, irrespective of the number of classes involved. Two\nvariants of this approach are proposed. The first method randomly projects the\noriginal data into new spaces, while the second method randomly projects the\noutputs of learned weak classifiers. These methods are not only conceptually\nsimple but also effective and easy to implement. A series of experiments on\nsynthetic, machine learning and visual recognition data sets demonstrate that\nour proposed methods compare favorably to existing multi-class boosting\nalgorithms in terms of both the convergence rate and classification accuracy.",
    "published": "2013-02-05T09:04:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Comparison of Relaxations of Multiset Cannonical Correlation Analysis\n  and Applications",
    "authors": [
      "Jan Rupnik",
      "Primoz Skraba",
      "John Shawe-Taylor",
      "Sabrina Guettes"
    ],
    "summary": "Canonical correlation analysis is a statistical technique that is used to\nfind relations between two sets of variables. An important extension in pattern\nanalysis is to consider more than two sets of variables. This problem can be\nexpressed as a quadratically constrained quadratic program (QCQP), commonly\nreferred to Multi-set Canonical Correlation Analysis (MCCA). This is a\nnon-convex problem and so greedy algorithms converge to local optima without\nany guarantees on global optimality. In this paper, we show that despite being\nhighly structured, finding the optimal solution is NP-Hard. This motivates our\nrelaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, can\nbe solved reasonably efficiently and comes with both absolute and\noutput-sensitive approximation quality. In addition to theoretical guarantees,\nwe do an extensive comparison of the QCQP method and the SDP relaxation on a\nvariety of synthetic and real world data. Finally, we present two useful\nextensions: we incorporate kernel methods and computing multiple sets of\ncanonical vectors.",
    "published": "2013-02-05T09:45:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The price of bandit information in multiclass online classification",
    "authors": [
      "Amit Daniely",
      "Tom Helbertal"
    ],
    "summary": "We consider two scenarios of multiclass online learning of a hypothesis class\n$H\\subseteq Y^X$. In the {\\em full information} scenario, the learner is\nexposed to instances together with their labels. In the {\\em bandit} scenario,\nthe true label is not exposed, but rather an indication whether the learner's\nprediction is correct or not. We show that the ratio between the error rates in\nthe two scenarios is at most $8\\cdot|Y|\\cdot \\log(|Y|)$ in the realizable case,\nand $\\tilde{O}(\\sqrt{|Y|})$ in the agnostic case. The results are tight up to a\nlogarithmic factor and essentially answer an open question from (Daniely et.\nal. - Multiclass learnability and the erm principle).\n  We apply these results to the class of $\\gamma$-margin multiclass linear\nclassifiers in $\\reals^d$. We show that the bandit error rate of this class is\n$\\tilde{\\Theta}(\\frac{|Y|}{\\gamma^2})$ in the realizable case and\n$\\tilde{\\Theta}(\\frac{1}{\\gamma}\\sqrt{|Y|T})$ in the agnostic case. This\nresolves an open question from (Kakade et. al. - Efficient bandit algorithms\nfor online multiclass prediction).",
    "published": "2013-02-05T14:31:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Passive Learning with Target Risk",
    "authors": [
      "Mehrdad Mahdavi",
      "Rong Jin"
    ],
    "summary": "In this paper we consider learning in passive setting but with a slight\nmodification. We assume that the target expected loss, also referred to as\ntarget risk, is provided in advance for learner as prior knowledge. Unlike most\nstudies in the learning theory that only incorporate the prior knowledge into\nthe generalization bounds, we are able to explicitly utilize the target risk in\nthe learning process. Our analysis reveals a surprising result on the sample\ncomplexity of learning: by exploiting the target risk in the learning\nalgorithm, we show that when the loss function is both strongly convex and\nsmooth, the sample complexity reduces to $\\O(\\log (\\frac{1}{\\epsilon}))$, an\nexponential improvement compared to the sample complexity\n$\\O(\\frac{1}{\\epsilon})$ for learning with strongly convex loss functions.\nFurthermore, our proof is constructive and is based on a computationally\nefficient stochastic optimization algorithm for such settings which demonstrate\nthat the proposed algorithm is practically useful.",
    "published": "2013-02-08T21:18:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization",
    "authors": [
      "H. Brendan McMahan"
    ],
    "summary": "We design and analyze minimax-optimal algorithms for online linear\noptimization games where the player's choice is unconstrained. The player\nstrives to minimize regret, the difference between his loss and the loss of a\npost-hoc benchmark strategy. The standard benchmark is the loss of the best\nstrategy chosen from a bounded comparator set. When the the comparison set and\nthe adversary's gradients satisfy L_infinity bounds, we give the value of the\ngame in closed form and prove it approaches sqrt(2T/pi) as T -> infinity.\n  Interesting algorithms result when we consider soft constraints on the\ncomparator, rather than restricting it to a bounded set. As a warmup, we\nanalyze the game with a quadratic penalty. The value of this game is exactly\nT/2, and this value is achieved by perhaps the simplest online algorithm of\nall: unprojected gradient descent with a constant learning rate.\n  We then derive a minimax-optimal algorithm for a much softer penalty\nfunction. This algorithm achieves good bounds under the standard notion of\nregret for any comparator point, without needing to specify the comparator set\nin advance. The value of this game converges to sqrt{e} as T ->infinity; we\ngive a closed-form for the exact value as a function of T. The resulting\nalgorithm is natural in unconstrained investment or betting scenarios, since it\nguarantees at worst constant loss, while allowing for exponential reward\nagainst an \"easy\" adversary.",
    "published": "2013-02-08T23:16:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Time Series Forest for Classification and Feature Extraction",
    "authors": [
      "Houtao Deng",
      "George Runger",
      "Eugene Tuv",
      "Martyanov Vladimir"
    ],
    "summary": "We propose a tree ensemble method, referred to as time series forest (TSF),\nfor time series classification. TSF employs a combination of the entropy gain\nand a distance measure, referred to as the Entrance (entropy and distance)\ngain, for evaluating the splits. Experimental studies show that the Entrance\ngain criterion improves the accuracy of TSF. TSF randomly samples features at\neach tree node and has a computational complexity linear in the length of a\ntime series and can be built using parallel computing techniques such as\nmulti-core computing used here. The temporal importance curve is also proposed\nto capture the important temporal characteristics useful for classification.\nExperimental studies show that TSF using simple features such as mean,\ndeviation and slope outperforms strong competitors such as one-nearest-neighbor\nclassifiers with dynamic time warping, is computationally efficient, and can\nprovide insights into the temporal characteristics.",
    "published": "2013-02-09T22:56:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extracting useful rules through improved decision tree induction using\n  information entropy",
    "authors": [
      "Mohd Mahmood Ali",
      "Mohd S Qaseem",
      "Lakshmi Rajamani",
      "A Govardhan"
    ],
    "summary": "Classification is widely used technique in the data mining domain, where\nscalability and efficiency are the immediate problems in classification\nalgorithms for large databases. We suggest improvements to the existing C4.5\ndecision tree algorithm. In this paper attribute oriented induction (AOI) and\nrelevance analysis are incorporated with concept hierarchys knowledge and\nHeightBalancePriority algorithm for construction of decision tree along with\nMulti level mining. The assignment of priorities to attributes is done by\nevaluating information entropy, at different levels of abstraction for building\ndecision tree using HeightBalancePriority algorithm. Modified DMQL queries are\nused to understand and explore the shortcomings of the decision trees generated\nby C4.5 classifier for education dataset and the results are compared with the\nproposed approach.",
    "published": "2013-02-11T10:29:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning",
    "authors": [
      "Ronald Ortner",
      "Daniil Ryabko"
    ],
    "summary": "We derive sublinear regret bounds for undiscounted reinforcement learning in\ncontinuous state space. The proposed algorithm combines state aggregation with\nthe use of upper confidence bounds for implementing optimism in the face of\nuncertainty. Beside the existence of an optimal policy which satisfies the\nPoisson equation, the only assumptions made are Holder continuity of rewards\nand transition probabilities.",
    "published": "2013-02-11T17:44:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Selecting the State-Representation in Reinforcement Learning",
    "authors": [
      "Odalric-Ambrym Maillard",
      "Rémi Munos",
      "Daniil Ryabko"
    ],
    "summary": "The problem of selecting the right state-representation in a reinforcement\nlearning problem is considered. Several models (functions mapping past\nobservations to a finite set) of the observations are given, and it is known\nthat for at least one of these models the resulting state dynamics are indeed\nMarkovian. Without knowing neither which of the models is the correct one, nor\nwhat are the probabilistic characteristics of the resulting MDP, it is required\nto obtain as much reward as the optimal policy for the correct model (or for\nthe best of the correct models, if there are several). We propose an algorithm\nthat achieves that, with a regret of order T^{2/3} where T is the horizon time.",
    "published": "2013-02-11T17:49:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Regret Bounds for Selecting the State Representation in\n  Reinforcement Learning",
    "authors": [
      "Odalric-Ambrym Maillard",
      "Phuong Nguyen",
      "Ronald Ortner",
      "Daniil Ryabko"
    ],
    "summary": "We consider an agent interacting with an environment in a single stream of\nactions, observations, and rewards, with no reset. This process is not assumed\nto be a Markov Decision Process (MDP). Rather, the agent has several\nrepresentations (mapping histories of past interactions to a discrete state\nspace) of the environment with unknown dynamics, only some of which result in\nan MDP. The goal is to minimize the average regret criterion against an agent\nwho knows an MDP representation giving the highest optimal reward, and acts\noptimally in it. Recent regret bounds for this setting are of order\n$O(T^{2/3})$ with an additive term constant yet exponential in some\ncharacteristics of the optimal MDP. We propose an algorithm whose regret after\n$T$ time steps is $O(\\sqrt{T})$, with all constants reasonably small. This is\noptimal in $T$ since $O(\\sqrt{T})$ is the optimal regret in the setting of\nlearning in a (single discrete) MDP.",
    "published": "2013-02-11T17:55:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Efficient Dual Approach to Distance Metric Learning",
    "authors": [
      "Chunhua Shen",
      "Junae Kim",
      "Fayao Liu",
      "Lei Wang",
      "Anton van den Hengel"
    ],
    "summary": "Distance metric learning is of fundamental interest in machine learning\nbecause the distance metric employed can significantly affect the performance\nof many learning methods. Quadratic Mahalanobis metric learning is a popular\napproach to the problem, but typically requires solving a semidefinite\nprogramming (SDP) problem, which is computationally expensive. Standard\ninterior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with\n$D$ the dimension of input data), and can thus only practically solve problems\nexhibiting less than a few thousand variables. Since the number of variables is\n$D (D+1) / 2 $, this implies a limit upon the size of problem that can\npractically be solved of around a few hundred dimensions. The complexity of the\npopular quadratic Mahalanobis metric learning approach thus limits the size of\nproblem to which metric learning can be applied. Here we propose a\nsignificantly more efficient approach to the metric learning problem based on\nthe Lagrange dual formulation of the problem. The proposed formulation is much\nsimpler to implement, and therefore allows much larger Mahalanobis metric\nlearning problems to be solved. The time complexity of the proposed method is\n$O (D ^ 3) $, which is significantly lower than that of the SDP approach.\nExperiments on a variety of datasets demonstrate that the proposed method\nachieves an accuracy comparable to the state-of-the-art, but is applicable to\nsignificantly larger problems. We also show that the proposed method can be\napplied to solve more general Frobenius-norm regularized SDP problems\napproximately.",
    "published": "2013-02-13T08:48:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem",
    "authors": [
      "Ittai Abraham",
      "Omar Alonso",
      "Vasilis Kandylas",
      "Aleksandrs Slivkins"
    ],
    "summary": "Very recently crowdsourcing has become the de facto platform for distributing\nand collecting human computation for a wide range of tasks and applications\nsuch as information retrieval, natural language processing and machine\nlearning. Current crowdsourcing platforms have some limitations in the area of\nquality control. Most of the effort to ensure good quality has to be done by\nthe experimenter who has to manage the number of workers needed to reach good\nresults.\n  We propose a simple model for adaptive quality control in crowdsourced\nmultiple-choice tasks which we call the \\emph{bandit survey problem}. This\nmodel is related to, but technically different from the well-known multi-armed\nbandit problem. We present several algorithms for this problem, and support\nthem with analysis and simulations. Our approach is based in our experience\nconducting relevance evaluation for a large commercial search engine.",
    "published": "2013-02-13T22:42:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "StructBoost: Boosting Methods for Predicting Structured Output Variables",
    "authors": [
      "Chunhua Shen",
      "Guosheng Lin",
      "Anton van den Hengel"
    ],
    "summary": "Boosting is a method for learning a single accurate predictor by linearly\ncombining a set of less accurate weak learners. Recently, structured learning\nhas found many applications in computer vision. Inspired by structured support\nvector machines (SSVM), here we propose a new boosting algorithm for structured\noutput prediction, which we refer to as StructBoost. StructBoost supports\nnonlinear structured learning by combining a set of weak structured learners.\nAs SSVM generalizes SVM, our StructBoost generalizes standard boosting\napproaches such as AdaBoost, or LPBoost to structured learning. The resulting\noptimization problem of StructBoost is more challenging than SSVM in the sense\nthat it may involve exponentially many variables and constraints. In contrast,\nfor SSVM one usually has an exponential number of constraints and a\ncutting-plane method is used. In order to efficiently solve StructBoost, we\nformulate an equivalent $ 1 $-slack formulation and solve it using a\ncombination of cutting planes and column generation. We show the versatility\nand usefulness of StructBoost on a range of problems such as optimizing the\ntree loss for hierarchical multi-class classification, optimizing the Pascal\noverlap criterion for robust visual tracking and learning conditional random\nfield parameters for image segmentation.",
    "published": "2013-02-14T01:01:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Thompson Sampling in Switching Environments with Bayesian Online Change\n  Point Detection",
    "authors": [
      "Joseph Mellor",
      "Jonathan Shapiro"
    ],
    "summary": "Thompson Sampling has recently been shown to be optimal in the Bernoulli\nMulti-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes\nstationary distributions for the rewards. It is often unrealistic to model the\nreal world as a stationary distribution. In this paper we derive and evaluate\nalgorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.\nWe propose a Thompson Sampling strategy equipped with a Bayesian change point\nmechanism to tackle this problem. We develop algorithms for a variety of cases\nwith constant switching rate: when switching occurs all arms change (Global\nSwitching), switching occurs independently for each arm (Per-Arm Switching),\nwhen the switching rate is known and when it must be inferred from data. This\nleads to a family of algorithms we collectively term Change-Point Thompson\nSampling (CTS). We show empirical results of the algorithm in 4 artificial\nenvironments, and 2 derived from real world data; news click-through[Yahoo!,\n2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other\nbandit algorithms. In real world data CTS is the most effective.",
    "published": "2013-02-15T10:48:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph-based Generalization Bounds for Learning Binary Relations",
    "authors": [
      "Ben London",
      "Bert Huang",
      "Lise Getoor"
    ],
    "summary": "We investigate the generalizability of learned binary relations: functions\nthat map pairs of instances to a logical indicator. This problem has\napplication in numerous areas of machine learning, such as ranking, entity\nresolution and link prediction. Our learning framework incorporates an example\nlabeler that, given a sequence $X$ of $n$ instances and a desired training size\n$m$, subsamples $m$ pairs from $X \\times X$ without replacement. The challenge\nin analyzing this learning scenario is that pairwise combinations of random\nvariables are inherently dependent, which prevents us from using traditional\nlearning-theoretic arguments. We present a unified, graph-based analysis, which\nallows us to analyze this dependence using well-known graph identities. We are\nthen able to bound the generalization error of learned binary relations using\nRademacher complexity and algorithmic stability. The rate of uniform\nconvergence is partially determined by the labeler's subsampling process. We\nthus examine how various assumptions about subsampling affect generalization;\nunder a natural random subsampling process, our bounds guarantee\n$\\tilde{O}(1/\\sqrt{n})$ uniform convergence.",
    "published": "2013-02-21T17:30:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Importance of Clipping in Neurocontrol by Direct Gradient Descent on\n  the Cost-to-Go Function and in Adaptive Dynamic Programming",
    "authors": [
      "Michael Fairbank"
    ],
    "summary": "In adaptive dynamic programming, neurocontrol and reinforcement learning, the\nobjective is for an agent to learn to choose actions so as to minimise a total\ncost function. In this paper we show that when discretized time is used to\nmodel the motion of the agent, it can be very important to do \"clipping\" on the\nmotion of the agent in the final time step of the trajectory. By clipping we\nmean that the final time step of the trajectory is to be truncated such that\nthe agent stops exactly at the first terminal state reached, and no distance\nfurther. We demonstrate that when clipping is omitted, learning performance can\nfail to reach the optimum; and when clipping is done properly, learning\nperformance can improve significantly.\n  The clipping problem we describe affects algorithms which use explicit\nderivatives of the model functions of the environment to calculate a learning\ngradient. These include Backpropagation Through Time for Control, and methods\nbased on Dual Heuristic Dynamic Programming. However the clipping problem does\nnot significantly affect methods based on Heuristic Dynamic Programming,\nTemporal Differences or Policy Gradient Learning algorithms. Similarly, the\nclipping problem does not affect fixed-length finite-horizon problems.",
    "published": "2013-02-22T12:11:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction by Random-Walk Perturbation",
    "authors": [
      "Luc Devroye",
      "Gábor Lugosi",
      "Gergely Neu"
    ],
    "summary": "We propose a version of the follow-the-perturbed-leader online prediction\nalgorithm in which the cumulative losses are perturbed by independent symmetric\nrandom walks. The forecaster is shown to achieve an expected regret of the\noptimal order O(sqrt(n log N)) where n is the time horizon and N is the number\nof experts. More importantly, it is shown that the forecaster changes its\nprediction at most O(sqrt(n log N)) times, in expectation. We also extend the\nanalysis to online combinatorial optimization and show that even in this more\ngeneral setting, the forecaster rarely switches between experts while having a\nregret of near-optimal order.",
    "published": "2013-02-23T13:33:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude\n  and Phase Functions",
    "authors": [
      "Yin Ding",
      "Ivan W. Selesnick"
    ],
    "summary": "This paper addresses the problem of expressing a signal as a sum of frequency\ncomponents (sinusoids) wherein each sinusoid may exhibit abrupt changes in its\namplitude and/or phase. The Fourier transform of a narrow-band signal, with a\ndiscontinuous amplitude and/or phase function, exhibits spectral and temporal\nspreading. The proposed method aims to avoid such spreading by explicitly\nmodeling the signal of interest as a sum of sinusoids with time-varying\namplitudes. So as to accommodate abrupt changes, it is further assumed that the\namplitude/phase functions are approximately piecewise constant (i.e., their\ntime-derivatives are sparse). The proposed method is based on a convex\nvariational (optimization) approach wherein the total variation (TV) of the\namplitude functions are regularized subject to a perfect (or approximate)\nreconstruction constraint. A computationally efficient algorithm is derived\nbased on convex optimization techniques. The proposed technique can be used to\nperform band-pass filtering that is relatively insensitive to narrow-band\namplitude/phase jumps present in data, which normally pose a challenge (due to\ntransients, leakage, etc.). The method is illustrated using both synthetic\nsignals and human EEG data for the purpose of band-pass filtering and the\nestimation of phase synchrony indexes.",
    "published": "2013-02-26T18:18:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning for Time Series Prediction",
    "authors": [
      "Oren Anava",
      "Elad Hazan",
      "Shie Mannor",
      "Ohad Shamir"
    ],
    "summary": "In this paper we address the problem of predicting a time series using the\nARMA (autoregressive moving average) model, under minimal assumptions on the\nnoise terms. Using regret minimization techniques, we develop effective online\nlearning algorithms for the prediction problem, without assuming that the noise\nterms are Gaussian, identically distributed or even independent. Furthermore,\nwe show that our algorithm's performances asymptotically approaches the\nperformance of the best ARMA model in hindsight.",
    "published": "2013-02-27T17:14:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Convex Optimization Against Adversaries with Memory and\n  Application to Statistical Arbitrage",
    "authors": [
      "Oren Anava",
      "Elad Hazan",
      "Shie Mannor"
    ],
    "summary": "The framework of online learning with memory naturally captures learning\nproblems with temporal constraints, and was previously studied for the experts\nsetting. In this work we extend the notion of learning with memory to the\ngeneral Online Convex Optimization (OCO) framework, and present two algorithms\nthat attain low regret. The first algorithm applies to Lipschitz continuous\nloss functions, obtaining optimal regret bounds for both convex and strongly\nconvex losses. The second algorithm attains the optimal regret bounds and\napplies more broadly to convex losses without requiring Lipschitz continuity,\nyet is more complicated to implement. We complement our theoretic results with\nan application to statistical arbitrage in finance: we devise algorithms for\nconstructing mean-reverting portfolios.",
    "published": "2013-02-27T17:46:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Similarity Prediction of Networked Data from Known and Unknown\n  Graphs",
    "authors": [
      "Claudio Gentile",
      "Mark Herbster",
      "Stephen Pasteris"
    ],
    "summary": "We consider online similarity prediction problems over networked data. We\nbegin by relating this task to the more standard class prediction problem,\nshowing that, given an arbitrary algorithm for class prediction, we can\nconstruct an algorithm for similarity prediction with \"nearly\" the same mistake\nbound, and vice versa. After noticing that this general construction is\ncomputationally infeasible, we target our study to {\\em feasible} similarity\nprediction algorithms on networked data. We initially assume that the network\nstructure is {\\em known} to the learner. Here we observe that Matrix Winnow\n\\cite{w07} has a near-optimal mistake guarantee, at the price of cubic\nprediction time per round. This motivates our effort for an efficient\nimplementation of a Perceptron algorithm with a weaker mistake guarantee but\nwith only poly-logarithmic prediction time. Our focus then turns to the\nchallenging case of networks whose structure is initially {\\em unknown} to the\nlearner. In this novel setting, where the network structure is only\nincrementally revealed, we obtain a mistake-bounded algorithm with a quadratic\nprediction time per round.",
    "published": "2013-02-28T17:15:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Hash Functions Using Column Generation",
    "authors": [
      "Xi Li",
      "Guosheng Lin",
      "Chunhua Shen",
      "Anton van den Hengel",
      "Anthony Dick"
    ],
    "summary": "Fast nearest neighbor searching is becoming an increasingly important tool in\nsolving many large-scale problems. Recently a number of approaches to learning\ndata-dependent hash functions have been developed. In this work, we propose a\ncolumn generation based method for learning data-dependent hash functions on\nthe basis of proximity comparison information. Given a set of triplets that\nencode the pairwise proximity comparison information, our method learns hash\nfunctions that preserve the relative comparison relationships in the data as\nwell as possible within the large-margin learning framework. The learning\nprocedure is implemented using column generation and hence is named CGHash. At\neach iteration of the column generation procedure, the best hash function is\nselected. Unlike most other hashing methods, our method generalizes to new data\npoints naturally; and has a training objective which is convex, thus ensuring\nthat the global optimum can be identified. Experiments demonstrate that the\nproposed method learns compact binary codes and that its retrieval performance\ncompares favorably with state-of-the-art methods when tested on a few benchmark\ndatasets.",
    "published": "2013-03-02T03:01:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Inductive Sparse Subspace Clustering",
    "authors": [
      "Xi Peng",
      "Lei Zhang",
      "Zhang Yi"
    ],
    "summary": "Sparse Subspace Clustering (SSC) has achieved state-of-the-art clustering\nquality by performing spectral clustering over a $\\ell^{1}$-norm based\nsimilarity graph. However, SSC is a transductive method which does not handle\nwith the data not used to construct the graph (out-of-sample data). For each\nnew datum, SSC requires solving $n$ optimization problems in O(n) variables for\nperforming the algorithm over the whole data set, where $n$ is the number of\ndata points. Therefore, it is inefficient to apply SSC in fast online\nclustering and scalable graphing. In this letter, we propose an inductive\nspectral clustering algorithm, called inductive Sparse Subspace Clustering\n(iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts the\nassumption that high-dimensional data actually lie on the low-dimensional\nmanifold such that out-of-sample data could be grouped in the embedding space\nlearned from in-sample data. Experimental results show that iSSC is promising\nin clustering out-of-sample data.",
    "published": "2013-03-02T07:47:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Convex and Scalable Weakly Labeled SVMs",
    "authors": [
      "Yu-Feng Li",
      "Ivor W. Tsang",
      "James T. Kwok",
      "Zhi-Hua Zhou"
    ],
    "summary": "In this paper, we study the problem of learning from weakly labeled data,\nwhere labels of the training examples are incomplete. This includes, for\nexample, (i) semi-supervised learning where labels are partially known; (ii)\nmulti-instance learning where labels are implicitly known; and (iii) clustering\nwhere labels are completely unknown. Unlike supervised learning, learning with\nweak labels involves a difficult Mixed-Integer Programming (MIP) problem.\nTherefore, it can suffer from poor scalability and may also get stuck in local\nminimum. In this paper, we focus on SVMs and propose the WellSVM via a novel\nlabel generation strategy. This leads to a convex relaxation of the original\nMIP, which is at least as tight as existing convex Semi-Definite Programming\n(SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM\nsubproblems that are much more scalable than previous convex SDP relaxations.\nExperiments on three weakly labeled learning tasks, namely, (i) semi-supervised\nlearning; (ii) multi-instance learning for locating regions of interest in\ncontent-based information retrieval; and (iii) clustering, clearly demonstrate\nimproved performance, and WellSVM is also readily applicable on large data\nsets.",
    "published": "2013-03-06T08:20:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-relational Learning Using Weighted Tensor Decomposition with\n  Modular Loss",
    "authors": [
      "Ben London",
      "Theodoros Rekatsinas",
      "Bert Huang",
      "Lise Getoor"
    ],
    "summary": "We propose a modular framework for multi-relational learning via tensor\ndecomposition. In our learning setting, the training data contains multiple\ntypes of relationships among a set of objects, which we represent by a sparse\nthree-mode tensor. The goal is to predict the values of the missing entries. To\ndo so, we model each relationship as a function of a linear combination of\nlatent factors. We learn this latent representation by computing a low-rank\ntensor decomposition, using quasi-Newton optimization of a weighted objective\nfunction. Sparsity in the observed data is captured by the weighted objective,\nleading to improved accuracy when training data is limited. Exploiting sparsity\nalso improves efficiency, potentially up to an order of magnitude over\nunweighted approaches. In addition, our framework accommodates arbitrary\ncombinations of smooth, task-specific loss functions, making it better suited\nfor learning different types of relations. For the typical cases of real-valued\nfunctions and binary relations, we propose several loss functions and derive\nthe associated parameter gradients. We evaluate our method on synthetic and\nreal data, showing significant improvements in both accuracy and scalability\nover related factorization techniques.",
    "published": "2013-03-07T16:10:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Transfer Learning for Voice Activity Detection: A Denoising Deep Neural\n  Network Perspective",
    "authors": [
      "Xiao-Lei Zhang",
      "Ji Wu"
    ],
    "summary": "Mismatching problem between the source and target noisy corpora severely\nhinder the practical use of the machine-learning-based voice activity detection\n(VAD). In this paper, we try to address this problem in the transfer learning\nprospective. Transfer learning tries to find a common learning machine or a\ncommon feature subspace that is shared by both the source corpus and the target\ncorpus. The denoising deep neural network is used as the learning machine.\nThree transfer techniques, which aim to learn common feature representations,\nare used for analysis. Experimental results demonstrate the effectiveness of\nthe transfer learning schemes on the mismatch problem.",
    "published": "2013-03-08T20:46:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Convex Discriminative Multitask Clustering",
    "authors": [
      "Xiao-Lei Zhang"
    ],
    "summary": "Multitask clustering tries to improve the clustering performance of multiple\ntasks simultaneously by taking their relationship into account. Most existing\nmultitask clustering algorithms fall into the type of generative clustering,\nand none are formulated as convex optimization problems. In this paper, we\npropose two convex Discriminative Multitask Clustering (DMTC) algorithms to\naddress the problems. Specifically, we first propose a Bayesian DMTC framework.\nThen, we propose two convex DMTC objectives within the framework. The first\none, which can be seen as a technical combination of the convex multitask\nfeature learning and the convex Multiclass Maximum Margin Clustering (M3C),\naims to learn a shared feature representation. The second one, which can be\nseen as a combination of the convex multitask relationship learning and M3C,\naims to learn the task relationship. The two objectives are solved in a uniform\nprocedure by the efficient cutting-plane algorithm. Experimental results on a\ntoy problem and two benchmark datasets demonstrate the effectiveness of the\nproposed algorithms.",
    "published": "2013-03-08T21:32:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization\n  and Layered Clustering-Based Approach",
    "authors": [
      "Xiao-Lei Zhang"
    ],
    "summary": "One important classifier ensemble for multiclass classification problems is\nError-Correcting Output Codes (ECOCs). It bridges multiclass problems and\nbinary-class classifiers by decomposing multiclass problems to a serial\nbinary-class problems. In this paper, we present a heuristic ternary code,\nnamed Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). It\nstarts with an arbitrary valid ECOC and iterates the following two steps until\nthe training risk converges. The first step, named Layered Clustering based\nECOC (LC-ECOC), constructs multiple strong classifiers on the most confusing\nbinary-class problem. The second step adds the new classifiers to ECOC by a\nnovel Optimized Weighted (OW) decoding algorithm, where the optimization\nproblem of the decoding is solved by the cutting plane algorithm. Technically,\nLC-ECOC makes the heuristic training process not blocked by some difficult\nbinary-class problem. OW decoding guarantees the non-increase of the training\nrisk for ensuring a small code length. Results on 14 UCI datasets and a music\ngenre classification problem demonstrate the effectiveness of WOLC-ECOC.",
    "published": "2013-03-08T21:40:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Last-Step Regression Algorithm for Non-Stationary Online Learning",
    "authors": [
      "Edward Moroshko",
      "Koby Crammer"
    ],
    "summary": "The goal of a learner in standard online learning is to maintain an average\nloss close to the loss of the best-performing single function in some class. In\nmany real-world problems, such as rating or ranking items, there is no single\nbest target function during the runtime of the algorithm, instead the best\n(local) target function is drifting over time. We develop a novel last-step\nminmax optimal algorithm in context of a drift. We analyze the algorithm in the\nworst-case regret framework and show that it maintains an average loss close to\nthat of the best slowly changing sequence of linear functions, as long as the\ntotal of drift is sublinear. In some situations, our bound improves over\nexisting bounds, and additionally the algorithm suffers logarithmic regret when\nthere is no drift. We also build on the H_infinity filter and its bound, and\ndevelop and analyze a second algorithm for drifting setting. Synthetic\nsimulations demonstrate the advantages of our algorithms in a worst-case\nconstant drift setting.",
    "published": "2013-03-15T12:20:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Quorum Sensing Inspired Algorithm for Dynamic Clustering",
    "authors": [
      "Feng Tan",
      "Jean-Jacques Slotine"
    ],
    "summary": "Quorum sensing is a decentralized biological process, through which a\ncommunity of cells with no global awareness coordinate their functional\nbehaviors based solely on cell-medium interactions and local decisions. This\npaper draws inspirations from quorum sensing and colony competition to derive a\nnew algorithm for data clustering. The algorithm treats each data as a single\ncell, and uses knowledge of local connectivity to cluster cells into multiple\ncolonies simultaneously. It simulates auto-inducers secretion in quorum sensing\nto tune the influence radius for each cell. At the same time, sparsely\ndistributed core cells spread their influences to form colonies, and\ninteractions between colonies eventually determine each cell's identity. The\nalgorithm has the flexibility to analyze not only static but also time-varying\ndata, which surpasses the capacity of many existing algorithms. Its stability\nand convergence properties are established. The algorithm is tested on several\napplications, including both synthetic and real benchmarks data sets, alleles\nclustering, community detection, image segmentation. In particular, the\nalgorithm's distinctive capability to deal with time-varying data allows us to\nexperiment it on novel applications such as robotic swarms grouping and\nswitching model identification. We believe that the algorithm's promising\nperformance would stimulate many more exciting applications.",
    "published": "2013-03-16T00:49:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On multi-class learning through the minimization of the confusion matrix\n  norm",
    "authors": [
      "Sokol Koço",
      "Cécile Capponi"
    ],
    "summary": "In imbalanced multi-class classification problems, the misclassification rate\nas an error measure may not be a relevant choice. Several methods have been\ndeveloped where the performance measure retained richer information than the\nmere misclassification rate: misclassification costs, ROC-based information,\netc. Following this idea of dealing with alternate measures of performance, we\npropose to address imbalanced classification problems by using a new measure to\nbe optimized: the norm of the confusion matrix. Indeed, recent results show\nthat using the norm of the confusion matrix as an error measure can be quite\ninteresting due to the fine-grain informations contained in the matrix,\nespecially in the case of imbalanced classes. Our first contribution then\nconsists in showing that optimizing criterion based on the confusion matrix\ngives rise to a common background for cost-sensitive methods aimed at dealing\nwith imbalanced classes learning problems. As our second contribution, we\npropose an extension of a recent multi-class boosting method --- namely\nAdaBoost.MM --- to the imbalanced class problem, by greedily minimizing the\nempirical norm of the confusion matrix. A theoretical analysis of the\nproperties of the proposed method is presented, while experimental results\nillustrate the behavior of the algorithm and show the relevancy of the approach\ncompared to other methods.",
    "published": "2013-03-16T20:09:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in\n  Locality-Sensitive Hashing",
    "authors": [
      "Yui Noma",
      "Makiko Konoshima"
    ],
    "summary": "Since Hamming distances can be calculated by bitwise computations, they can\nbe calculated with less computational load than L2 distances. Similarity\nsearches can therefore be performed faster in Hamming distance space. The\nelements of Hamming distance space are bit strings. On the other hand, the\narrangement of hyperplanes induce the transformation from the feature vectors\ninto feature bit strings. This transformation method is a type of\nlocality-sensitive hashing that has been attracting attention as a way of\nperforming approximate similarity searches at high speed. Supervised learning\nof hyperplane arrangements allows us to obtain a method that transforms them\ninto feature bit strings reflecting the information of labels applied to\nhigher-dimensional feature vectors. In this p aper, we propose a supervised\nlearning method for hyperplane arrangements in feature space that uses a Markov\nchain Monte Carlo (MCMC) method. We consider the probability density functions\nused during learning, and evaluate their performance. We also consider the\nsampling method for learning data pairs needed in learning, and we evaluate its\nperformance. We confirm that the accuracy of this learning method when using a\nsuitable probability density function and sampling method is greater than the\naccuracy of existing learning methods.",
    "published": "2013-03-18T07:14:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-Scale Learning with Less RAM via Randomization",
    "authors": [
      "Daniel Golovin",
      "D. Sculley",
      "H. Brendan McMahan",
      "Michael Young"
    ],
    "summary": "We reduce the memory footprint of popular large-scale online learning methods\nby projecting our weight vector onto a coarse discrete set using randomized\nrounding. Compared to standard 32-bit float encodings, this reduces RAM usage\nby more than 50% during training and by up to 95% when making predictions from\na fixed model, with almost no loss in accuracy. We also show that randomized\ncounting can be used to implement per-coordinate learning rates, improving\nmodel quality with little additional RAM. We prove these memory-saving methods\nachieve regret guarantees similar to their exact variants. Empirical evaluation\nconfirms excellent performance, dominating standard approaches across memory\nversus accuracy tradeoffs.",
    "published": "2013-03-19T17:00:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Note on k-support Norm Regularized Risk Minimization",
    "authors": [
      "Matthew Blaschko"
    ],
    "summary": "The k-support norm has been recently introduced to perform correlated\nsparsity regularization. Although Argyriou et al. only reported experiments\nusing squared loss, here we apply it to several other commonly used settings\nresulting in novel machine learning algorithms with interesting and familiar\nlimit cases. Source code for the algorithms described here is available.",
    "published": "2013-03-26T06:01:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Inductive Hashing on Manifolds",
    "authors": [
      "Fumin Shen",
      "Chunhua Shen",
      "Qinfeng Shi",
      "Anton van den Hengel",
      "Zhenmin Tang"
    ],
    "summary": "Learning based hashing methods have attracted considerable attention due to\ntheir ability to greatly increase the scale at which existing algorithms may\noperate. Most of these methods are designed to generate binary codes that\npreserve the Euclidean distance in the original space. Manifold learning\ntechniques, in contrast, are better able to model the intrinsic structure\nembedded in the original high-dimensional data. The complexity of these models,\nand the problems with out-of-sample data, have previously rendered them\nunsuitable for application to large-scale embedding, however. In this work, we\nconsider how to learn compact binary embeddings on their intrinsic manifolds.\nIn order to address the above-mentioned difficulties, we describe an efficient,\ninductive solution to the out-of-sample data problem, and a process by which\nnon-parametric manifold learning may be used as the basis of a hashing method.\nOur proposed approach thus allows the development of a range of new hashing\ntechniques exploiting the flexibility of the wide variety of manifold learning\napproaches available. We particularly show that hashing on the basis of t-SNE .",
    "published": "2013-03-28T05:45:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "O(logT) Projections for Stochastic Optimization of Smooth and Strongly\n  Convex Functions",
    "authors": [
      "Lijun Zhang",
      "Tianbao Yang",
      "Rong Jin",
      "Xiaofei He"
    ],
    "summary": "Traditional algorithms for stochastic optimization require projecting the\nsolution at each iteration into a given domain to ensure its feasibility. When\nfacing complex domains, such as positive semi-definite cones, the projection\noperation can be expensive, leading to a high computational cost per iteration.\nIn this paper, we present a novel algorithm that aims to reduce the number of\nprojections for stochastic optimization. The proposed algorithm combines the\nstrength of several recent developments in stochastic optimization, including\nmini-batch, extra-gradient, and epoch gradient descent, in order to effectively\nexplore the smoothness and strong convexity. We show, both in expectation and\nwith a high probability, that when the objective function is both smooth and\nstrongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of\nconvergence with only $O(\\log T)$ projections. Our empirical study verifies the\ntheoretical result.",
    "published": "2013-04-02T19:11:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch\n  Stochastic Gradient Descent (SGD)",
    "authors": [
      "Qi Qian",
      "Rong Jin",
      "Jinfeng Yi",
      "Lijun Zhang",
      "Shenghuo Zhu"
    ],
    "summary": "Distance metric learning (DML) is an important task that has found\napplications in many domains. The high computational cost of DML arises from\nthe large number of variables to be determined and the constraint that a\ndistance metric has to be a positive semi-definite (PSD) matrix. Although\nstochastic gradient descent (SGD) has been successfully applied to improve the\nefficiency of DML, it can still be computationally expensive because in order\nto ensure that the solution is a PSD matrix, it has to, at every iteration,\nproject the updated distance metric onto the PSD cone, an expensive operation.\nWe address this challenge by developing two strategies within SGD, i.e.\nmini-batch and adaptive sampling, to effectively reduce the number of updates\n(i.e., projections onto the PSD cone) in SGD. We also develop hybrid approaches\nthat combine the strength of adaptive sampling with that of mini-batch online\nlearning techniques to further improve the computational efficiency of SGD for\nDML. We prove the theoretical guarantees for both adaptive sampling and\nmini-batch based approaches for DML. We also conduct an extensive empirical\nstudy to verify the effectiveness of the proposed algorithms for DML.",
    "published": "2013-04-03T21:14:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast SVM training using approximate extreme points",
    "authors": [
      "Manu Nandan",
      "Pramod P. Khargonekar",
      "Sachin S. Talathi"
    ],
    "summary": "Applications of non-linear kernel Support Vector Machines (SVMs) to large\ndatasets is seriously hampered by its excessive training time. We propose a\nmodification, called the approximate extreme points support vector machine\n(AESVM), that is aimed at overcoming this burden. Our approach relies on\nconducting the SVM optimization over a carefully selected subset, called the\nrepresentative set, of the training dataset. We present analytical results that\nindicate the similarity of AESVM and SVM solutions. A linear time algorithm\nbased on convex hulls and extreme points is used to compute the representative\nset in kernel space. Extensive computational experiments on nine datasets\ncompared AESVM to LIBSVM \\citep{LIBSVM}, CVM \\citep{Tsang05}, BVM\n\\citep{Tsang07}, LASVM \\citep{Bordes05}, $\\text{SVM}^{\\text{perf}}$\n\\citep{Joachims09}, and the random features method \\citep{rahimi07}. Our AESVM\nimplementation was found to train much faster than the other methods, while its\nclassification accuracy was similar to that of LIBSVM in all cases. In\nparticular, for a seizure detection dataset, AESVM training was almost $10^3$\ntimes faster than LIBSVM and LASVM and more than forty times faster than CVM\nand BVM. Additionally, AESVM also gave competitively fast classification times.",
    "published": "2013-04-04T15:08:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Generalized Online Mirror Descent with Applications to Classification\n  and Regression",
    "authors": [
      "Francesco Orabona",
      "Koby Crammer",
      "Nicolò Cesa-Bianchi"
    ],
    "summary": "Online learning algorithms are fast, memory-efficient, easy to implement, and\napplicable to many prediction problems, including classification, regression,\nand ranking. Several online algorithms were proposed in the past few decades,\nsome based on additive updates, like the Perceptron, and some on multiplicative\nupdates, like Winnow. A unifying perspective on the design and the analysis of\nonline algorithms is provided by online mirror descent, a general prediction\nstrategy from which most first-order algorithms can be obtained as special\ncases. We generalize online mirror descent to time-varying regularizers with\ngeneric updates. Unlike standard mirror descent, our more general formulation\nalso captures second order algorithms, algorithms for composite losses and\nalgorithms for adaptive filtering. Moreover, we recover, and sometimes improve,\nknown regret bounds as special cases of our analysis using specific\nregularizers. Finally, we show the power of our approach by deriving a new\nsecond order algorithm with a regret bound invariant with respect to arbitrary\nrescalings of individual features.",
    "published": "2013-04-10T15:26:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering",
    "authors": [
      "Badreddine Meftahi",
      "Ourida Ben Boubaker Saidi"
    ],
    "summary": "Many studies in data mining have proposed a new learning called\nsemi-Supervised. Such type of learning combines unlabeled and labeled data\nwhich are hard to obtain. However, in unsupervised methods, the only unlabeled\ndata are used. The problem of significance and the effectiveness of\nsemi-supervised clustering results is becoming of main importance. This paper\npursues the thesis that muchgreater accuracy can be achieved in such clustering\nby improving the similarity computing. Hence, we introduce a new approach of\nsemisupervised clustering using an innovative new homogeneity measure of\ngenerated clusters. Our experimental results demonstrate significantly improved\naccuracy as a result.",
    "published": "2013-04-13T20:19:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey on Multi-view Learning",
    "authors": [
      "Chang Xu",
      "Dacheng Tao",
      "Chao Xu"
    ],
    "summary": "In recent years, a great many methods of learning from multi-view data by\nconsidering the diversity of different views have been proposed. These views\nmay be obtained from multiple sources or different feature subsets. In trying\nto organize and highlight similarities and differences between the variety of\nmulti-view learning approaches, we review a number of representative multi-view\nlearning algorithms in different areas and classify them into three groups: 1)\nco-training, 2) multiple kernel learning, and 3) subspace learning. Notably,\nco-training style algorithms train alternately to maximize the mutual agreement\non two distinct views of the data; multiple kernel learning algorithms exploit\nkernels that naturally correspond to different views and combine kernels either\nlinearly or non-linearly to improve learning performance; and subspace learning\nalgorithms aim to obtain a latent subspace shared by multiple views by assuming\nthat the input views are generated from this latent subspace. Though there is\nsignificant variance in the approaches to integrating multiple views to improve\nlearning performance, they mainly exploit either the consensus principle or the\ncomplementary principle to ensure the success of multi-view learning. Since\naccessing multiple views is the fundament of multi-view learning, with the\nexception of study on learning a model from multiple views, it is also valuable\nto study how to construct multiple views and how to evaluate these views.\nOverall, by exploring the consistency and complementary properties of different\nviews, multi-view learning is rendered more effective, more promising, and has\nbetter generalization ability than single-view learning.",
    "published": "2013-04-20T14:43:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Continuum armed bandit problem of few variables in high dimensions",
    "authors": [
      "Hemant Tyagi",
      "Bernd Gärtner"
    ],
    "summary": "We consider the stochastic and adversarial settings of continuum armed\nbandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d\n-> R are assumed to intrinsically depend on at most k coordinate variables\nimplying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknown\ni_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R with\nexponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed across\ntime, we propose a simple modification of the CAB1 algorithm where we construct\nthe discrete set of sampling points to obtain a bound of\nO(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the\nregret, with C(k,d) depending at most polynomially in k and sub-logarithmically\nin d. The construction is based on creating partitions of {1,..,d} into k\ndisjoint subsets and is probabilistic, hence our result holds with high\nprobability. Secondly we extend our results to also handle the more general\ncase where (i_1,...,i_k) can change over time and derive regret bounds for the\nsame.",
    "published": "2013-04-21T20:03:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Irreflexive and Hierarchical Relations as Translations",
    "authors": [
      "Antoine Bordes",
      "Nicolas Usunier",
      "Alberto Garcia-Duran",
      "Jason Weston",
      "Oksana Yakhnenko"
    ],
    "summary": "We consider the problem of embedding entities and relations of knowledge\nbases in low-dimensional vector spaces. Unlike most existing approaches, which\nare primarily efficient for modeling equivalence relations, our approach is\ndesigned to explicitly model irreflexive relations, such as hierarchies, by\ninterpreting them as translations operating on the low-dimensional embeddings\nof the entities. Preliminary experiments show that, despite its simplicity and\na smaller number of parameters than previous approaches, our approach achieves\nstate-of-the-art performance according to standard evaluation protocols on data\nfrom WordNet and Freebase.",
    "published": "2013-04-26T13:28:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fractal structures in Adversarial Prediction",
    "authors": [
      "Rina Panigrahy",
      "Preyas Popat"
    ],
    "summary": "Fractals are self-similar recursive structures that have been used in\nmodeling several real world processes. In this work we study how \"fractal-like\"\nprocesses arise in a prediction game where an adversary is generating a\nsequence of bits and an algorithm is trying to predict them. We will see that\nunder a certain formalization of the predictive payoff for the algorithm it is\nmost optimal for the adversary to produce a fractal-like sequence to minimize\nthe algorithm's ability to predict. Indeed it has been suggested before that\nfinancial markets exhibit a fractal-like behavior. We prove that a fractal-like\ndistribution arises naturally out of an optimization from the adversary's\nperspective.\n  In addition, we give optimal trade-offs between predictability and expected\ndeviation (i.e. sum of bits) for our formalization of predictive payoff. This\nresult is motivated by the observation that several time series data exhibit\nhigher deviations than expected for a completely random walk.",
    "published": "2013-04-29T07:16:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering Unclustered Data: Unsupervised Binary Labeling of Two\n  Datasets Having Different Class Balances",
    "authors": [
      "Marthinus Christoffel du Plessis",
      "Masashi Sugiyama"
    ],
    "summary": "We consider the unsupervised learning problem of assigning labels to\nunlabeled data. A naive approach is to use clustering methods, but this works\nwell only when data is properly clustered and each cluster corresponds to an\nunderlying class. In this paper, we first show that this unsupervised labeling\nproblem in balanced binary cases can be solved if two unlabeled datasets having\ndifferent class balances are available. More specifically, estimation of the\nsign of the difference between probability densities of two unlabeled datasets\ngives the solution. We then introduce a new method to directly estimate the\nsign of the density difference without density estimation. Finally, we\ndemonstrate the usefulness of the proposed method against several clustering\nmethods on various toy problems and real-world datasets.",
    "published": "2013-05-01T06:32:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Perceptron Mistake Bounds",
    "authors": [
      "Mehryar Mohri",
      "Afshin Rostamizadeh"
    ],
    "summary": "We present a brief survey of existing mistake bounds and introduce novel\nbounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds\ngeneralize beyond standard margin-loss type bounds, allow for any convex and\nLipschitz loss function, and admit a very simple proof.",
    "published": "2013-05-01T15:45:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Learning of Representations: Looking Forward",
    "authors": [
      "Yoshua Bengio"
    ],
    "summary": "Deep learning research aims at discovering learning algorithms that discover\nmultiple levels of distributed representations, with higher levels representing\nmore abstract concepts. Although the study of deep learning has already led to\nimpressive theoretical results, learning algorithms and breakthrough\nexperiments, several challenges lie ahead. This paper proposes to examine some\nof these challenges, centering on the questions of scaling deep learning\nalgorithms to much larger models and datasets, reducing optimization\ndifficulties due to ill-conditioning or local minima, designing more efficient\nand powerful inference and sampling procedures, and learning to disentangle the\nfactors of variation underlying the observed data. It also proposes a few\nforward-looking research directions aimed at overcoming these challenges.",
    "published": "2013-05-02T14:33:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Spectral Classification Using Restricted Boltzmann Machine",
    "authors": [
      "Fuqiang Chen",
      "Yan Wu",
      "Yude Bu",
      "Guodong Zhao"
    ],
    "summary": "In this study, a novel machine learning algorithm, restricted Boltzmann\nmachine (RBM), is introduced. The algorithm is applied for the spectral\nclassification in astronomy. RBM is a bipartite generative graphical model with\ntwo separate layers (one visible layer and one hidden layer), which can extract\nhigher level features to represent the original data. Despite generative, RBM\ncan be used for classification when modified with a free energy and a soft-max\nfunction. Before spectral classification, the original data is binarized\naccording to some rule. Then we resort to the binary RBM to classify\ncataclysmic variables (CVs) and non-CVs (one half of all the given data for\ntraining and the other half for testing). The experiment result shows\nstate-of-the-art accuracy of 100%, which indicates the efficiency of the binary\nRBM algorithm.",
    "published": "2013-05-03T10:20:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning from Imprecise and Fuzzy Observations: Data Disambiguation\n  through Generalized Loss Minimization",
    "authors": [
      "Eyke Hüllermeier"
    ],
    "summary": "Methods for analyzing or learning from \"fuzzy data\" have attracted increasing\nattention in recent years. In many cases, however, existing methods (for\nprecise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner,\nand without carefully considering the interpretation of a fuzzy set when being\nused for modeling data. Distinguishing between an ontic and an epistemic\ninterpretation of fuzzy set-valued data, and focusing on the latter, we argue\nthat a \"fuzzification\" of learning algorithms based on an application of the\ngeneric extension principle is not appropriate. In fact, the extension\nprinciple fails to properly exploit the inductive bias underlying statistical\nand machine learning methods, although this bias, at least in principle, offers\na means for \"disambiguating\" the fuzzy data. Alternatively, we therefore\npropose a method which is based on the generalization of loss functions in\nempirical risk minimization, and which performs model identification and data\ndisambiguation simultaneously. Elaborating on the fuzzification of specific\ntypes of losses, we establish connections to well-known loss functions in\nregression and classification. We compare our approach with related methods and\nillustrate its use in logistic regression for binary classification.",
    "published": "2013-05-03T13:26:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Simple Deep Random Model Ensemble",
    "authors": [
      "Xiao-Lei Zhang",
      "Ji Wu"
    ],
    "summary": "Representation learning and unsupervised learning are two central topics of\nmachine learning and signal processing. Deep learning is one of the most\neffective unsupervised representation learning approach. The main contributions\nof this paper to the topics are as follows. (i) We propose to view the\nrepresentative deep learning approaches as special cases of the knowledge reuse\nframework of clustering ensemble. (ii) We propose to view sparse coding when\nused as a feature encoder as the consensus function of clustering ensemble, and\nview dictionary learning as the training process of the base clusterings of\nclustering ensemble. (ii) Based on the above two views, we propose a very\nsimple deep learning algorithm, named deep random model ensemble (DRME). It is\na stack of random model ensembles. Each random model ensemble is a special\nk-means ensemble that discards the expectation-maximization optimization of\neach base k-means but only preserves the default initialization method of the\nbase k-means. (iv) We propose to select the most powerful representation among\nthe layers by applying DRME to clustering where the single-linkage is used as\nthe clustering algorithm. Moreover, the DRME based clustering can also detect\nthe number of the natural clusters accurately. Extensive experimental\ncomparisons with 5 representation learning methods on 19 benchmark data sets\ndemonstrate the effectiveness of DRME.",
    "published": "2013-05-05T14:58:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Differential Equations Approach to Optimizing Regret Trade-offs",
    "authors": [
      "Alexandr Andoni",
      "Rina Panigrahy"
    ],
    "summary": "We consider the classical question of predicting binary sequences and study\nthe {\\em optimal} algorithms for obtaining the best possible regret and payoff\nfunctions for this problem. The question turns out to be also equivalent to the\nproblem of optimal trade-offs between the regrets of two experts in an \"experts\nproblem\", studied before by \\cite{kearns-regret}. While, say, a regret of\n$\\Theta(\\sqrt{T})$ is known, we argue that it important to ask what is the\nprovably optimal algorithm for this problem --- both because it leads to\nnatural algorithms, as well as because regret is in fact often comparable in\nmagnitude to the final payoffs and hence is a non-negligible term.\n  In the basic setting, the result essentially follows from a classical result\nof Cover from '65. Here instead, we focus on another standard setting, of\ntime-discounted payoffs, where the final \"stopping time\" is not specified. We\nexhibit an explicit characterization of the optimal regret for this setting.\n  To obtain our main result, we show that the optimal payoff functions have to\nsatisfy the Hermite differential equation, and hence are given by the solutions\nto this equation. It turns out that characterization of the payoff function is\nqualitatively different from the classical (non-discounted) setting, and,\nnamely, there's essentially a unique optimal solution.",
    "published": "2013-05-07T00:02:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "One-Pass AUC Optimization",
    "authors": [
      "Wei Gao",
      "Rong Jin",
      "Shenghuo Zhu",
      "Zhi-Hua Zhou"
    ],
    "summary": "AUC is an important performance measure and many algorithms have been devoted\nto AUC optimization, mostly by minimizing a surrogate convex loss on a training\ndata set. In this work, we focus on one-pass AUC optimization that requires\nonly going through the training data once without storing the entire training\ndataset, where conventional online learning algorithms cannot be applied\ndirectly because AUC is measured by a sum of losses defined over pairs of\ninstances from different classes. We develop a regression-based algorithm which\nonly needs to maintain the first and second order statistics of training data\nin memory, resulting a storage requirement independent from the size of\ntraining data. To efficiently handle high dimensional data, we develop a\nrandomized algorithm that approximates the covariance matrices by low rank\nmatrices. We verify, both theoretically and empirically, the effectiveness of\nthe proposed algorithm.",
    "published": "2013-05-07T00:30:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Class Imbalance Problem in Data Mining Review",
    "authors": [
      "Rushi Longadge",
      "Snehalata Dongre"
    ],
    "summary": "In last few years there are major changes and evolution has been done on\nclassification of data. As the application area of technology is increases the\nsize of data also increases. Classification of data becomes difficult because\nof unbounded size and imbalance nature of data. Class imbalance problem become\ngreatest issue in data mining. Imbalance problem occur where one of the two\nclasses having more sample than other classes. The most of algorithm are more\nfocusing on classification of major sample while ignoring or misclassifying\nminority sample. The minority samples are those that rarely occur but very\nimportant. There are different methods available for classification of\nimbalance data set which is divided into three main categories, the algorithmic\napproach, data-preprocessing approach and feature selection approach. Each of\nthis technique has their own advantages and disadvantages. In this paper\nsystematic study of each approach is define which gives the right direction for\nresearch in class imbalance problem.",
    "published": "2013-05-08T03:39:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet\n  Allocation",
    "authors": [
      "James Foulds",
      "Levi Boyles",
      "Christopher Dubois",
      "Padhraic Smyth",
      "Max Welling"
    ],
    "summary": "In the internet era there has been an explosion in the amount of digital text\ninformation available, leading to difficulties of scale for traditional\ninference algorithms for topic models. Recent advances in stochastic\nvariational inference algorithms for latent Dirichlet allocation (LDA) have\nmade it feasible to learn topic models on large-scale corpora, but these\nmethods do not currently take full advantage of the collapsed representation of\nthe model. We propose a stochastic algorithm for collapsed variational Bayesian\ninference for LDA, which is simpler and more efficient than the state of the\nart method. We show connections between collapsed variational Bayesian\ninference and MAP estimation for LDA, and leverage these connections to prove\nconvergence properties of the proposed algorithm. In experiments on large-scale\ntext corpora, the algorithm was found to converge faster and often to a better\nsolution than the previous method. Human-subject experiments also demonstrated\nthat the method can learn coherent topics in seconds on small corpora,\nfacilitating the use of topic models in interactive document analysis software.",
    "published": "2013-05-10T23:06:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An efficient algorithm for learning with semi-bandit feedback",
    "authors": [
      "Gergely Neu",
      "Gábor Bartók"
    ],
    "summary": "We consider the problem of online combinatorial optimization under\nsemi-bandit feedback. The goal of the learner is to sequentially select its\nactions from a combinatorial decision set so as to minimize its cumulative\nloss. We propose a learning algorithm for this problem based on combining the\nFollow-the-Perturbed-Leader (FPL) prediction method with a novel loss\nestimation procedure called Geometric Resampling (GR). Contrary to previous\nsolutions, the resulting algorithm can be efficiently implemented for any\ndecision set where efficient offline combinatorial optimization is possible at\nall. Assuming that the elements of the decision set can be described with\nd-dimensional binary vectors with at most m non-zero entries, we show that the\nexpected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As a\nside result, we also improve the best known regret bounds for FPL in the full\ninformation setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m)\nover previous bounds for this algorithm.",
    "published": "2013-05-13T10:39:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Estimating or Propagating Gradients Through Stochastic Neurons",
    "authors": [
      "Yoshua Bengio"
    ],
    "summary": "Stochastic neurons can be useful for a number of reasons in deep learning\nmodels, but in many cases they pose a challenging problem: how to estimate the\ngradient of a loss function with respect to the input of such stochastic\nneurons, i.e., can we \"back-propagate\" through these stochastic neurons? We\nexamine this question, existing approaches, and present two novel families of\nsolutions, applicable in different settings. In particular, it is demonstrated\nthat a simple biologically plausible formula gives rise to an an unbiased (but\nnoisy) estimator of the gradient with respect to a binary stochastic neuron\nfiring probability. Unlike other estimators which view the noise as a small\nperturbation in order to estimate gradients by finite differences, this\nestimator is unbiased even without assuming that the stochastic perturbation is\nsmall. This estimator is also interesting because it can be applied in very\ngeneral settings which do not allow gradient back-propagation, including the\nestimation of the gradient with respect to future rewards, as required in\nreinforcement learning setups. We also propose an approach to approximating\nthis unbiased but high-variance estimator by learning to predict it using a\nbiased estimator. The second approach we propose assumes that an estimator of\nthe gradient can be back-propagated and it provides an unbiased estimator of\nthe gradient, but can only work with non-linearities unlike the hard threshold,\nbut like the rectifier, that are not flat for all of their range. This is\nsimilar to traditional sigmoidal units but has the advantage that for many\ninputs, a hard decision (e.g., a 0 output) can be produced, which would be\nconvenient for conditional computation and achieving sparse representations and\nsparse gradients.",
    "published": "2013-05-14T00:29:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Contractive De-noising Auto-encoder",
    "authors": [
      "Fu-qiang Chen",
      "Yan Wu",
      "Guo-dong Zhao",
      "Jun-ming Zhang",
      "Ming Zhu",
      "Jing Bai"
    ],
    "summary": "Auto-encoder is a special kind of neural network based on reconstruction.\nDe-noising auto-encoder (DAE) is an improved auto-encoder which is robust to\nthe input by corrupting the original data first and then reconstructing the\noriginal input by minimizing the reconstruction error function. And contractive\nauto-encoder (CAE) is another kind of improved auto-encoder to learn robust\nfeature by introducing the Frobenius norm of the Jacobean matrix of the learned\nfeature with respect to the original input. In this paper, we combine\nde-noising auto-encoder and contractive auto- encoder, and propose another\nimproved auto-encoder, contractive de-noising auto- encoder (CDAE), which is\nrobust to both the original input and the learned feature. We stack CDAE to\nextract more abstract features and apply SVM for classification. The experiment\nresult on benchmark dataset MNIST shows that our proposed CDAE performed better\nthan both DAE and CAE, proving the effective of our method.",
    "published": "2013-05-17T13:42:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Ensembles of Classifiers based on Dimensionality Reduction",
    "authors": [
      "Alon Schclar",
      "Lior Rokach",
      "Amir Amit"
    ],
    "summary": "We present a novel approach for the construction of ensemble classifiers\nbased on dimensionality reduction. Dimensionality reduction methods represent\ndatasets using a small number of attributes while preserving the information\nconveyed by the original dataset. The ensemble members are trained based on\ndimension-reduced versions of the training set. These versions are obtained by\napplying dimensionality reduction to the original training set using different\nvalues of the input parameters. This construction meets both the diversity and\naccuracy criteria which are required to construct an ensemble classifier where\nthe former criterion is obtained by the various input parameter values and the\nlatter is achieved due to the decorrelation and noise reduction properties of\ndimensionality reduction. In order to classify a test sample, it is first\nembedded into the dimension reduced space of each individual classifier by\nusing an out-of-sample extension algorithm. Each classifier is then applied to\nthe embedded sample and the classification is obtained via a voting scheme. We\npresent three variations of the proposed approach based on the Random\nProjections, the Diffusion Maps and the Random Subspaces dimensionality\nreduction algorithms. We also present a multi-strategy ensemble which combines\nAdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost,\nRotation Forest ensemble classifiers and also with the base classifier which\ndoes not incorporate dimensionality reduction. Our experiments used seventeen\nbenchmark datasets from the UCI repository. The results obtained by the\nproposed algorithms were superior in many cases to other algorithms.",
    "published": "2013-05-19T10:24:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generalized Denoising Auto-Encoders as Generative Models",
    "authors": [
      "Yoshua Bengio",
      "Li Yao",
      "Guillaume Alain",
      "Pascal Vincent"
    ],
    "summary": "Recent work has shown how denoising and contractive autoencoders implicitly\ncapture the structure of the data-generating density, in the case where the\ncorruption noise is Gaussian, the reconstruction error is the squared error,\nand the data is continuous-valued. This has led to various proposals for\nsampling from this implicitly learned density function, using Langevin and\nMetropolis-Hastings MCMC. However, it remained unclear how to connect the\ntraining procedure of regularized auto-encoders to the implicit estimation of\nthe underlying data-generating distribution when the data are discrete, or\nusing other forms of corruption process and reconstruction errors. Another\nissue is the mathematical justification which is only valid in the limit of\nsmall corruption noise. We propose here a different attack on the problem,\nwhich deals with all these issues: arbitrary (but noisy enough) corruption,\narbitrary reconstruction loss (seen as a log-likelihood), handling both\ndiscrete and continuous-valued variables, and removing the bias due to\nnon-infinitesimal corruption noise (or non-infinitesimal contractive penalty).",
    "published": "2013-05-29T00:25:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Test cost and misclassification cost trade-off using reframing",
    "authors": [
      "Celestine Periale Maguedong-Djoumessi",
      "José Hernández-Orallo"
    ],
    "summary": "Many solutions to cost-sensitive classification (and regression) rely on some\nor all of the following assumptions: we have complete knowledge about the cost\ncontext at training time, we can easily re-train whenever the cost context\nchanges, and we have technique-specific methods (such as cost-sensitive\ndecision trees) that can take advantage of that information. In this paper we\naddress the problem of selecting models and minimising joint cost (integrating\nboth misclassification cost and test costs) without any of the above\nassumptions. We introduce methods and plots (such as the so-called JROC plots)\nthat can work with any off-the-shelf predictive technique, including ensembles,\nsuch that we reframe the model to use the appropriate subset of attributes (the\nfeature configuration) during deployment time. In other words, models are\ntrained with the available attributes (once and for all) and then deployed by\nsetting missing values on the attributes that are deemed ineffective for\nreducing the joint cost. As the number of feature configuration combinations\ngrows exponentially with the number of features we introduce quadratic methods\nthat are able to approximate the optimal configuration and model choices, as\nshown by the experimental results.",
    "published": "2013-05-30T13:52:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Understanding ACT-R - an Outsider's Perspective",
    "authors": [
      "Jacob Whitehill"
    ],
    "summary": "The ACT-R theory of cognition developed by John Anderson and colleagues\nendeavors to explain how humans recall chunks of information and how they solve\nproblems. ACT-R also serves as a theoretical basis for \"cognitive tutors\",\ni.e., automatic tutoring systems that help students learn mathematics, computer\nprogramming, and other subjects. The official ACT-R definition is distributed\nacross a large body of literature spanning many articles and monographs, and\nhence it is difficult for an \"outsider\" to learn the most important aspects of\nthe theory. This paper aims to provide a tutorial to the core components of the\nACT-R theory.",
    "published": "2013-06-01T15:48:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Guided Random Forest in the RRF Package",
    "authors": [
      "Houtao Deng"
    ],
    "summary": "Random Forest (RF) is a powerful supervised learner and has been popularly\nused in many applications such as bioinformatics.\n  In this work we propose the guided random forest (GRF) for feature selection.\nSimilar to a feature selection method called guided regularized random forest\n(GRRF), GRF is built using the importance scores from an ordinary RF. However,\nthe trees in GRRF are built sequentially, are highly correlated and do not\nallow for parallel computing, while the trees in GRF are built independently\nand can be implemented in parallel. Experiments on 10 high-dimensional gene\ndata sets show that, with a fixed parameter value (without tuning the\nparameter), RF applied to features selected by GRF outperforms RF applied to\nall features on 9 data sets and 7 of them have significant differences at the\n0.05 level. Therefore, both accuracy and interpretability are significantly\nimproved. GRF selects more features than GRRF, however, leads to better\nclassification accuracy. Note in this work the guided random forest is guided\nby the importance scores from an ordinary random forest, however, it can also\nbe guided by other methods such as human insights (by specifying $\\lambda_i$).\nGRF can be used in \"RRF\" v1.4 (and later versions), a package that also\nincludes the regularized random forest methods.",
    "published": "2013-06-02T18:30:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Generative Stochastic Networks Trainable by Backprop",
    "authors": [
      "Yoshua Bengio",
      "Éric Thibodeau-Laufer",
      "Guillaume Alain",
      "Jason Yosinski"
    ],
    "summary": "We introduce a novel training principle for probabilistic models that is an\nalternative to maximum likelihood. The proposed Generative Stochastic Networks\n(GSN) framework is based on learning the transition operator of a Markov chain\nwhose stationary distribution estimates the data distribution. The transition\ndistribution of the Markov chain is conditional on the previous state,\ngenerally involving a small move, so this conditional distribution has fewer\ndominant modes, being unimodal in the limit of small moves. Thus, it is easier\nto learn because it is easier to approximate its partition function, more like\nlearning to perform supervised function approximation, with gradients that can\nbe obtained by backprop. We provide theorems that generalize recent work on the\nprobabilistic interpretation of denoising autoencoders and obtain along the way\nan interesting justification for dependency networks and generalized\npseudolikelihood, along with a definition of an appropriate joint distribution\nand sampling mechanism even when the conditionals are not consistent. GSNs can\nbe used with missing inputs and can be used to sample subsets of variables\ngiven the rest. We validate these theoretical results with experiments on two\nimage datasets using an architecture that mimics the Deep Boltzmann Machine\nGibbs sampler but allows training to proceed with simple backprop, without the\nneed for layerwise pretraining.",
    "published": "2013-06-05T13:01:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Performance analysis of unsupervised feature selection methods",
    "authors": [
      "A. Nisthana Parveen",
      "H. Hannah Inbarani",
      "E. N. Sathishkumar"
    ],
    "summary": "Feature selection (FS) is a process which attempts to select more informative\nfeatures. In some cases, too many redundant or irrelevant features may\noverpower main features for classification. Feature selection can remedy this\nproblem and therefore improve the prediction accuracy and reduce the\ncomputational overhead of classification algorithms. The main aim of feature\nselection is to determine a minimal feature subset from a problem domain while\nretaining a suitably high accuracy in representing the original features. In\nthis paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick\nReduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are\napplied to discover discriminative features that will be the most adequate ones\nfor classification. Efficiency of the approaches is evaluated using standard\nclassification metrics.",
    "published": "2013-06-06T07:42:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Auditing: Active Learning with Outcome-Dependent Query Costs",
    "authors": [
      "Sivan Sabato",
      "Anand D. Sarwate",
      "Nathan Srebro"
    ],
    "summary": "We propose a learning setting in which unlabeled data is free, and the cost\nof a label depends on its value, which is not known in advance. We study binary\nclassification in an extreme case, where the algorithm only pays for negative\nlabels. Our motivation are applications such as fraud detection, in which\ninvestigating an honest transaction should be avoided if possible. We term the\nsetting auditing, and consider the auditing complexity of an algorithm: the\nnumber of negative labels the algorithm requires in order to learn a hypothesis\nwith low relative error. We design auditing algorithms for simple hypothesis\nclasses (thresholds and rectangles), and show that with these algorithms, the\nauditing complexity can be significantly lower than the active label\ncomplexity. We also discuss a general competitive approach for auditing and\npossible modifications to the framework.",
    "published": "2013-06-10T20:18:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Guaranteed Classification via Regularized Similarity Learning",
    "authors": [
      "Zheng-Chu Guo",
      "Yiming Ying"
    ],
    "summary": "Learning an appropriate (dis)similarity function from the available data is a\ncentral problem in machine learning, since the success of many machine learning\nalgorithms critically depends on the choice of a similarity function to compare\nexamples. Despite many approaches for similarity metric learning have been\nproposed, there is little theoretical study on the links between similarity\nmet- ric learning and the classification performance of the result classifier.\nIn this paper, we propose a regularized similarity learning formulation\nassociated with general matrix-norms, and establish their generalization\nbounds. We show that the generalization error of the resulting linear separator\ncan be bounded by the derived generalization bound of similarity learning. This\nshows that a good gen- eralization of the learnt similarity function guarantees\na good classification of the resulting linear classifier. Our results extend\nand improve those obtained by Bellet at al. [3]. Due to the techniques\ndependent on the notion of uniform stability [6], the bound obtained there\nholds true only for the Frobenius matrix- norm regularization. Our techniques\nusing the Rademacher complexity [5] and its related Khinchin-type inequality\nenable us to establish bounds for regularized similarity learning formulations\nassociated with general matrix-norms including sparse L 1 -norm and mixed\n(2,1)-norm.",
    "published": "2013-06-13T13:47:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On-line PCA with Optimal Regrets",
    "authors": [
      "Jiazhong Nie",
      "Wojciech Kotlowski",
      "Manfred K. Warmuth"
    ],
    "summary": "We carefully investigate the on-line version of PCA, where in each trial a\nlearning algorithm plays a k-dimensional subspace, and suffers the compression\nloss on the next instance when projected into the chosen subspace. In this\nsetting, we analyze two popular on-line algorithms, Gradient Descent (GD) and\nExponentiated Gradient (EG). We show that both algorithms are essentially\noptimal in the worst-case. This comes as a surprise, since EG is known to\nperform sub-optimally when the instances are sparse. This different behavior of\nEG for PCA is mainly related to the non-negativity of the loss in this case,\nwhich makes the PCA setting qualitatively different from other settings studied\nin the literature. Furthermore, we show that when considering regret bounds as\nfunction of a loss budget, EG remains optimal and strictly outperforms GD.\nNext, we study the extension of the PCA setting, in which the Nature is allowed\nto play with dense instances, which are positive matrices with bounded largest\neigenvalue. Again we can show that EG is optimal and strictly better than GD in\nthis setting.",
    "published": "2013-06-17T15:29:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiarmed Bandits With Limited Expert Advice",
    "authors": [
      "Satyen Kale"
    ],
    "summary": "We solve the COLT 2013 open problem of \\citet{SCB} on minimizing regret in\nthe setting of advice-efficient multiarmed bandits with expert advice. We give\nan algorithm for the setting of K arms and N experts out of which we are\nallowed to query and use only M experts' advices in each round, which has a\nregret bound of \\tilde{O}\\bigP{\\sqrt{\\frac{\\min\\{K, M\\} N}{M} T}} after T\nrounds. We also prove that any algorithm for this problem must have expected\nregret at least \\tilde{\\Omega}\\bigP{\\sqrt{\\frac{\\min\\{K, M\\} N}{M}T}}, thus\nshowing that our upper bound is nearly tight.",
    "published": "2013-06-19T19:25:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Machine Teaching for Bayesian Learners in the Exponential Family",
    "authors": [
      "Xiaojin Zhu"
    ],
    "summary": "What if there is a teacher who knows the learning goal and wants to design\ngood training data for a machine learner? We propose an optimal teaching\nframework aimed at learners who employ Bayesian models. Our framework is\nexpressed as an optimization problem over teaching examples that balance the\nfuture loss of the learner and the effort of the teacher. This optimization\nproblem is in general hard. In the case where the learner employs conjugate\nexponential family models, we present an approximate algorithm for finding the\noptimal teaching set. Our algorithm optimizes the aggregate sufficient\nstatistics, then unpacks them into actual teaching examples. We give several\nexamples to illustrate our framework.",
    "published": "2013-06-20T18:04:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Song-based Classification techniques for Endangered Bird Conservation",
    "authors": [
      "Erick Stattner",
      "Wilfried Segretier",
      "Martine Collard",
      "Philippe Hunel",
      "Nicolas Vidot"
    ],
    "summary": "The work presented in this paper is part of a global framework which long\nterm goal is to design a wireless sensor network able to support the\nobservation of a population of endangered birds. We present the first stage for\nwhich we have conducted a knowledge discovery approach on a sample of\nacoustical data. We use MFCC features extracted from bird songs and we exploit\ntwo knowledge discovery techniques. One that relies on clustering-based\napproaches, that highlights the homogeneity in the songs of the species. The\nother, based on predictive modeling, that demonstrates the good performances of\nvarious machine learning techniques for the identification process. The\nknowledge elicited provides promising results to consider a widespread study\nand to elicit guidelines for designing a first version of the automatic\napproach for data collection based on acoustic sensors.",
    "published": "2013-06-22T19:32:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Model Reframing by Feature Context Change",
    "authors": [
      "Celestine-Periale Maguedong-Djoumessi"
    ],
    "summary": "The feature space (including both input and output variables) characterises a\ndata mining problem. In predictive (supervised) problems, the quality and\navailability of features determines the predictability of the dependent\nvariable, and the performance of data mining models in terms of\nmisclassification or regression error. Good features, however, are usually\ndifficult to obtain. It is usual that many instances come with missing values,\neither because the actual value for a given attribute was not available or\nbecause it was too expensive. This is usually interpreted as a utility or\ncost-sensitive learning dilemma, in this case between misclassification (or\nregression error) costs and attribute tests costs. Both misclassification cost\n(MC) and test cost (TC) can be integrated into a single measure, known as joint\ncost (JC). We introduce methods and plots (such as the so-called JROC plots)\nthat can work with any of-the-shelf predictive technique, including ensembles,\nsuch that we re-frame the model to use the appropriate subset of attributes\n(the feature configuration) during deployment time. In other words, models are\ntrained with the available attributes (once and for all) and then deployed by\nsetting missing values on the attributes that are deemed ineffective for\nreducing the joint cost. As the number of feature configuration combinations\ngrows exponentially with the number of features we introduce quadratic methods\nthat are able to approximate the optimal configuration and model choices, as\nshown by the experimental results.",
    "published": "2013-06-23T23:36:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploratory Learning",
    "authors": [
      "Bhavana Dalvi",
      "William W. Cohen",
      "Jamie Callan"
    ],
    "summary": "In multiclass semi-supervised learning (SSL), it is sometimes the case that\nthe number of classes present in the data is not known, and hence no labeled\nexamples are provided for some classes. In this paper we present variants of\nwell-known semi-supervised multiclass learning methods that are robust when the\ndata contains an unknown number of classes. In particular, we present an\n\"exploratory\" extension of expectation-maximization (EM) that explores\ndifferent numbers of classes while learning. \"Exploratory\" SSL greatly improves\nperformance on three datasets in terms of F1 on the classes with seed examples\ni.e., the classes which are expected to be in the data. Our Exploratory EM\nalgorithm also outperforms a SSL method based non-parametric Bayesian\nclustering.",
    "published": "2013-07-01T01:09:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A PAC-Bayesian Tutorial with A Dropout Bound",
    "authors": [
      "David McAllester"
    ],
    "summary": "This tutorial gives a concise overview of existing PAC-Bayesian theory\nfocusing on three generalization bounds. The first is an Occam bound which\nhandles rules with finite precision parameters and which states that\ngeneralization loss is near training loss when the number of bits needed to\nwrite the rule is small compared to the sample size. The second is a\nPAC-Bayesian bound providing a generalization guarantee for posterior\ndistributions rather than for individual rules. The PAC-Bayesian bound\nnaturally handles infinite precision rule parameters, $L_2$ regularization,\n{\\em provides a bound for dropout training}, and defines a natural notion of a\nsingle distinguished PAC-Bayesian posterior distribution. The third bound is a\ntraining-variance bound --- a kind of bias-variance analysis but with bias\nreplaced by expected training loss. The training-variance bound dominates the\nother bounds but is more difficult to interpret. It seems to suggest variance\nreduction methods such as bagging and may ultimately provide a more meaningful\nanalysis of dropouts.",
    "published": "2013-07-08T15:03:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Minimum Error Rate Training and the Convex Hull Semiring",
    "authors": [
      "Chris Dyer"
    ],
    "summary": "We describe the line search used in the minimum error rate training algorithm\nMERT as the \"inside score\" of a weighted proof forest under a semiring defined\nin terms of well-understood operations from computational geometry. This\nconception leads to a straightforward complexity analysis of the dynamic\nprogramming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009)\nand practical approaches to implementation.",
    "published": "2013-07-13T19:38:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-scale Multi-label Learning with Missing Labels",
    "authors": [
      "Hsiang-Fu Yu",
      "Prateek Jain",
      "Purushottam Kar",
      "Inderjit S. Dhillon"
    ],
    "summary": "The multi-label classification problem has generated significant interest in\nrecent years. However, existing approaches do not adequately address two key\nchallenges: (a) the ability to tackle problems with a large number (say\nmillions) of labels, and (b) the ability to handle data with missing labels. In\nthis paper, we directly address both these problems by studying the multi-label\nproblem in a generic empirical risk minimization (ERM) framework. Our\nframework, despite being simple, is surprisingly able to encompass several\nrecent label-compression based methods which can be derived as special cases of\nour method. To optimize the ERM problem, we develop techniques that exploit the\nstructure of specific loss functions - such as the squared loss function - to\noffer efficient algorithms. We further show that our learning framework admits\nformal excess risk bounds even in the presence of missing labels. Our risk\nbounds are tight and demonstrate better generalization performance for low-rank\npromoting trace-norm regularization when compared to (rank insensitive)\nFrobenius norm regularization. Finally, we present extensive empirical results\non a variety of benchmark datasets and show that our methods perform\nsignificantly better than existing label compression based methods and can\nscale up to very large datasets such as the Wikipedia dataset.",
    "published": "2013-07-18T23:55:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Distribution-Free Multi-Armed Bandits with Combinatorial\n  Strategies",
    "authors": [
      "Xiang-yang Li",
      "Shaojie Tang",
      "Yaqin Zhou"
    ],
    "summary": "In this paper we study a generalized version of classical multi-armed bandits\n(MABs) problem by allowing for arbitrary constraints on constituent bandits at\neach decision point. The motivation of this study comes from many situations\nthat involve repeatedly making choices subject to arbitrary constraints in an\nuncertain environment: for instance, regularly deciding which advertisements to\ndisplay online in order to gain high click-through-rate without knowing user\npreferences, or what route to drive home each day under uncertain weather and\ntraffic conditions. Assume that there are $K$ unknown random variables (RVs),\ni.e., arms, each evolving as an \\emph{i.i.d} stochastic process over time. At\neach decision epoch, we select a strategy, i.e., a subset of RVs, subject to\narbitrary constraints on constituent RVs.\n  We then gain a reward that is a linear combination of observations on\nselected RVs.\n  The performance of prior results for this problem heavily depends on the\ndistribution of strategies generated by corresponding learning policy. For\nexample, if the reward-difference between the best and second best strategy\napproaches zero, prior result may lead to arbitrarily large regret.\n  Meanwhile, when there are exponential number of possible strategies at each\ndecision point, naive extension of a prior distribution-free policy would cause\npoor performance in terms of regret, computation and space complexity.\n  To this end, we propose an efficient Distribution-Free Learning (DFL) policy\nthat achieves zero regret, regardless of the probability distribution of the\nresultant strategies.\n  Our learning policy has both $O(K)$ time complexity and $O(K)$ space\ncomplexity. In successive generations, we show that even if finding the optimal\nstrategy at each decision point is NP-hard, our policy still allows for\napproximated solutions while retaining near zero-regret.",
    "published": "2013-07-20T16:40:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A scalable stage-wise approach to large-margin multi-class loss based\n  boosting",
    "authors": [
      "Sakrapee Paisitkriangkrai",
      "Chunhua Shen",
      "Anton van den Hengel"
    ],
    "summary": "We present a scalable and effective classification model to train multi-class\nboosting for multi-class classification problems. Shen and Hao introduced a\ndirect formulation of multi- class boosting in the sense that it directly\nmaximizes the multi- class margin [C. Shen and Z. Hao, \"A direct formulation\nfor totally-corrective multi- class boosting\", in Proc. IEEE Conf. Comp. Vis.\nPatt. Recogn., 2011]. The major problem of their approach is its high\ncomputational complexity for training, which hampers its application on\nreal-world problems. In this work, we propose a scalable and simple stage-wise\nmulti-class boosting method, which also directly maximizes the multi-class\nmargin. Our approach of- fers a few advantages: 1) it is simple and\ncomputationally efficient to train. The approach can speed up the training time\nby more than two orders of magnitude without sacrificing the classification\naccuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice of\nparameters and empirically demonstrates excellent generalization performance.\nExperimental results on challenging multi-class machine learning and vision\ntasks demonstrate that the proposed approach substantially improves the\nconvergence rate and accuracy of the final visual detector at no additional\ncomputational cost compared to existing multi-class boosting.",
    "published": "2013-07-21T06:06:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A New Strategy of Cost-Free Learning in the Class Imbalance Problem",
    "authors": [
      "Xiaowan Zhang",
      "Bao-Gang Hu"
    ],
    "summary": "In this work, we define cost-free learning (CFL) formally in comparison with\ncost-sensitive learning (CSL). The main difference between them is that a CFL\napproach seeks optimal classification results without requiring any cost\ninformation, even in the class imbalance problem. In fact, several CFL\napproaches exist in the related studies, such as sampling and some\ncriteria-based pproaches. However, to our best knowledge, none of the existing\nCFL and CSL approaches are able to process the abstaining classifications\nproperly when no information is given about errors and rejects. Based on\ninformation theory, we propose a novel CFL which seeks to maximize normalized\nmutual information of the targets and the decision outputs of classifiers.\nUsing the strategy, we can deal with binary/multi-class classifications\nwith/without abstaining. Significant features are observed from the new\nstrategy. While the degree of class imbalance is changing, the proposed\nstrategy is able to balance the errors and rejects accordingly and\nautomatically. Another advantage of the strategy is its ability of deriving\noptimal rejection thresholds for abstaining classifications and the\n\"equivalent\" costs in binary classifications. The connection between rejection\nthresholds and ROC curve is explored. Empirical investigation is made on\nseveral benchmark data sets in comparison with other existing approaches. The\nclassification results demonstrate a promising perspective of the strategy in\nmachine learning.",
    "published": "2013-07-22T14:36:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Propound Method for the Improvement of Cluster Quality",
    "authors": [
      "Shveta Kundra Bhatia",
      "V. S. Dixit"
    ],
    "summary": "In this paper Knockout Refinement Algorithm (KRA) is proposed to refine\noriginal clusters obtained by applying SOM and K-Means clustering algorithms.\nKRA Algorithm is based on Contingency Table concepts. Metrics are computed for\nthe Original and Refined Clusters. Quality of Original and Refined Clusters are\ncompared in terms of metrics. The proposed algorithm (KRA) is tested in the\neducational domain and results show that it generates better quality clusters\nin terms of improved metric values.",
    "published": "2013-07-25T17:07:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Minimax Online Learning with Unknown Time Horizon",
    "authors": [
      "Haipeng Luo",
      "Robert E. Schapire"
    ],
    "summary": "We consider online learning when the time horizon is unknown. We apply a\nminimax analysis, beginning with the fixed horizon case, and then moving on to\ntwo unknown-horizon settings, one that assumes the horizon is chosen randomly\naccording to some known distribution, and the other which allows the adversary\nfull control over the horizon. For the random horizon setting with restricted\nlosses, we derive a fully optimal minimax algorithm. And for the adversarial\nhorizon setting, we prove a nontrivial lower bound which shows that the\nadversary obtains strictly more power than when the horizon is fixed and known.\nBased on the minimax solution of the random horizon setting, we then propose a\nnew adaptive algorithm which \"pretends\" that the horizon is drawn from a\ndistribution from a special family, but no matter how the actual horizon is\nchosen, the worst-case regret is of the optimal rate. Furthermore, our\nalgorithm can be combined and applied in many ways, for instance, to online\nconvex optimization, follow the perturbed leader, exponential weights algorithm\nand first order bounds. Experiments show that our algorithm outperforms many\nother existing algorithms in an online linear optimization setting.",
    "published": "2013-07-31T01:49:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Planning-ahead SMO Algorithm",
    "authors": [
      "Tobias Glasmachers"
    ],
    "summary": "The sequential minimal optimization (SMO) algorithm and variants thereof are\nthe de facto standard method for solving large quadratic programs for support\nvector machine (SVM) training. In this paper we propose a simple yet powerful\nmodification. The main emphasis is on an algorithm improving the SMO step size\nby planning-ahead. The theoretical analysis ensures its convergence to the\noptimum. Experiments involving a large number of datasets were carried out to\ndemonstrate the superiority of the new algorithm.",
    "published": "2013-07-31T12:38:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multiclass learnability and the ERM principle",
    "authors": [
      "Amit Daniely",
      "Sivan Sabato",
      "Shai Ben-David",
      "Shai Shalev-Shwartz"
    ],
    "summary": "We study the sample complexity of multiclass prediction in several learning\nsettings. For the PAC setting our analysis reveals a surprising phenomenon: In\nsharp contrast to binary classification, we show that there exist multiclass\nhypothesis classes for which some Empirical Risk Minimizers (ERM learners) have\nlower sample complexity than others. Furthermore, there are classes that are\nlearnable by some ERM learners, while other ERM learners will fail to learn\nthem. We propose a principle for designing good ERM learners, and use this\nprinciple to prove tight bounds on the sample complexity of learning {\\em\nsymmetric} multiclass hypothesis classes---classes that are invariant under\npermutations of label names. We further provide a characterization of mistake\nand regret bounds for multiclass learning in the online setting and the bandit\nsetting, using new generalizations of Littlestone's dimension.",
    "published": "2013-08-13T15:15:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Estimating or Propagating Gradients Through Stochastic Neurons for\n  Conditional Computation",
    "authors": [
      "Yoshua Bengio",
      "Nicholas Léonard",
      "Aaron Courville"
    ],
    "summary": "Stochastic neurons and hard non-linearities can be useful for a number of\nreasons in deep learning models, but in many cases they pose a challenging\nproblem: how to estimate the gradient of a loss function with respect to the\ninput of such stochastic or non-smooth neurons? I.e., can we \"back-propagate\"\nthrough these stochastic neurons? We examine this question, existing\napproaches, and compare four families of solutions, applicable in different\nsettings. One of them is the minimum variance unbiased gradient estimator for\nstochatic binary neurons (a special case of the REINFORCE algorithm). A second\napproach, introduced here, decomposes the operation of a binary stochastic\nneuron into a stochastic binary part and a smooth differentiable part, which\napproximates the expected effect of the pure stochatic binary neuron to first\norder. A third approach involves the injection of additive or multiplicative\nnoise in a computational graph that is otherwise differentiable. A fourth\napproach heuristically copies the gradient with respect to the stochastic\noutput directly as an estimator of the gradient with respect to the sigmoid\nargument (we call this the straight-through estimator). To explore a context\nwhere these estimators are useful, we consider a small-scale version of {\\em\nconditional computation}, where sparse stochastic units form a distributed\nrepresentation of gaters that can turn off in combinatorially many ways large\nchunks of the computation performed in the rest of the neural network. In this\ncase, it is important that the gating units produce an actual 0 most of the\ntime. The resulting sparsity can be potentially be exploited to greatly reduce\nthe computational cost of large deep networks for which conditional computation\nwould be useful.",
    "published": "2013-08-15T15:19:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Optimization for Machine Learning",
    "authors": [
      "Andrew Cotter"
    ],
    "summary": "It has been found that stochastic algorithms often find good solutions much\nmore rapidly than inherently-batch approaches. Indeed, a very useful rule of\nthumb is that often, when solving a machine learning problem, an iterative\ntechnique which relies on performing a very large number of\nrelatively-inexpensive updates will often outperform one which performs a\nsmaller number of much \"smarter\" but computationally-expensive updates.\n  In this thesis, we will consider the application of stochastic algorithms to\ntwo of the most important machine learning problems. Part i is concerned with\nthe supervised problem of binary classification using kernelized linear\nclassifiers, for which the data have labels belonging to exactly two classes\n(e.g. \"has cancer\" or \"doesn't have cancer\"), and the learning problem is to\nfind a linear classifier which is best at predicting the label. In Part ii, we\nwill consider the unsupervised problem of Principal Component Analysis, for\nwhich the learning task is to find the directions which contain most of the\nvariance of the data distribution.\n  Our goal is to present stochastic algorithms for both problems which are,\nabove all, practical--they work well on real-world data, in some cases better\nthan all known competing algorithms. A secondary, but still very important,\ngoal is to derive theoretical bounds on the performance of these algorithms\nwhich are at least competitive with, and often better than, those known for\nother approaches.",
    "published": "2013-08-15T20:59:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Knapsack Constrained Contextual Submodular List Prediction with\n  Application to Multi-document Summarization",
    "authors": [
      "Jiaji Zhou",
      "Stephane Ross",
      "Yisong Yue",
      "Debadeepta Dey",
      "J. Andrew Bagnell"
    ],
    "summary": "We study the problem of predicting a set or list of options under knapsack\nconstraint. The quality of such lists are evaluated by a submodular reward\nfunction that measures both quality and diversity. Similar to DAgger (Ross et\nal., 2010), by a reduction to online learning, we show how to adapt two\nsequence prediction models to imitate greedy maximization under knapsack\nconstraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013).\nExperiments on extractive multi-document summarization show that our approach\noutperforms existing state-of-the-art methods.",
    "published": "2013-08-16T03:46:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comment on \"robustness and regularization of support vector machines\" by\n  H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510,\n  2009, arXiv:0803.3490)",
    "authors": [
      "Yahya Forghani",
      "Hadi Sadoghi Yazdi"
    ],
    "summary": "This paper comments on the published work dealing with robustness and\nregularization of support vector machines (Journal of Machine Learning\nResearch, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They\nproposed a theorem to show that it is possible to relate robustness in the\nfeature space and robustness in the sample space directly. In this paper, we\npropose a counter example that rejects their theorem.",
    "published": "2013-08-17T03:56:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Sample-Complexity of General Reinforcement Learning",
    "authors": [
      "Tor Lattimore",
      "Marcus Hutter",
      "Peter Sunehag"
    ],
    "summary": "We present a new algorithm for general reinforcement learning where the true\nenvironment is known to belong to a finite class of N arbitrary models. The\nalgorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with\nhigh probability. Infinite classes are also considered where we show that\ncompactness is a key criterion for determining the existence of uniform\nsample-complexity bounds. A matching lower bound is given for the finite case.",
    "published": "2013-08-22T11:39:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Ensemble of Distributed Learners for Online Classification of Dynamic\n  Data Streams",
    "authors": [
      "Luca Canzian",
      "Yu Zhang",
      "Mihaela van der Schaar"
    ],
    "summary": "We present an efficient distributed online learning scheme to classify data\ncaptured from distributed, heterogeneous, and dynamic data sources. Our scheme\nconsists of multiple distributed local learners, that analyze different streams\nof data that are correlated to a common event that needs to be classified. Each\nlearner uses a local classifier to make a local prediction. The local\npredictions are then collected by each learner and combined using a weighted\nmajority rule to output the final prediction. We propose a novel online\nensemble learning algorithm to update the aggregation rule in order to adapt to\nthe underlying data dynamics. We rigorously determine a bound for the worst\ncase misclassification probability of our algorithm which depends on the\nmisclassification probabilities of the best static aggregation rule, and of the\nbest local classifier. Importantly, the worst case misclassification\nprobability of our algorithm tends asymptotically to 0 if the misclassification\nprobability of the best static aggregation rule or the misclassification\nprobability of the best local classifier tend to 0. Then we extend our\nalgorithm to address challenges specific to the distributed implementation and\nwe prove new bounds that apply to these settings. Finally, we test our scheme\nby performing an evaluation study on several data sets. When applied to data\nsets widely used by the literature dealing with dynamic data streams and\nconcept drift, our scheme exhibits performance gains ranging from 34% to 71%\nwith respect to state of the art solutions.",
    "published": "2013-08-24T02:33:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Prediction of breast cancer recurrence using Classification Restricted\n  Boltzmann Machine with Dropping",
    "authors": [
      "Jakub M. Tomczak"
    ],
    "summary": "In this paper, we apply Classification Restricted Boltzmann Machine\n(ClassRBM) to the problem of predicting breast cancer recurrence. According to\nthe Polish National Cancer Registry, in 2010 only, the breast cancer caused\nalmost 25% of all diagnosed cases of cancer in Poland. We propose how to use\nClassRBM for predicting breast cancer return and discovering relevant inputs\n(symptoms) in illness reappearance. Next, we outline a general probabilistic\nframework for learning Boltzmann machines with masks, which we refer to as\nDropping. The fashion of generating masks leads to different learning methods,\ni.e., DropOut, DropConnect. We propose a new method called DropPart which is a\ngeneralization of DropConnect. In DropPart the Beta distribution instead of\nBernoulli distribution in DropConnect is used. At the end, we carry out an\nexperiment using real-life dataset consisting of 949 cases, provided by the\nInstitute of Oncology Ljubljana.",
    "published": "2013-08-28T22:08:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Relative Comparison Kernel Learning with Auxiliary Kernels",
    "authors": [
      "Eric Heim",
      "Hamed Valizadegan",
      "Milos Hauskrecht"
    ],
    "summary": "In this work we consider the problem of learning a positive semidefinite\nkernel matrix from relative comparisons of the form: \"object A is more similar\nto object B than it is to C\", where comparisons are given by humans. Existing\nsolutions to this problem assume many comparisons are provided to learn a high\nquality kernel. However, this can be considered unrealistic for many real-world\ntasks since relative assessments require human input, which is often costly or\ndifficult to obtain. Because of this, only a limited number of these\ncomparisons may be provided. In this work, we explore methods for aiding the\nprocess of learning a kernel with the help of auxiliary kernels built from more\neasily extractable information regarding the relationships among objects. We\npropose a new kernel learning approach in which the target kernel is defined as\na conic combination of auxiliary kernels and a kernel whose elements are\nlearned directly. We formulate a convex optimization to solve for this target\nkernel that adds only minor overhead to methods that use no auxiliary\ninformation. Empirical results show that in the presence of few training\nrelative comparisons, our method can learn kernels that generalize to more\nout-of-sample comparisons than methods that do not utilize auxiliary\ninformation, as well as similar methods that learn metrics over objects.",
    "published": "2013-09-02T19:29:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Group Learning and Opinion Diffusion in a Broadcast Network",
    "authors": [
      "Yang Liu",
      "Mingyan Liu"
    ],
    "summary": "We analyze the following group learning problem in the context of opinion\ndiffusion: Consider a network with $M$ users, each facing $N$ options. In a\ndiscrete time setting, at each time step, each user chooses $K$ out of the $N$\noptions, and receive randomly generated rewards, whose statistics depend on the\noptions chosen as well as the user itself, and are unknown to the users. Each\nuser aims to maximize their expected total rewards over a certain time horizon\nthrough an online learning process, i.e., a sequence of exploration (sampling\nthe return of each option) and exploitation (selecting empirically good\noptions) steps.\n  Within this context we consider two group learning scenarios, (1) users with\nuniform preferences and (2) users with diverse preferences, and examine how a\nuser should construct its learning process to best extract information from\nother's decisions and experiences so as to maximize its own reward. Performance\nis measured in {\\em weak regret}, the difference between the user's total\nreward and the reward from a user-specific best single-action policy (i.e.,\nalways selecting the set of options generating the highest mean rewards for\nthis user). Within each scenario we also consider two cases: (i) when users\nexchange full information, meaning they share the actual rewards they obtained\nfrom their choices, and (ii) when users exchange limited information, e.g.,\nonly their choices but not rewards obtained from these choices.",
    "published": "2013-09-14T19:56:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Metric-learning based framework for Support Vector Machines and\n  Multiple Kernel Learning",
    "authors": [
      "Huyen Do",
      "Alexandros Kalousis"
    ],
    "summary": "Most metric learning algorithms, as well as Fisher's Discriminant Analysis\n(FDA), optimize some cost function of different measures of within-and\nbetween-class distances. On the other hand, Support Vector Machines(SVMs) and\nseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM large\nmargin theory. Recently, SVMs have been analyzed from SVM and metric learning,\nand to develop new algorithms that build on the strengths of each. Inspired by\nthe metric learning interpretation of SVM, we develop here a new\nmetric-learning based SVM framework in which we incorporate metric learning\nconcepts within SVM. We extend the optimization problem of SVM to include some\nmeasure of the within-class distance and along the way we develop a new\nwithin-class distance measure which is appropriate for SVM. In addition, we\nadopt the same approach for MKL and show that it can be also formulated as a\nMahalanobis metric learning problem. Our end result is a number of SVM/MKL\nalgorithms that incorporate metric learning concepts. We experiment with them\non a set of benchmark datasets and observe important predictive performance\nimprovements.",
    "published": "2013-09-16T09:39:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Bound Majorization",
    "authors": [
      "Anna Choromanska",
      "Tony Jebara"
    ],
    "summary": "Recently a majorization method for optimizing partition functions of\nlog-linear models was proposed alongside a novel quadratic variational\nupper-bound. In the batch setting, it outperformed state-of-the-art first- and\nsecond-order optimization methods on various learning tasks. We propose a\nstochastic version of this bound majorization method as well as a low-rank\nmodification for high-dimensional data-sets. The resulting stochastic\nsecond-order method outperforms stochastic gradient descent (across variations\nand various tunings) both in terms of the number of iterations and computation\ntime till convergence while finding a better quality parameter setting. The\nproposed method bridges first- and second-order stochastic optimization methods\nby maintaining a computational complexity that is linear in the data dimension\nand while exploiting second order information about the pseudo-global curvature\nof the objective function (as opposed to the local curvature in the Hessian).",
    "published": "2013-09-22T14:46:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Kernel Classification Framework for Metric Learning",
    "authors": [
      "Faqiang Wang",
      "Wangmeng Zuo",
      "Lei Zhang",
      "Deyu Meng",
      "David Zhang"
    ],
    "summary": "Learning a distance metric from the given training samples plays a crucial\nrole in many machine learning tasks, and various models and optimization\nalgorithms have been proposed in the past decade. In this paper, we generalize\nseveral state-of-the-art metric learning methods, such as large margin nearest\nneighbor (LMNN) and information theoretic metric learning (ITML), into a kernel\nclassification framework. First, doublets and triplets are constructed from the\ntraining samples, and a family of degree-2 polynomial kernel functions are\nproposed for pairs of doublets or triplets. Then, a kernel classification\nframework is established, which can not only generalize many popular metric\nlearning methods such as LMNN and ITML, but also suggest new metric learning\nmethods, which can be efficiently implemented, interestingly, by using the\nstandard support vector machine (SVM) solvers. Two novel metric learning\nmethods, namely doublet-SVM and triplet-SVM, are then developed under the\nproposed framework. Experimental results show that doublet-SVM and triplet-SVM\nachieve competitive classification accuracies with state-of-the-art metric\nlearning methods such as ITML and LMNN but with significantly less training\ntime.",
    "published": "2013-09-23T14:39:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fenchel Duals for Drifting Adversaries",
    "authors": [
      "Suman K Bera",
      "Anamitra R Choudhury",
      "Syamantak Das",
      "Sambuddha Roy",
      "Jayram S. Thatchachar"
    ],
    "summary": "We describe a primal-dual framework for the design and analysis of online\nconvex optimization algorithms for {\\em drifting regret}. Existing literature\nshows (nearly) optimal drifting regret bounds only for the $\\ell_2$ and the\n$\\ell_1$-norms. Our work provides a connection between these algorithms and the\nOnline Mirror Descent ($\\omd$) updates; one key insight that results from our\nwork is that in order for these algorithms to succeed, it suffices to have the\ngradient of the regularizer to be bounded (in an appropriate norm). For\nsituations (like for the $\\ell_1$ norm) where the vanilla regularizer does not\nhave this property, we have to {\\em shift} the regularizer to ensure this.\nThus, this helps explain the various updates presented in \\cite{bansal10,\nbuchbinder12}. We also consider the online variant of the problem with\n1-lookahead, and with movement costs in the $\\ell_2$-norm. Our primal dual\napproach yields nearly optimal competitive ratios for this problem.",
    "published": "2013-09-23T18:14:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori\n  Perturbations",
    "authors": [
      "Tamir Hazan",
      "Subhransu Maji",
      "Tommi Jaakkola"
    ],
    "summary": "In this paper we describe how MAP inference can be used to sample efficiently\nfrom Gibbs distributions. Specifically, we provide means for drawing either\napproximate or unbiased samples from Gibbs' distributions by introducing low\ndimensional perturbations and solving the corresponding MAP assignments. Our\napproach also leads to new ways to derive lower bounds on partition functions.\nWe demonstrate empirically that our method excels in the typical \"high signal -\nhigh coupling\" regime. The setting results in ragged energy landscapes that are\nchallenging for alternative approaches to sampling and/or lower bounds.",
    "published": "2013-09-29T13:48:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Extensive Experimental Study on the Cluster-based Reference Set\n  Reduction for speeding-up the k-NN Classifier",
    "authors": [
      "Stefanos Ougiaroglou",
      "Georgios Evangelidis",
      "Dimitris A. Dervos"
    ],
    "summary": "The k-Nearest Neighbor (k-NN) classification algorithm is one of the most\nwidely-used lazy classifiers because of its simplicity and ease of\nimplementation. It is considered to be an effective classifier and has many\napplications. However, its major drawback is that when sequential search is\nused to find the neighbors, it involves high computational cost. Speeding-up\nk-NN search is still an active research field. Hwang and Cho have recently\nproposed an adaptive cluster-based method for fast Nearest Neighbor searching.\nThe effectiveness of this method is based on the adjustment of three\nparameters. However, the authors evaluated their method by setting specific\nparameter values and using only one dataset. In this paper, an extensive\nexperimental study of this method is presented. The results, which are based on\nfive real life datasets, illustrate that if the parameters of the method are\ncarefully defined, one can achieve even better classification performance.",
    "published": "2013-09-30T08:24:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Feature Discovery for App Usage Prediction in Smartphones",
    "authors": [
      "Zhung-Xun Liao",
      "Shou-Chung Li",
      "Wen-Chih Peng",
      "Philip S Yu"
    ],
    "summary": "With the increasing number of mobile Apps developed, they are now closely\nintegrated into daily life. In this paper, we develop a framework to predict\nmobile Apps that are most likely to be used regarding the current device status\nof a smartphone. Such an Apps usage prediction framework is a crucial\nprerequisite for fast App launching, intelligent user experience, and power\nmanagement of smartphones. By analyzing real App usage log data, we discover\ntwo kinds of features: The Explicit Feature (EF) from sensing readings of\nbuilt-in sensors, and the Implicit Feature (IF) from App usage relations. The\nIF feature is derived by constructing the proposed App Usage Graph (abbreviated\nas AUG) that models App usage transitions. In light of AUG, we are able to\ndiscover usage relations among Apps. Since users may have different usage\nbehaviors on their smartphones, we further propose one personalized feature\nselection algorithm. We explore minimum description length (MDL) from the\ntraining data and select those features which need less length to describe the\ntraining data. The personalized feature selection can successfully reduce the\nlog size and the prediction time. Finally, we adopt the kNN classification\nmodel to predict Apps usage. Note that through the features selected by the\nproposed personalized feature selection algorithm, we only need to keep these\nfeatures, which in turn reduces the prediction time and avoids the curse of\ndimensionality when using the kNN classifier. We conduct a comprehensive\nexperimental study based on a real mobile App usage dataset. The results\ndemonstrate the effectiveness of the proposed framework and show the predictive\ncapability for App usage prediction.",
    "published": "2013-09-26T14:44:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering on Multiple Incomplete Datasets via Collective Kernel\n  Learning",
    "authors": [
      "Weixiang Shao",
      "Xiaoxiao Shi",
      "Philip S. Yu"
    ],
    "summary": "Multiple datasets containing different types of features may be available for\na given task. For instance, users' profiles can be used to group users for\nrecommendation systems. In addition, a model can also use users' historical\nbehaviors and credit history to group users. Each dataset contains different\ninformation and suffices for learning. A number of clustering algorithms on\nmultiple datasets were proposed during the past few years. These algorithms\nassume that at least one dataset is complete. So far as we know, all the\nprevious methods will not be applicable if there is no complete dataset\navailable. However, in reality, there are many situations where no dataset is\ncomplete. As in building a recommendation system, some new users may not have a\nprofile or historical behaviors, while some may not have a credit history.\nHence, no available dataset is complete. In order to solve this problem, we\npropose an approach called Collective Kernel Learning to infer hidden sample\nsimilarity from multiple incomplete datasets. The idea is to collectively\ncompletes the kernel matrices of incomplete datasets by optimizing the\nalignment of the shared instances of the datasets. Furthermore, a clustering\nalgorithm is proposed based on the kernel matrix. The experiments on both\nsynthetic and real datasets demonstrate the effectiveness of the proposed\napproach. The proposed clustering algorithm outperforms the comparison\nalgorithms by as much as two times in normalized mutual information.",
    "published": "2013-10-04T06:18:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Multi-Instance Multi-Label Learning",
    "authors": [
      "Sheng-Jun Huang",
      "Zhi-Hua Zhou"
    ],
    "summary": "In many real-world tasks, particularly those involving data objects with\ncomplicated semantics such as images and texts, one object can be represented\nby multiple instances and simultaneously be associated with multiple labels.\nSuch tasks can be formulated as multi-instance multi-label learning (MIML)\nproblems, and have been extensively studied during the past few years. Existing\nMIML approaches have been found useful in many applications; however, most of\nthem can only handle moderate-sized data. To efficiently handle large data\nsets, in this paper we propose the MIMLfast approach, which first constructs a\nlow-dimensional subspace shared by all labels, and then trains label specific\nlinear models to optimize approximated ranking loss via stochastic gradient\ndescent. Although the MIML problem is complicated, MIMLfast is able to achieve\nexcellent performance by exploiting label relations with shared space and\ndiscovering sub-concepts for complicated labels. Experiments show that the\nperformance of MIMLfast is highly competitive to state-of-the-art techniques,\nwhereas its time cost is much less; particularly, on a data set with 20K bags\nand 180K instances, MIMLfast is more than 100 times faster than existing MIML\napproaches. On a larger data set where none of existing approaches can return\nresults in 24 hours, MIMLfast takes only 12 minutes. Moreover, our approach is\nable to identify the most representative instance for each label, and thus\nproviding a chance to understand the relation between input patterns and output\nlabel semantics.",
    "published": "2013-10-08T09:03:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Localized Iterative Methods for Interpolation in Graph Structured Data",
    "authors": [
      "Sunil K. Narang",
      "Akshay Gadde",
      "Eduard Sanou",
      "Antonio Ortega"
    ],
    "summary": "In this paper, we present two localized graph filtering based methods for\ninterpolating graph signals defined on the vertices of arbitrary graphs from\nonly a partial set of samples. The first method is an extension of previous\nwork on reconstructing bandlimited graph signals from partially observed\nsamples. The iterative graph filtering approach very closely approximates the\nsolution proposed in the that work, while being computationally more efficient.\nAs an alternative, we propose a regularization based framework in which we\ndefine the cost of reconstruction to be a combination of smoothness of the\ngraph signal and the reconstruction error with respect to the known samples,\nand find solutions that minimize this cost. We provide both a closed form\nsolution and a computationally efficient iterative solution of the optimization\nproblem. The experimental results on the recommendation system datasets\ndemonstrate effectiveness of the proposed methods.",
    "published": "2013-10-09T22:24:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scaling Graph-based Semi Supervised Learning to Large Number of Labels\n  Using Count-Min Sketch",
    "authors": [
      "Partha Pratim Talukdar",
      "William Cohen"
    ],
    "summary": "Graph-based Semi-supervised learning (SSL) algorithms have been successfully\nused in a large number of applications. These methods classify initially\nunlabeled nodes by propagating label information over the structure of graph\nstarting from seed nodes. Graph-based SSL algorithms usually scale linearly\nwith the number of distinct labels (m), and require O(m) space on each node.\nUnfortunately, there exist many applications of practical significance with\nvery large m over large graphs, demanding better space and time complexity. In\nthis paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm which\ncompactly stores label distribution on each node using Count-min Sketch, a\nrandomized data structure. We present theoretical analysis showing that under\nmild conditions, MAD-SKETCH can reduce space complexity at each node from O(m)\nto O(log m), and achieve similar savings in time complexity as well. We support\nour analysis through experiments on multiple real world datasets. We observe\nthat MAD-SKETCH achieves similar performance as existing state-of-the-art\ngraph- based SSL algorithms, while requiring smaller memory footprint and at\nthe same time achieving up to 10x speedup. We find that MAD-SKETCH is able to\nscale to datasets with one million labels, which is beyond the scope of\nexisting graph- based SSL algorithms.",
    "published": "2013-10-10T20:30:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear\n  Spectral Penalties",
    "authors": [
      "Marco Signoretto",
      "Lieven De Lathauwer",
      "Johan A. K. Suykens"
    ],
    "summary": "We present a general framework to learn functions in tensor product\nreproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a\nnovel representer theorem suitable for existing as well as new spectral\npenalties for tensors. When the functions in the TP-RKHS are defined on the\nCartesian product of finite discrete sets, in particular, our main problem\nformulation admits as a special case existing tensor completion problems. Other\nspecial cases include transfer learning with multimodal side information and\nmultilinear multitask learning. For the latter case, our kernel-based view is\ninstrumental to derive nonlinear extensions of existing model classes. We give\na novel algorithm and show in experiments the usefulness of the proposed\nextensions.",
    "published": "2013-10-18T11:37:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Thompson Sampling in Dynamic Systems for Contextual Bandit Problems",
    "authors": [
      "Tianbing Xu",
      "Yaming Yu",
      "John Turner",
      "Amelia Regan"
    ],
    "summary": "We consider the multiarm bandit problems in the timevarying dynamic system\nfor rich structural features. For the nonlinear dynamic model, we propose the\napproximate inference for the posterior distributions based on Laplace\nApproximation. For the context bandit problems, Thompson Sampling is adopted\nbased on the underlying posterior distributions of the parameters. More\nspecifically, we introduce the discount decays on the previous samples impact\nand analyze the different decay rates with the underlying sample dynamics.\nConsequently, the exploration and exploitation is adaptively tradeoff according\nto the dynamics in the system.",
    "published": "2013-10-17T04:17:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph-Based Approaches to Clustering Network-Constrained Trajectory Data",
    "authors": [
      "Mohamed Khalil El Mahrsi",
      "Fabrice Rossi"
    ],
    "summary": "Clustering trajectory data attracted considerable attention in the last few\nyears. Most of prior work assumed that moving objects can move freely in an\neuclidean space and did not consider the eventual presence of an underlying\nroad network and its influence on evaluating the similarity between\ntrajectories. In this paper, we present an approach to clustering such\nnetwork-constrained trajectory data. More precisely we aim at discovering\ngroups of road segments that are often travelled by the same trajectories. To\nachieve this end, we model the interactions between segments w.r.t. their\nsimilarity as a weighted graph to which we apply a community detection\nalgorithm to discover meaningful clusters. We showcase our proposition through\nexperimental results obtained on synthetic datasets.",
    "published": "2013-10-19T17:24:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Task Regularization with Covariance Dictionary for Linear\n  Classifiers",
    "authors": [
      "Fanyi Xiao",
      "Ruikun Luo",
      "Zhiding Yu"
    ],
    "summary": "In this paper we propose a multi-task linear classifier learning problem\ncalled D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance\nshared by all tasks to do multi-task knowledge transfer among different tasks.\nWe formally define the learning problem of D-SVM and show two interpretations\nof this problem, from both the probabilistic and kernel perspectives. From the\nprobabilistic perspective, we show that our learning formulation is actually a\nMAP estimation on all optimization variables. We also show its equivalence to a\nmultiple kernel learning problem in which one is trying to find a re-weighting\nkernel for features from a dictionary of basis (despite the fact that only\nlinear classifiers are learned). Finally, we describe an alternative\noptimization scheme to minimize the objective function and present empirical\nstudies to valid our algorithm.",
    "published": "2013-10-21T01:06:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Theory and Algorithms for Revenue Optimization in Second-Price\n  Auctions with Reserve",
    "authors": [
      "Mehryar Mohri",
      "Andres Muñoz Medina"
    ],
    "summary": "Second-price auctions with reserve play a critical role for modern search\nengine and popular online sites since the revenue of these companies often\ndirectly de- pends on the outcome of such auctions. The choice of the reserve\nprice is the main mechanism through which the auction revenue can be influenced\nin these electronic markets. We cast the problem of selecting the reserve price\nto optimize revenue as a learning problem and present a full theoretical\nanalysis dealing with the complex properties of the corresponding loss\nfunction. We further give novel algorithms for solving this problem and report\nthe results of several experiments in both synthetic and real data\ndemonstrating their effectiveness.",
    "published": "2013-10-21T18:27:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Relative Deviation Learning Bounds and Generalization with Unbounded\n  Loss Functions",
    "authors": [
      "Corinna Cortes",
      "Spencer Greenberg",
      "Mehryar Mohri"
    ],
    "summary": "We present an extensive analysis of relative deviation bounds, including\ndetailed proofs of two-sided inequalities and their implications. We also give\ndetailed proofs of two-sided generalization bounds that hold in the general\ncase of unbounded loss functions, under the assumption that a moment of the\nloss is bounded. These bounds are useful in the analysis of importance\nweighting and other learning tasks such as unbounded regression.",
    "published": "2013-10-22T04:28:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Optimization for Sparse Gaussian Process Regression",
    "authors": [
      "Yanshuai Cao",
      "Marcus A. Brubaker",
      "David J. Fleet",
      "Aaron Hertzmann"
    ],
    "summary": "We propose an efficient optimization algorithm for selecting a subset of\ntraining data to induce sparsity for Gaussian process regression. The algorithm\nestimates an inducing set and the hyperparameters using a single objective,\neither the marginal likelihood or a variational free energy. The space and time\ncomplexity are linear in training set size, and the algorithm can be applied to\nlarge regression problems on discrete or continuous domains. Empirical\nevaluation shows state-of-art performance in discrete cases and competitive\nresults in the continuous case.",
    "published": "2013-10-22T18:44:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Combining Structured and Unstructured Randomness in Large Scale PCA",
    "authors": [
      "Nikos Karampatziakis",
      "Paul Mineiro"
    ],
    "summary": "Principal Component Analysis (PCA) is a ubiquitous tool with many\napplications in machine learning including feature construction, subspace\nembedding, and outlier detection. In this paper, we present an algorithm for\ncomputing the top principal components of a dataset with a large number of rows\n(examples) and columns (features). Our algorithm leverages both structured and\nunstructured random projections to retain good accuracy while being\ncomputationally efficient. We demonstrate the technique on the winning\nsubmission the KDD 2010 Cup.",
    "published": "2013-10-23T17:33:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Unsupervised Feature Learning Approach to Improve Automatic Incident\n  Detection",
    "authors": [
      "Jimmy SJ. Ren",
      "Wei Wang",
      "Jiawei Wang",
      "Stephen Liao"
    ],
    "summary": "Sophisticated automatic incident detection (AID) technology plays a key role\nin contemporary transportation systems. Though many papers were devoted to\nstudy incident classification algorithms, few study investigated how to enhance\nfeature representation of incidents to improve AID performance. In this paper,\nwe propose to use an unsupervised feature learning algorithm to generate higher\nlevel features to represent incidents. We used real incident data in the\nexperiments and found that effective feature mapping function can be learnt\nfrom the data crosses the test sites. With the enhanced features, detection\nrate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are\nsignificantly improved in all of the three representative cases. This approach\nalso provides an alternative way to reduce the amount of labeled data, which is\nexpensive to obtain, required in training better incident classifiers since the\nfeature learning is unsupervised.",
    "published": "2013-10-29T13:18:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An efficient distributed learning algorithm based on effective local\n  functional approximations",
    "authors": [
      "Dhruv Mahajan",
      "Nikunj Agrawal",
      "S. Sathiya Keerthi",
      "S. Sundararajan",
      "Leon Bottou"
    ],
    "summary": "Scalable machine learning over big data is an important problem that is\nreceiving a lot of attention in recent years. On popular distributed\nenvironments such as Hadoop running on a cluster of commodity machines,\ncommunication costs are substantial and algorithms need to be designed suitably\nconsidering those costs. In this paper we give a novel approach to the\ndistributed training of linear classifiers (involving smooth losses and L2\nregularization) that is designed to reduce the total communication costs. At\neach iteration, the nodes minimize locally formed approximate objective\nfunctions; then the resulting minimizers are combined to form a descent\ndirection to move. Our approach gives a lot of freedom in the formation of the\napproximate objective function as well as in the choice of methods to solve\nthem. The method is shown to have $O(log(1/\\epsilon))$ time convergence. The\nmethod can be viewed as an iterative parameter mixing method. A special\ninstantiation yields a parallel stochastic gradient descent method with strong\nconvergence. When communication times between nodes are large, our method is\nmuch faster than the Terascale method (Agarwal et al., 2011), which is a state\nof the art distributed solver based on the statistical query model (Chuet al.,\n2006) that computes function and gradient values in a distributed fashion. We\nalso evaluate against other recent distributed methods and demonstrate superior\nperformance of our method.",
    "published": "2013-10-31T08:00:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multilabel Classification through Random Graph Ensembles",
    "authors": [
      "Hongyu Su",
      "Juho Rousu"
    ],
    "summary": "We present new methods for multilabel classification, relying on ensemble\nlearning on a collection of random output graphs imposed on the multilabel and\na kernel-based structured output learner as the base classifier. For ensemble\nlearning, differences among the output graphs provide the required base\nclassifier diversity and lead to improved performance in the increasing size of\nthe ensemble. We study different methods of forming the ensemble prediction,\nincluding majority voting and two methods that perform inferences over the\ngraph structures before or after combining the base models into the ensemble.\nWe compare the methods against the state-of-the-art machine learning approaches\non a set of heterogeneous multilabel benchmark problems, including multilabel\nAdaBoost, convex multitask feature learning, as well as single target learning\napproaches represented by Bagging and SVM. In our experiments, the random graph\nensembles are very competitive and robust, ranking first or second on most of\nthe datasets. Overall, our results show that random graph ensembles are viable\nalternatives to flat multilabel and multitask learners.",
    "published": "2013-10-31T09:00:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Optimization of Smooth Loss",
    "authors": [
      "Rong Jin"
    ],
    "summary": "In this paper, we first prove a high probability bound rather than an\nexpectation bound for stochastic optimization with smooth loss. Furthermore,\nthe existing analysis requires the knowledge of optimal classifier for tuning\nthe step size in order to achieve the desired bound. However, this information\nis usually not accessible in advanced. We also propose a strategy to address\nthe limitation.",
    "published": "2013-11-30T01:07:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Practical Collapsed Stochastic Variational Inference for the HDP",
    "authors": [
      "Arnim Bleier"
    ],
    "summary": "Recent advances have made it feasible to apply the stochastic variational\nparadigm to a collapsed representation of latent Dirichlet allocation (LDA).\nWhile the stochastic variational paradigm has successfully been applied to an\nuncollapsed representation of the hierarchical Dirichlet process (HDP), no\nattempts to apply this type of inference in a collapsed setting of\nnon-parametric topic modeling have been put forward so far. In this paper we\nexplore such a collapsed stochastic variational Bayes inference for the HDP.\nThe proposed online algorithm is easy to implement and accounts for the\ninference of hyper-parameters. First experiments show a promising improvement\nin predictive performance.",
    "published": "2013-12-02T10:58:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sensing-Aware Kernel SVM",
    "authors": [
      "Weicong Ding",
      "Prakash Ishwar",
      "Venkatesh Saligrama",
      "W. Clem Karl"
    ],
    "summary": "We propose a novel approach for designing kernels for support vector machines\n(SVMs) when the class label is linked to the observation through a latent state\nand the likelihood function of the observation given the state (the sensing\nmodel) is available. We show that the Bayes-optimum decision boundary is a\nhyperplane under a mapping defined by the likelihood function. Combining this\nwith the maximum margin principle yields kernels for SVMs that leverage\nknowledge of the sensing model in an optimal way. We derive the optimum kernel\nfor the bag-of-words (BoWs) sensing model and demonstrate its superior\nperformance over other kernels in document and image classification tasks.\nThese results indicate that such optimum sensing-aware kernel SVMs can match\nthe performance of rather sophisticated state-of-the-art approaches.",
    "published": "2013-12-02T16:47:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "SpeedMachines: Anytime Structured Prediction",
    "authors": [
      "Alexander Grubb",
      "Daniel Munoz",
      "J. Andrew Bagnell",
      "Martial Hebert"
    ],
    "summary": "Structured prediction plays a central role in machine learning applications\nfrom computational biology to computer vision. These models require\nsignificantly more computation than unstructured models, and, in many\napplications, algorithms may need to make predictions within a computational\nbudget or in an anytime fashion. In this work we propose an anytime technique\nfor learning structured prediction that, at training time, incorporates both\nstructural elements and feature computation trade-offs that affect test-time\ninference. We apply our technique to the challenging problem of scene\nunderstanding in computer vision and demonstrate efficient and anytime\npredictions that gradually improve towards state-of-the-art classification\nperformance as the allotted time increases.",
    "published": "2013-12-02T20:26:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Image Representation Learning Using Graph Regularized Auto-Encoders",
    "authors": [
      "Yiyi Liao",
      "Yue Wang",
      "Yong Liu"
    ],
    "summary": "We consider the problem of image representation for the tasks of unsupervised\nlearning and semi-supervised learning. In those learning tasks, the raw image\nvectors may not provide enough representation for their intrinsic structures\ndue to their highly dense feature space. To overcome this problem, the raw\nimage vectors should be mapped to a proper representation space which can\ncapture the latent structure of the original data and represent the data\nexplicitly for further learning tasks such as clustering.\n  Inspired by the recent research works on deep neural network and\nrepresentation learning, in this paper, we introduce the multiple-layer\nauto-encoder into image representation, we also apply the locally invariant\nideal to our image representation with auto-encoders and propose a novel\nmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact\nrepresentation which uncovers the hidden semantics and simultaneously respects\nthe intrinsic geometric structure.\n  Extensive experiments on image clustering show encouraging results of the\nproposed algorithm in comparison to the state-of-the-art algorithms on\nreal-word cases.",
    "published": "2013-12-03T11:59:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Interpreting random forest classification models using a feature\n  contribution method",
    "authors": [
      "Anna Palczewska",
      "Jan Palczewski",
      "Richard Marchese Robinson",
      "Daniel Neagu"
    ],
    "summary": "Model interpretation is one of the key aspects of the model evaluation\nprocess. The explanation of the relationship between model variables and\noutputs is relatively easy for statistical models, such as linear regressions,\nthanks to the availability of model parameters and their statistical\nsignificance. For \"black box\" models, such as random forest, this information\nis hidden inside the model structure. This work presents an approach for\ncomputing feature contributions for random forest classification models. It\nallows for the determination of the influence of each variable on the model\nprediction for an individual instance. By analysing feature contributions for a\ntraining dataset, the most significant variables can be determined and their\ntypical contribution towards predictions made for individual classes, i.e.,\nclass-specific feature contribution \"patterns\", are discovered. These patterns\nrepresent a standard behaviour of the model and allow for an additional\nassessment of the model reliability for a new data. Interpretation of feature\ncontributions for two UCI benchmark datasets shows the potential of the\nproposed methodology. The robustness of results is demonstrated through an\nextensive analysis of feature contributions calculated for a large number of\ngenerated random forest models.",
    "published": "2013-12-04T11:57:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bandit Online Optimization Over the Permutahedron",
    "authors": [
      "Nir Ailon",
      "Kohei Hatano",
      "Eiji Takimoto"
    ],
    "summary": "The permutahedron is the convex polytope with vertex set consisting of the\nvectors $(\\pi(1),\\dots, \\pi(n))$ for all permutations (bijections) $\\pi$ over\n$\\{1,\\dots, n\\}$. We study a bandit game in which, at each step $t$, an\nadversary chooses a hidden weight weight vector $s_t$, a player chooses a\nvertex $\\pi_t$ of the permutahedron and suffers an observed loss of\n$\\sum_{i=1}^n \\pi(i) s_t(i)$.\n  A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a\nregret of $O(n\\sqrt{T \\log n})$ for a time horizon of $T$. Unfortunately,\nCombBand requires at each step an $n$-by-$n$ matrix permanent approximation to\nwithin improved accuracy as $T$ grows, resulting in a total running time that\nis super linear in $T$, making it impractical for large time horizons.\n  We provide an algorithm of regret $O(n^{3/2}\\sqrt{T})$ with total time\ncomplexity $O(n^3T)$. The ideas are a combination of CombBand and a recent\nalgorithm by Ailon (2013) for online optimization over the permutahedron in the\nfull information setting. The technical core is a bound on the variance of the\nPlackett-Luce noisy sorting process's \"pseudo loss\". The bound is obtained by\nestablishing positive semi-definiteness of a family of 3-by-3 matrices\ngenerated from rational functions of exponentials of 3 parameters.",
    "published": "2013-12-05T13:00:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Curriculum Learning for Handwritten Text Line Recognition",
    "authors": [
      "Jérôme Louradour",
      "Christopher Kermorvant"
    ],
    "summary": "Recurrent Neural Networks (RNN) have recently achieved the best performance\nin off-line Handwriting Text Recognition. At the same time, learning RNN by\ngradient descent leads to slow convergence, and training times are particularly\nlong when the training database consists of full lines of text. In this paper,\nwe propose an easy way to accelerate stochastic gradient descent in this\nset-up, and in the general context of learning to recognize sequences. The\nprinciple is called Curriculum Learning, or shaping. The idea is to first learn\nto recognize short sequences before training on all available training\nsequences. Experiments on three different handwritten text databases (Rimes,\nIAM, OpenHaRT) show that a simple implementation of this strategy can\nsignificantly speed up the training of RNN for Text Recognition, and even\nsignificantly improve performance in some cases.",
    "published": "2013-12-05T23:53:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Understanding Deep Architectures using a Recursive Convolutional Network",
    "authors": [
      "David Eigen",
      "Jason Rolfe",
      "Rob Fergus",
      "Yann LeCun"
    ],
    "summary": "A key challenge in designing convolutional network models is sizing them\nappropriately. Many factors are involved in these decisions, including number\nof layers, feature maps, kernel sizes, etc. Complicating this further is the\nfact that each of these influence not only the numbers and dimensions of the\nactivation units, but also the total number of parameters. In this paper we\nfocus on assessing the independent contributions of three of these linked\nvariables: The numbers of layers, feature maps, and parameters. To accomplish\nthis, we employ a recursive convolutional network whose weights are tied\nbetween layers; this allows us to vary each of the three factors in a\ncontrolled setting. We find that while increasing the numbers of layers and\nparameters each have clear benefit, the number of feature maps (and hence\ndimensionality of the representation) appears ancillary, and finds most of its\nbenefit through the introduction of more weights. Our results (i) empirically\nconfirm the notion that adding layers alone increases computational power,\nwithin the context of convolutional layers, and (ii) suggest that precise\nsizing of convolutional feature map dimensions is itself of little concern;\nmore attention should be paid to the number of parameters in these layers\ninstead.",
    "published": "2013-12-06T12:55:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "CEAI: CCM based Email Authorship Identification Model",
    "authors": [
      "Sarwat Nizamani",
      "Nasrullah Memon"
    ],
    "summary": "In this paper we present a model for email authorship identification (EAI) by\nemploying a Cluster-based Classification (CCM) technique. Traditionally,\nstylometric features have been successfully employed in various authorship\nanalysis tasks; we extend the traditional feature-set to include some more\ninteresting and effective features for email authorship identification (e.g.\nthe last punctuation mark used in an email, the tendency of an author to use\ncapitalization at the start of an email, or the punctuation after a greeting or\nfarewell). We also included Info Gain feature selection based content features.\nIt is observed that the use of such features in the authorship identification\nprocess has a positive impact on the accuracy of the authorship identification\ntask. We performed experiments to justify our arguments and compared the\nresults with other base line models. Experimental results reveal that the\nproposed CCM-based email authorship identification model, along with the\nproposed feature set, outperforms the state-of-the-art support vector machine\n(SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. The\nproposed model attains an accuracy rate of 94% for 10 authors, 89% for 25\nauthors, and 81% for 50 authors, respectively on Enron dataset, while 89.5%\naccuracy has been achieved on authors' constructed real email dataset. The\nresults on Enron dataset have been achieved on quite a large number of authors\nas compared to the models proposed by Iqbal et al. [1, 2].",
    "published": "2013-12-06T18:25:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Kernel-based Distance Metric Learning in the Output Space",
    "authors": [
      "Cong Li",
      "Michael Georgiopoulos",
      "Georgios C. Anagnostopoulos"
    ],
    "summary": "In this paper we present two related, kernel-based Distance Metric Learning\n(DML) methods. Their respective models non-linearly map data from their\noriginal space to an output space, and subsequent distance measurements are\nperformed in the output space via a Mahalanobis metric. The dimensionality of\nthe output space can be directly controlled to facilitate the learning of a\nlow-rank metric. Both methods allow for simultaneous inference of the\nassociated metric and the mapping to the output space, which can be used to\nvisualize the data, when the output space is 2- or 3-dimensional. Experimental\nresults for a collection of classification tasks illustrate the advantages of\nthe proposed methods over other traditional and kernel-based DML approaches.",
    "published": "2013-12-09T20:58:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Task Classification Hypothesis Space with Improved Generalization\n  Bounds",
    "authors": [
      "Cong Li",
      "Michael Georgiopoulos",
      "Georgios C. Anagnostopoulos"
    ],
    "summary": "This paper presents a RKHS, in general, of vector-valued functions intended\nto be used as hypothesis space for multi-task classification. It extends\nsimilar hypothesis spaces that have previously considered in the literature.\nAssuming this space, an improved Empirical Rademacher Complexity-based\ngeneralization bound is derived. The analysis is itself extended to an MKL\nsetting. The connection between the proposed hypothesis space and a Group-Lasso\ntype regularizer is discussed. Finally, experimental results, with some\nSVM-based Multi-Task Learning problems, underline the quality of the derived\nbounds and validate the paper's analysis.",
    "published": "2013-12-09T21:27:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Performance Analysis Of Regularized Linear Regression Models For\n  Oxazolines And Oxazoles Derivitive Descriptor Dataset",
    "authors": [
      "Doreswamy",
      "Chanabasayya . M. Vastrad"
    ],
    "summary": "Regularized regression techniques for linear regression have been created the\nlast few ten years to reduce the flaws of ordinary least squares regression\nwith regard to prediction accuracy. In this paper, new methods for using\nregularized regression in model choice are introduced, and we distinguish the\nconditions in which regularized regression develops our ability to discriminate\nmodels. We applied all the five methods that use penalty-based (regularization)\nshrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with\nfar more predictors than observations. The lasso, ridge, elasticnet, lars and\nrelaxed lasso further possess the desirable property that they simultaneously\nselect relevant predictive descriptors and optimally estimate their effects.\nHere, we comparatively evaluate the performance of five regularized linear\nregression methods The assessment of the performance of each model by means of\nbenchmark experiments is an established exercise. Cross-validation and\nresampling methods are generally used to arrive point evaluates the\nefficiencies which are compared to recognize methods with acceptable features.\nPredictive accuracy was evaluated using the root mean squared error (RMSE) and\nSquare of usual correlation between predictors and observed mean inhibitory\nconcentration of antitubercular activity (R square). We found that all five\nregularized regression models were able to produce feasible models and\nefficient capturing the linearity in the data. The elastic net and lars had\nsimilar accuracies as well as lasso and relaxed lasso had similar accuracies\nbut outperformed ridge regression in terms of the RMSE and R square metrics.",
    "published": "2013-12-10T13:16:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Active Player Modelling",
    "authors": [
      "Julian Togelius",
      "Noor Shaker",
      "Georgios N. Yannakakis"
    ],
    "summary": "We argue for the use of active learning methods for player modelling. In\nactive learning, the learning algorithm chooses where to sample the search\nspace so as to optimise learning progress. We hypothesise that player modelling\nbased on active learning could result in vastly more efficient learning, but\nwill require big changes in how data is collected. Some example active player\nmodelling scenarios are described. A particular form of active learning is also\nequivalent to an influential formalisation of (human and machine) curiosity,\nand games with active learning could therefore be seen as being curious about\nthe player. We further hypothesise that this form of curiosity is symmetric,\nand therefore that games that explore their players based on the principles of\nactive learning will turn out to select game configurations that are\ninteresting to the player that is being explored.",
    "published": "2013-12-10T20:36:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Bayesian Passive-Aggressive Learning",
    "authors": [
      "Tianlin Shi",
      "Jun Zhu"
    ],
    "summary": "Online Passive-Aggressive (PA) learning is an effective framework for\nperforming max-margin online learning. But the deterministic formulation and\nestimated single large-margin model could limit its capability in discovering\ndescriptive structures underlying complex data. This pa- per presents online\nBayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA\nand extends naturally to incorporate latent variables and perform nonparametric\nBayesian inference, thus providing great flexibility for explorative analysis.\nWe apply BayesPA to topic modeling and derive efficient online learning\nalgorithms for max-margin topic models. We further develop nonparametric\nmethods to resolve the number of topics. Experimental results on real datasets\nshow that our approaches significantly improve time efficiency while\nmaintaining comparable results with the batch counterparts.",
    "published": "2013-12-12T02:46:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem",
    "authors": [
      "Masrour Zoghi",
      "Shimon Whiteson",
      "Remi Munos",
      "Maarten de Rijke"
    ],
    "summary": "This paper proposes a new method for the K-armed dueling bandit problem, a\nvariation on the regular K-armed bandit problem that offers only relative\nfeedback about pairs of arms. Our approach extends the Upper Confidence Bound\nalgorithm to the relative setting by using estimates of the pairwise\nprobabilities to select a promising arm and applying Upper Confidence Bound\nwith the winner as a benchmark. We prove a finite-time regret bound of order\nO(log t). In addition, our empirical results using real data from an\ninformation retrieval application show that it greatly outperforms the state of\nthe art.",
    "published": "2013-12-12T03:08:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Baseline-free Sampling in Parameter Exploring Policy\n  Gradients: Super Symmetric PGPE",
    "authors": [
      "Frank Sehnke"
    ],
    "summary": "Policy Gradient methods that explore directly in parameter space are among\nthe most effective and robust direct policy search methods and have drawn a lot\nof attention lately. The basic method from this field, Policy Gradients with\nParameter-based Exploration, uses two samples that are symmetric around the\ncurrent hypothesis to circumvent misleading reward in \\emph{asymmetrical}\nreward distributed problems gathered with the usual baseline approach. The\nexploration parameters are still updated by a baseline approach - leaving the\nexploration prone to asymmetric reward distributions. In this paper we will\nshow how the exploration parameters can be sampled quasi symmetric despite\nhaving limited instead of free parameters for exploration. We give a\ntransformation approximation to get quasi symmetric samples with respect to the\nexploration without changing the overall sampling distribution. Finally we will\ndemonstrate that sampling symmetrically also for the exploration parameters is\nsuperior in needs of samples and robustness than the original sampling\napproach.",
    "published": "2013-12-13T14:10:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Feature Graph Architectures",
    "authors": [
      "Richard Davis",
      "Sanjay Chawla",
      "Philip Leong"
    ],
    "summary": "In this article we propose feature graph architectures (FGA), which are deep\nlearning systems employing a structured initialisation and training method\nbased on a feature graph which facilitates improved generalisation performance\ncompared with a standard shallow architecture. The goal is to explore\nalternative perspectives on the problem of deep network training. We evaluate\nFGA performance for deep SVMs on some experimental datasets, and show how\ngeneralisation and stability results may be derived for these models. We\ndescribe the effect of permutations on the model accuracy, and give a criterion\nfor the optimal permutation in terms of feature correlations. The experimental\nresults show that the algorithm produces robust and significant test set\nimprovements over a standard shallow SVM training method for a range of\ndatasets. These gains are achieved with a moderate increase in time complexity.",
    "published": "2013-12-15T23:40:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Factored Representations in a Deep Mixture of Experts",
    "authors": [
      "David Eigen",
      "Marc'Aurelio Ranzato",
      "Ilya Sutskever"
    ],
    "summary": "Mixtures of Experts combine the outputs of several \"expert\" networks, each of\nwhich specializes in a different part of the input space. This is achieved by\ntraining a \"gating\" network that maps each input to a distribution over the\nexperts. Such models show promise for building larger networks that are still\ncheap to compute at test time, and more parallelizable at training time. In\nthis this work, we extend the Mixture of Experts to a stacked model, the Deep\nMixture of Experts, with multiple sets of gating and experts. This\nexponentially increases the number of effective experts by associating each\ninput with a combination of experts at each layer, yet maintains a modest model\nsize. On a randomly translated version of the MNIST dataset, we find that the\nDeep Mixture of Experts automatically learns to develop location-dependent\n(\"where\") experts at the first layer, and class-specific (\"what\") experts at\nthe second layer. In addition, we see that the different combinations are in\nuse when the model is applied to a dataset of speech monophones. These\ndemonstrate effective use of all expert combinations.",
    "published": "2013-12-16T11:15:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Deep Representations By Distributed Random Samplings",
    "authors": [
      "Xiao-Lei Zhang"
    ],
    "summary": "In this paper, we propose an extremely simple deep model for the unsupervised\nnonlinear dimensionality reduction -- deep distributed random samplings, which\nperforms like a stack of unsupervised bootstrap aggregating. First, its network\nstructure is novel: each layer of the network is a group of mutually\nindependent $k$-centers clusterings. Second, its learning method is extremely\nsimple: the $k$ centers of each clustering are only $k$ randomly selected\nexamples from the training data; for small-scale data sets, the $k$ centers are\nfurther randomly reconstructed by a simple cyclic-shift operation. Experimental\nresults on nonlinear dimensionality reduction show that the proposed method can\nlearn abstract representations on both large-scale and small-scale problems,\nand meanwhile is much faster than deep neural networks on large-scale problems.",
    "published": "2013-12-16T15:40:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep\n  Neural Networks",
    "authors": [
      "Andrew Davis",
      "Itamar Arel"
    ],
    "summary": "Scalability properties of deep neural networks raise key research questions,\nparticularly as the problems considered become larger and more challenging.\nThis paper expands on the idea of conditional computation introduced by Bengio,\net. al., where the nodes of a deep network are augmented by a set of gating\nunits that determine when a node should be calculated. By factorizing the\nweight matrix into a low-rank approximation, an estimation of the sign of the\npre-nonlinearity activation can be efficiently obtained. For networks using\nrectified-linear hidden units, this implies that the computation of a hidden\nunit with an estimated negative pre-nonlinearity can be ommitted altogether, as\nits value will become zero when nonlinearity is applied. For sparse neural\nnetworks, this can result in considerable speed gains. Experimental results\nusing the MNIST and SVHN data sets with a fully-connected deep neural network\ndemonstrate the performance robustness of the proposed scheme with respect to\nthe error introduced by the conditional computation process.",
    "published": "2013-12-16T18:58:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evolution and Computational Learning Theory: A survey on Valiant's paper",
    "authors": [
      "Arka Bhattacharya"
    ],
    "summary": "Darwin's theory of evolution is considered to be one of the greatest\nscientific gems in modern science. It not only gives us a description of how\nliving things evolve, but also shows how a population evolves through time and\nalso, why only the fittest individuals continue the generation forward. The\npaper basically gives a high level analysis of the works of Valiant[1]. Though,\nwe know the mechanisms of evolution, but it seems that there does not exist any\nstrong quantitative and mathematical theory of the evolution of certain\nmechanisms. What is defined exactly as the fitness of an individual, why is\nthat only certain individuals in a population tend to mutate, how computation\nis done in finite time when we have exponentially many examples: there seems to\nbe a lot of questions which need to be answered. [1] basically treats Darwinian\ntheory as a form of computational learning theory, which calculates the net\nfitness of the hypotheses and thus distinguishes functions and their classes\nwhich could be evolvable using polynomial amount of resources. Evolution is\nconsidered as a function of the environment and the previous evolutionary\nstages that chooses the best hypothesis using learning techniques that makes\nmutation possible and hence, gives a quantitative idea that why only the\nfittest individuals tend to survive and have the power to mutate.",
    "published": "2013-12-17T00:32:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Comparative Evaluation of Curriculum Learning with Filtering and\n  Boosting",
    "authors": [
      "Michael R. Smith",
      "Tony Martinez"
    ],
    "summary": "Not all instances in a data set are equally beneficial for inferring a model\nof the data. Some instances (such as outliers) are detrimental to inferring a\nmodel of the data. Several machine learning techniques treat instances in a\ndata set differently during training such as curriculum learning, filtering,\nand boosting. However, an automated method for determining how beneficial an\ninstance is for inferring a model of the data does not exist. In this paper, we\npresent an automated method that orders the instances in a data set by\ncomplexity based on the their likelihood of being misclassified (instance\nhardness). The underlying assumption of this method is that instances with a\nhigh likelihood of being misclassified represent more complex concepts in a\ndata set. Ordering the instances in a data set allows a learning algorithm to\nfocus on the most beneficial instances and ignore the detrimental ones. We\ncompare ordering the instances in a data set in curriculum learning, filtering\nand boosting. We find that ordering the instances significantly increases\nclassification accuracy and that filtering has the largest impact on\nclassification accuracy. On a set of 52 data sets, ordering the instances\nincreases the average accuracy from 81% to 84%.",
    "published": "2013-12-17T22:12:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Online Bootstrapping for Large Scale Learning",
    "authors": [
      "Zhen Qin",
      "Vaclav Petricek",
      "Nikos Karampatziakis",
      "Lihong Li",
      "John Langford"
    ],
    "summary": "Bootstrapping is a useful technique for estimating the uncertainty of a\npredictor, for example, confidence intervals for prediction. It is typically\nused on small to moderate sized datasets, due to its high computation cost.\nThis work describes a highly scalable online bootstrapping strategy,\nimplemented inside Vowpal Wabbit, that is several times faster than traditional\nstrategies. Our experiments indicate that, in addition to providing a black\nbox-like method for estimating uncertainty, our implementation of online\nbootstrapping may also help to train models with better prediction performance\ndue to model averaging.",
    "published": "2013-12-18T02:10:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks",
    "authors": [
      "Jinseok Nam",
      "Jungi Kim",
      "Eneldo Loza Mencía",
      "Iryna Gurevych",
      "Johannes Fürnkranz"
    ],
    "summary": "Neural networks have recently been proposed for multi-label classification\nbecause they are able to capture and model label dependencies in the output\nlayer. In this work, we investigate limitations of BP-MLL, a neural network\n(NN) architecture that aims at minimizing pairwise ranking error. Instead, we\npropose to use a comparably simple NN approach with recently proposed learning\ntechniques for large-scale multi-label text classification tasks. In\nparticular, we show that BP-MLL's ranking loss minimization can be efficiently\nand effectively replaced with the commonly used cross entropy error function,\nand demonstrate that several advances in neural network training that have been\ndeveloped in the realm of deep learning can be effectively employed in this\nsetting. Our experimental results show that simple NN models equipped with\nadvanced techniques such as rectified linear units, dropout, and AdaGrad\nperform as well as or even outperform state-of-the-art approaches on six\nlarge-scale textual datasets with diverse characteristics.",
    "published": "2013-12-19T06:53:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Playing Atari with Deep Reinforcement Learning",
    "authors": [
      "Volodymyr Mnih",
      "Koray Kavukcuoglu",
      "David Silver",
      "Alex Graves",
      "Ioannis Antonoglou",
      "Daan Wierstra",
      "Martin Riedmiller"
    ],
    "summary": "We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.",
    "published": "2013-12-19T16:00:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings",
    "authors": [
      "Mohammad Norouzi",
      "Tomas Mikolov",
      "Samy Bengio",
      "Yoram Singer",
      "Jonathon Shlens",
      "Andrea Frome",
      "Greg S. Corrado",
      "Jeffrey Dean"
    ],
    "summary": "Several recent publications have proposed methods for mapping images into\ncontinuous semantic embedding spaces. In some cases the embedding space is\ntrained jointly with the image transformation. In other cases the semantic\nembedding space is established by an independent natural language processing\ntask, and then the image transformation into that space is learned in a second\nstage. Proponents of these image embedding systems have stressed their\nadvantages over the traditional \\nway{} classification framing of image\nunderstanding, particularly in terms of the promise for zero-shot learning --\nthe ability to correctly annotate images of previously unseen object\ncategories. In this paper, we propose a simple method for constructing an image\nembedding system from any existing \\nway{} image classifier and a semantic word\nembedding model, which contains the $\\n$ class labels in its vocabulary. Our\nmethod maps images into the semantic embedding space via convex combination of\nthe class label embedding vectors, and requires no additional training. We show\nthat this simple and direct method confers many of the advantages associated\nwith more complex image embedding schemes, and indeed outperforms state of the\nart methods on the ImageNet zero-shot learning task.",
    "published": "2013-12-19T17:30:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "k-Sparse Autoencoders",
    "authors": [
      "Alireza Makhzani",
      "Brendan Frey"
    ],
    "summary": "Recently, it has been observed that when representations are learnt in a way\nthat encourages sparsity, improved performance is obtained on classification\ntasks. These methods involve combinations of activation functions, sampling\nsteps and different kinds of penalties. To investigate the effectiveness of\nsparsity by itself, we propose the k-sparse autoencoder, which is an\nautoencoder with linear activation function, where in hidden layers only the k\nhighest activities are kept. When applied to the MNIST and NORB datasets, we\nfind that this method achieves better classification results than denoising\nautoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders\nare simple to train and the encoding stage is very fast, making them\nwell-suited to large problem sizes, where conventional sparse coding algorithms\ncannot be applied.",
    "published": "2013-12-19T17:46:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Principled Non-Linear Feature Selection",
    "authors": [
      "Dimitrios Athanasakis",
      "John Shawe-Taylor",
      "Delmiro Fernandez-Reyes"
    ],
    "summary": "Recent non-linear feature selection approaches employing greedy optimisation\nof Centred Kernel Target Alignment(KTA) exhibit strong results in terms of\ngeneralisation accuracy and sparsity. However, they are computationally\nprohibitive for large datasets. We propose randSel, a randomised feature\nselection algorithm, with attractive scaling properties. Our theoretical\nanalysis of randSel provides strong probabilistic guarantees for correct\nidentification of relevant features. RandSel's characteristics make it an ideal\ncandidate for identifying informative learned representations. We've conducted\nexperimentation to establish the performance of this approach, and present\nencouraging results, including a 3rd position result in the recent ICML black\nbox learning challenge as well as competitive results for signal peptide\nprediction, an important problem in bioinformatics.",
    "published": "2013-12-20T10:16:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Seeding for Gaussian Mixture Models",
    "authors": [
      "Johannes Blömer",
      "Kathrin Bujna"
    ],
    "summary": "We present new initialization methods for the expectation-maximization\nalgorithm for multivariate Gaussian mixture models. Our methods are adaptions\nof the well-known $K$-means++ initialization and the Gonzalez algorithm.\nThereby we aim to close the gap between simple random, e.g. uniform, and\ncomplex methods, that crucially depend on the right choice of hyperparameters.\nOur extensive experiments indicate the usefulness of our methods compared to\ncommon techniques and methods, which e.g. apply the original $K$-means++ and\nGonzalez directly, with respect to artificial as well as real-world data sets.",
    "published": "2013-12-20T14:08:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning States Representations in POMDP",
    "authors": [
      "Gabriella Contardo",
      "Ludovic Denoyer",
      "Thierry Artieres",
      "Patrick Gallinari"
    ],
    "summary": "We propose to deal with sequential processes where only partial observations\nare available by learning a latent representation space on which policies may\nbe accurately learned.",
    "published": "2013-12-20T17:03:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unit Tests for Stochastic Optimization",
    "authors": [
      "Tom Schaul",
      "Ioannis Antonoglou",
      "David Silver"
    ],
    "summary": "Optimization by stochastic gradient descent is an important component of many\nlarge-scale machine learning algorithms. A wide variety of such optimization\nalgorithms have been devised; however, it is unclear whether these algorithms\nare robust and widely applicable across many different optimization landscapes.\nIn this paper we develop a collection of unit tests for stochastic\noptimization. Each unit test rapidly evaluates an optimization algorithm on a\nsmall-scale, isolated, and well-understood difficulty, rather than in\nreal-world scenarios where many such issues are entangled. Passing these unit\ntests is not sufficient, but absolutely necessary for any algorithms with\nclaims to generality or robustness. We give initial quantitative and\nqualitative results on numerous established algorithms. The testing framework\nis open-source, extensible, and easy to apply to new algorithms.",
    "published": "2013-12-20T17:44:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stopping Criteria in Contrastive Divergence: Alternatives to the\n  Reconstruction Error",
    "authors": [
      "David Buchaca",
      "Enrique Romero",
      "Ferran Mazzanti",
      "Jordi Delgado"
    ],
    "summary": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning\ndevices to ascertain generative models of data distributions. RBMs are often\ntrained using the Contrastive Divergence learning algorithm (CD), an\napproximation to the gradient of the data log-likelihood. A simple\nreconstruction error is often used to decide whether the approximation provided\nby the CD algorithm is good enough, though several authors (Schulz et al.,\n2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of\nthis procedure. However, not many alternatives to the reconstruction error have\nbeen used in the literature. In this manuscript we investigate simple\nalternatives to the reconstruction error in order to detect as soon as possible\nthe decrease in the log-likelihood during learning.",
    "published": "2013-12-20T18:14:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The return of AdaBoost.MH: multi-class Hamming trees",
    "authors": [
      "Balázs Kégl"
    ],
    "summary": "Within the framework of AdaBoost.MH, we propose to train vector-valued\ndecision trees to optimize the multi-class edge without reducing the\nmulti-class problem to $K$ binary one-against-all classifications. The key\nelement of the method is a vector-valued decision stump, factorized into an\ninput-independent vector of length $K$ and label-independent scalar classifier.\nAt inner tree nodes, the label-dependent vector is discarded and the binary\nclassifier can be used for partitioning the input space into two regions. The\nalgorithm retains the conceptual elegance, power, and computational efficiency\nof binary AdaBoost. In experiments it is on par with support vector machines\nand with the best existing multi-class boosting algorithm AOSOLogitBoost, and\nit is significantly better than other known implementations of AdaBoost.MH.",
    "published": "2013-12-20T19:33:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comparison three methods of clustering: k-means, spectral clustering and\n  hierarchical clustering",
    "authors": [
      "Kamran Kowsari"
    ],
    "summary": "Comparison of three kind of the clustering and find cost function and loss\nfunction and calculate them. Error rate of the clustering methods and how to\ncalculate the error percentage always be one on the important factor for\nevaluating the clustering methods, so this paper introduce one way to calculate\nthe error rate of clustering methods. Clustering algorithms can be divided into\nseveral categories including partitioning clustering algorithms, hierarchical\nalgorithms and density based algorithms. Generally speaking we should compare\nclustering algorithms by Scalability, Ability to work with different attribute,\nClusters formed by conventional, Having minimal knowledge of the computer to\nrecognize the input parameters, Classes for dealing with noise and extra\ndeposition that same error rate for clustering a new data, Thus, there is no\neffect on the input data, different dimensions of high levels, K-means is one\nof the simplest approach to clustering that clustering is an unsupervised\nproblem.",
    "published": "2013-12-19T21:45:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning",
    "authors": [
      "Son N. Tran",
      "Artur d'Avila Garcez"
    ],
    "summary": "Transfer Learning is concerned with the application of knowledge gained from\nsolving a problem to a different but related problem domain. In this paper, we\npropose a method and efficient algorithm for ranking and selecting\nrepresentations from a Restricted Boltzmann Machine trained on a source domain\nto be transferred onto a target domain. Experiments carried out using the\nMNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature\nranking and transfer learning method offers statistically significant\nimprovements on the training of RBMs. Our method is general in that the\nknowledge chosen by the ranking function does not depend on its relation to any\nspecific target domain, and it works with unsupervised learning and\nknowledge-based transfer.",
    "published": "2013-12-21T01:50:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral\n  Learning",
    "authors": [
      "François Denis",
      "Mattias Gybels",
      "Amaury Habrard"
    ],
    "summary": "Learning probabilistic models over strings is an important issue for many\napplications. Spectral methods propose elegant solutions to the problem of\ninferring weighted automata from finite samples of variable-length strings\ndrawn from an unknown target distribution. These methods rely on a singular\nvalue decomposition of a matrix $H_S$, called the Hankel matrix, that records\nthe frequencies of (some of) the observed strings. The accuracy of the learned\ndistribution depends both on the quantity of information embedded in $H_S$ and\non the distance between $H_S$ and its mean $H_r$. Existing concentration bounds\nseem to indicate that the concentration over $H_r$ gets looser with the size of\n$H_r$, suggesting to make a trade-off between the quantity of used information\nand the size of $H_r$. We propose new dimension-free concentration bounds for\nseveral variants of Hankel matrices. Experiments demonstrate that these bounds\nare tight and that they significantly improve existing bounds. These results\nsuggest that the concentration rate of the Hankel matrix around its mean does\nnot constitute an argument for limiting its size.",
    "published": "2013-12-21T18:10:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Invariant Factorization Of Time-Series",
    "authors": [
      "Josif Grabocka",
      "Lars Schmidt-Thieme"
    ],
    "summary": "Time-series classification is an important domain of machine learning and a\nplethora of methods have been developed for the task. In comparison to existing\napproaches, this study presents a novel method which decomposes a time-series\ndataset into latent patterns and membership weights of local segments to those\npatterns. The process is formalized as a constrained objective function and a\ntailored stochastic coordinate descent optimization is applied. The time-series\nare projected to a new feature representation consisting of the sums of the\nmembership weights, which captures frequencies of local patterns. Features from\nvarious sliding window sizes are concatenated in order to encapsulate the\ninteraction of patterns from different sizes. Finally, a large-scale\nexperimental comparison against 6 state of the art baselines and 43 real life\ndatasets is conducted. The proposed method outperforms all the baselines with\nstatistically significant margins in terms of prediction accuracy.",
    "published": "2013-12-23T22:15:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Iterative Nearest Neighborhood Oversampling in Semisupervised Learning\n  from Imbalanced Data",
    "authors": [
      "Fengqi Li",
      "Chuang Yu",
      "Nanhai Yang",
      "Feng Xia",
      "Guangming Li",
      "Fatemeh Kaveh-Yazdy"
    ],
    "summary": "Transductive graph-based semi-supervised learning methods usually build an\nundirected graph utilizing both labeled and unlabeled samples as vertices.\nThose methods propagate label information of labeled samples to neighbors\nthrough their edges in order to get the predicted labels of unlabeled samples.\nMost popular semi-supervised learning approaches are sensitive to initial label\ndistribution happened in imbalanced labeled datasets. The class boundary will\nbe severely skewed by the majority classes in an imbalanced classification. In\nthis paper, we proposed a simple and effective approach to alleviate the\nunfavorable influence of imbalance problem by iteratively selecting a few\nunlabeled samples and adding them into the minority classes to form a balanced\nlabeled dataset for the learning methods afterwards. The experiments on UCI\ndatasets and MNIST handwritten digits dataset showed that the proposed approach\noutperforms other existing state-of-art methods.",
    "published": "2013-12-24T12:24:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Rate-Distortion Auto-Encoders",
    "authors": [
      "Luis G. Sanchez Giraldo",
      "Jose C. Principe"
    ],
    "summary": "A rekindled the interest in auto-encoder algorithms has been spurred by\nrecent work on deep learning. Current efforts have been directed towards\neffective training of auto-encoder architectures with a large number of coding\nunits. Here, we propose a learning algorithm for auto-encoders based on a\nrate-distortion objective that minimizes the mutual information between the\ninputs and the outputs of the auto-encoder subject to a fidelity constraint.\nThe goal is to learn a representation that is minimally committed to the input\ndata, but that is rich enough to reconstruct the inputs up to certain level of\ndistortion. Minimizing the mutual information acts as a regularization term\nwhereas the fidelity constraint can be understood as a risk functional in the\nconventional statistical learning setting. The proposed algorithm uses a\nrecently introduced measure of entropy based on infinitely divisible matrices\nthat avoids the plug in estimation of densities. Experiments using\nover-complete bases show that the rate-distortion auto-encoders can learn a\nregularized input-output mapping in an implicit manner.",
    "published": "2013-12-28T02:08:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Approximating the Bethe partition function",
    "authors": [
      "Adrian Weller",
      "Tony Jebara"
    ],
    "summary": "When belief propagation (BP) converges, it does so to a stationary point of\nthe Bethe free energy $F$, and is often strikingly accurate. However, it may\nconverge only to a local optimum or may not converge at all. An algorithm was\nrecently introduced for attractive binary pairwise MRFs which is guaranteed to\nreturn an $\\epsilon$-approximation to the global minimum of $F$ in polynomial\ntime provided the maximum degree $\\Delta=O(\\log n)$, where $n$ is the number of\nvariables. Here we significantly improve this algorithm and derive several\nresults including a new approach based on analyzing first derivatives of $F$,\nwhich leads to performance that is typically far superior and yields a fully\npolynomial-time approximation scheme (FPTAS) for attractive models without any\ndegree restriction. Further, the method applies to general (non-attractive)\nmodels, though with no polynomial time guarantee in this case, leading to the\nimportant result that approximating $\\log$ of the Bethe partition function,\n$\\log Z_B=-\\min F$, for a general model to additive $\\epsilon$-accuracy may be\nreduced to a discrete MAP inference problem. We explore an application to\npredicting equipment failure on an urban power network and demonstrate that the\nBethe approximation can perform well even when BP fails to converge.",
    "published": "2013-12-30T22:40:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Controlled Sparsity Kernel Learning",
    "authors": [
      "Dinesh Govindaraj",
      "Raman Sankaran",
      "Sreedal Menon",
      "Chiranjib Bhattacharyya"
    ],
    "summary": "Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a\npopular front of research in recent times due to its success in application\nproblems like Object Categorization. This success is due to the fact that MKL\nhas the ability to choose from a variety of feature kernels to identify the\noptimal kernel combination. But the initial formulation of MKL was only able to\nselect the best of the features and misses out many other informative kernels\npresented. To overcome this, the Lp norm based formulation was proposed by\nKloft et. al. This formulation is capable of choosing a non-sparse set of\nkernels through a control parameter p. Unfortunately, the parameter p does not\nhave a direct meaning to the number of kernels selected. We have observed that\nstricter control over the number of kernels selected gives us an edge over\nthese techniques in terms of accuracy of classification and also helps us to\nfine tune the algorithms to the time requirements at hand. In this work, we\npropose a Controlled Sparsity Kernel Learning (CSKL) formulation that can\nstrictly control the number of kernels which we wish to select. The CSKL\nformulation introduces a parameter t which directly corresponds to the number\nof kernels selected. It is important to note that a search in t space is finite\nand fast as compared to p. We have also provided an efficient Reduced Gradient\nDescent based algorithm to solve the CSKL formulation, which is proven to\nconverge. Through our experiments on the Caltech101 Object Categorization\ndataset, we have also shown that one can achieve better accuracies than the\nprevious formulations through the right choice of t.",
    "published": "2013-12-31T09:13:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "EigenGP: Gaussian Process Models with Adaptive Eigenfunctions",
    "authors": [
      "Hao Peng",
      "Yuan Qi"
    ],
    "summary": "Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost for big\ndata. In this paper, we propose a new Bayesian approach, EigenGP, that learns\nboth basis dictionary elements--eigenfunctions of a GP prior--and prior\nprecisions in a sparse finite model. It is well known that, among all\northogonal basis functions, eigenfunctions can provide the most compact\nrepresentation. Unlike other sparse Bayesian finite models where the basis\nfunction has a fixed form, our eigenfunctions live in a reproducing kernel\nHilbert space as a finite linear combination of kernel functions. We learn the\ndictionary elements--eigenfunctions--and the prior precisions over these\nelements as well as all the other hyperparameters from data by maximizing the\nmodel marginal likelihood. We explore computational linear algebra to simplify\nthe gradient computation significantly. Our experimental results demonstrate\nimproved predictive performance of EigenGP over alternative sparse GP methods\nas well as relevance vector machine.",
    "published": "2014-01-02T03:12:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits",
    "authors": [
      "Nicolas Galichet",
      "Michèle Sebag",
      "Olivier Teytaud"
    ],
    "summary": "Motivated by applications in energy management, this paper presents the\nMulti-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting the\nexploration of risky arms, MARAB takes as arm quality its conditional value at\nrisk. When the user-supplied risk level goes to 0, the arm quality tends toward\nthe essential infimum of the arm distribution density, and MARAB tends toward\nthe MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal\nvalue. As a first contribution, this paper presents a theoretical analysis of\nthe MIN algorithm under mild assumptions, establishing its robustness\ncomparatively to UCB. The analysis is supported by extensive experimental\nvalidation of MIN and MARAB compared to UCB and state-of-art risk-aware MAB\nalgorithms on artificial and real-world problems.",
    "published": "2014-01-06T15:53:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation",
    "authors": [
      "Elad Liebman",
      "Maytal Saar-Tsechansky",
      "Peter Stone"
    ],
    "summary": "In recent years, there has been growing focus on the study of automated\nrecommender systems. Music recommendation systems serve as a prominent domain\nfor such works, both from an academic and a commercial perspective. A\nfundamental aspect of music perception is that music is experienced in temporal\ncontext and in sequence. In this work we present DJ-MC, a novel\nreinforcement-learning framework for music recommendation that does not\nrecommend songs individually but rather song sequences, or playlists, based on\na model of preferences for both songs and song transitions. The model is\nlearned online and is uniquely adapted for each listener. To reduce exploration\ntime, DJ-MC exploits user feedback to initialize a model, which it subsequently\nupdates by reinforcement. We evaluate our framework with human participants\nusing both real song and playlist data. Our results indicate that DJ-MC's\nability to recommend sequences of songs provides a significant improvement over\nmore straightforward approaches, which do not take transitions into account.",
    "published": "2014-01-09T01:50:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering, Coding, and the Concept of Similarity",
    "authors": [
      "L. Thorne McCarty"
    ],
    "summary": "This paper develops a theory of clustering and coding which combines a\ngeometric model with a probabilistic model in a principled way. The geometric\nmodel is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\\bf x})$,\nwhich we interpret as a measure of dissimilarity. The probabilistic model\nconsists of a stochastic process with an invariant probability measure which\nmatches the density of the sample input data. The link between the two models\nis a potential function, $U({\\bf x})$, and its gradient, $\\nabla U({\\bf x})$.\nWe use the gradient to define the dissimilarity metric, which guarantees that\nour measure of dissimilarity will depend on the probability measure. Finally,\nwe use the dissimilarity metric to define a coordinate system on the embedded\nRiemannian manifold, which gives us a low-dimensional encoding of our original\ndata.",
    "published": "2014-01-10T17:36:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Latent Tree Models and Approximate Inference in Bayesian Networks",
    "authors": [
      "Yi Wang",
      "Nevin L. Zhang",
      "Tao Chen"
    ],
    "summary": "We propose a novel method for approximate inference in Bayesian networks\n(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)\nfrom the data offline, and when online, make inference with the LTM instead of\nthe original BN. Because LTMs are tree-structured, inference takes linear time.\nIn the meantime, they can represent complex relationship among leaf nodes and\nhence the approximation accuracy is often good. Empirical evidence shows that\nour method can achieve good approximation accuracy at low online computational\ncost.",
    "published": "2014-01-15T04:46:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach",
    "authors": [
      "Balázs Csanád Csáji",
      "László Monostori"
    ],
    "summary": "The paper investigates stochastic resource allocation problems with scarce,\nreusable resources and non-preemtive, time-dependent, interconnected tasks.\nThis approach is a natural generalization of several standard resource\nmanagement problems, such as scheduling and transportation problems. First,\nreactive solutions are considered and defined as control policies of suitably\nreformulated Markov decision processes (MDPs). We argue that this reformulation\nhas several favorable properties, such as it has finite state and action\nspaces, it is aperiodic, hence all policies are proper and the space of control\npolicies can be safely restricted. Next, approximate dynamic programming (ADP)\nmethods, such as fitted Q-learning, are suggested for computing an efficient\ncontrol policy. In order to compactly maintain the cost-to-go function, two\nrepresentations are studied: hash tables and support vector regression (SVR),\nparticularly, nu-SVRs. Several additional improvements, such as the application\nof limited-lookahead rollout algorithms in the initial phases, action space\ndecomposition, task clustering and distributed sampling are investigated, too.\nFinally, experimental results on both benchmark and industry-related data are\npresented.",
    "published": "2014-01-15T04:50:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based\n  Approach",
    "authors": [
      "Saher Esmeir",
      "Shaul Markovitch"
    ],
    "summary": "Machine learning techniques are gaining prevalence in the production of a\nwide range of classifiers for complex real-world applications with nonuniform\ntesting and misclassification costs. The increasing complexity of these\napplications poses a real challenge to resource management during learning and\nclassification. In this work we introduce ACT (anytime cost-sensitive tree\nlearner), a novel framework for operating in such complex environments. ACT is\nan anytime algorithm that allows learning time to be increased in return for\nlower classification costs. It builds a tree top-down and exploits additional\ntime resources to obtain better estimations for the utility of the different\ncandidate splits. Using sampling techniques, ACT approximates the cost of the\nsubtree under each candidate split and favors the one with a minimal cost. As a\nstochastic algorithm, ACT is expected to be able to escape local minima, into\nwhich greedy methods may be trapped. Experiments with a variety of datasets\nwere conducted to compare ACT to the state-of-the-art cost-sensitive tree\nlearners. The results show that for the majority of domains ACT produces\nsignificantly less costly trees. ACT also exhibits good anytime behavior with\ndiminishing returns.",
    "published": "2014-01-15T05:09:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Regression Conformal Prediction with Nearest Neighbours",
    "authors": [
      "Harris Papadopoulos",
      "Vladimir Vovk",
      "Alex Gammerman"
    ],
    "summary": "In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours\nRegression (k-NNR) algorithm and propose ways of extending the typical\nnonconformity measure used for regression so far. Unlike traditional regression\nmethods which produce point predictions, Conformal Predictors output predictive\nregions that satisfy a given confidence level. The regions produced by any\nConformal Predictor are automatically valid, however their tightness and\ntherefore usefulness depends on the nonconformity measure used by each CP. In\neffect a nonconformity measure evaluates how strange a given example is\ncompared to a set of other examples based on some traditional machine learning\nalgorithm. We define six novel nonconformity measures based on the k-Nearest\nNeighbours Regression algorithm and develop the corresponding CPs following\nboth the original (transductive) and the inductive CP approaches. A comparison\nof the predictive regions produced by our measures with those of the typical\nregression measure suggests that a major improvement in terms of predictive\nregion tightness is achieved by the new measures.",
    "published": "2014-01-16T05:12:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Convex Optimization for Binary Classifier Aggregation in Multiclass\n  Problems",
    "authors": [
      "Sunho Park",
      "TaeHyun Hwang",
      "Seungjin Choi"
    ],
    "summary": "Multiclass problems are often decomposed into multiple binary problems that\nare solved by individual binary classifiers whose results are integrated into a\nfinal answer. Various methods, including all-pairs (APs), one-versus-all (OVA),\nand error correcting output code (ECOC), have been studied, to decompose\nmulticlass problems into binary problems. However, little study has been made\nto optimally aggregate binary problems to determine a final answer to the\nmulticlass problem. In this paper we present a convex optimization method for\nan optimal aggregation of binary classifiers to estimate class membership\nprobabilities in multiclass problems. We model the class membership probability\nas a softmax function which takes a conic combination of discrepancies induced\nby individual binary classifiers, as an input. With this model, we formulate\nthe regularized maximum likelihood estimation as a convex optimization problem,\nwhich is solved by the primal-dual interior point method. Connections of our\nmethod to large margin classifiers are presented, showing that the large margin\nformulation can be considered as a limiting case of our convex formulation.\nNumerical experiments on synthetic and real-world data sets demonstrate that\nour method outperforms existing aggregation methods as well as direct methods,\nin terms of the classification accuracy and the quality of class membership\nprobability estimates.",
    "published": "2014-01-16T19:49:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Unifying Framework for Typical Multi-Task Multiple Kernel Learning\n  Problems",
    "authors": [
      "Cong Li",
      "Michael Georgiopoulos",
      "Georgios C. Anagnostopoulos"
    ],
    "summary": "Over the past few years, Multi-Kernel Learning (MKL) has received significant\nattention among data-driven feature selection techniques in the context of\nkernel-based learning. MKL formulations have been devised and solved for a\nbroad spectrum of machine learning problems, including Multi-Task Learning\n(MTL). Solving different MKL formulations usually involves designing algorithms\nthat are tailored to the problem at hand, which is, typically, a non-trivial\naccomplishment.\n  In this paper we present a general Multi-Task Multi-Kernel Learning\n(Multi-Task MKL) framework that subsumes well-known Multi-Task MKL\nformulations, as well as several important MKL approaches on single-task\nproblems. We then derive a simple algorithm that can solve the unifying\nframework. To demonstrate the flexibility of the proposed framework, we\nformulate a new learning problem, namely Partially-Shared Common Space (PSCS)\nMulti-Task MKL, and demonstrate its merits through experimentation.",
    "published": "2014-01-21T01:16:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II)",
    "authors": [
      "Shaobo Lin",
      "Xia Liu",
      "Jian Fang",
      "Zongben Xu"
    ],
    "summary": "An extreme learning machine (ELM) can be regarded as a two stage feed-forward\nneural network (FNN) learning system which randomly assigns the connections\nwith and within hidden neurons in the first stage and tunes the connections\nwith output neurons in the second stage. Therefore, ELM training is essentially\na linear learning problem, which significantly reduces the computational\nburden. Numerous applications show that such a computation burden reduction\ndoes not degrade the generalization capability. It has, however, been open that\nwhether this is true in theory. The aim of our work is to study the theoretical\nfeasibility of ELM by analyzing the pros and cons of ELM. In the previous part\non this topic, we pointed out that via appropriate selection of the activation\nfunction, ELM does not degrade the generalization capability in the expectation\nsense. In this paper, we launch the study in a different direction and show\nthat the randomness of ELM also leads to certain negative consequences. On one\nhand, we find that the randomness causes an additional uncertainty problem of\nELM, both in approximation and learning. On the other hand, we theoretically\njustify that there also exists an activation function such that the\ncorresponding ELM degrades the generalization capability. In particular, we\nprove that the generalization capability of ELM with Gaussian kernel is\nessentially worse than that of FNN with Gaussian kernel. To facilitate the use\nof ELM, we also provide a remedy to such a degradation. We find that the\nwell-developed coefficient regularization technique can essentially improve the\ngeneralization capability. The obtained results reveal the essential\ncharacteristic of ELM and give theoretical guidance concerning how to use ELM.",
    "published": "2014-01-24T01:57:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Steady-state performance of non-negative least-mean-square algorithm and\n  its variants",
    "authors": [
      "Jie Chen",
      "José Carlos M. Bermudez",
      "Cédric Richard"
    ],
    "summary": "Non-negative least-mean-square (NNLMS) algorithm and its variants have been\nproposed for online estimation under non-negativity constraints. The transient\nbehavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMS\nalgorithms have been studied in our previous work. In this technical report, we\nderive closed-form expressions for the steady-state excess mean-square error\n(EMSE) for the four algorithms. Simulations results illustrate the accuracy of\nthe theoretical results. This is a complementary material to our previous work.",
    "published": "2014-01-24T15:36:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Riffled Independence for Efficient Inference with Partial Rankings",
    "authors": [
      "Jonathan Huang",
      "Ashish Kapoor",
      "Carlos Guestrin"
    ],
    "summary": "Distributions over rankings are used to model data in a multitude of real\nworld settings such as preference analysis and political elections. Modeling\nsuch distributions presents several computational challenges, however, due to\nthe factorial size of the set of rankings over an item set. Some of these\nchallenges are quite familiar to the artificial intelligence community, such as\nhow to compactly represent a distribution over a combinatorially large space,\nand how to efficiently perform probabilistic inference with these\nrepresentations. With respect to ranking, however, there is the additional\nchallenge of what we refer to as human task complexity users are rarely willing\nto provide a full ranking over a long list of candidates, instead often\npreferring to provide partial ranking information. Simultaneously addressing\nall of these challenges i.e., designing a compactly representable model which\nis amenable to efficient inference and can be learned using partial ranking\ndata is a difficult task, but is necessary if we would like to scale to\nproblems with nontrivial size. In this paper, we show that the recently\nproposed riffled independence assumptions cleanly and efficiently address each\nof the above challenges. In particular, we establish a tight mathematical\nconnection between the concepts of riffled independence and of partial\nrankings. This correspondence not only allows us to then develop efficient and\nexact algorithms for performing inference tasks using riffled independence\nbased represen- tations with partial rankings, but somewhat surprisingly, also\nshows that efficient inference is not possible for riffle independent models\n(in a certain sense) with observations which do not take the form of partial\nrankings. Finally, using our inference algorithm, we introduce the first method\nfor learning riffled independence based models from partially ranked data.",
    "published": "2014-01-23T02:42:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Toward Supervised Anomaly Detection",
    "authors": [
      "Nico Goernitz",
      "Marius Micha Kloft",
      "Konrad Rieck",
      "Ulf Brefeld"
    ],
    "summary": "Anomaly detection is being regarded as an unsupervised learning task as\nanomalies stem from adversarial or unlikely events with unknown distributions.\nHowever, the predictive performance of purely unsupervised anomaly detection\noften fails to match the required detection rates in many tasks and there\nexists a need for labeled data to guide the model generation. Our first\ncontribution shows that classical semi-supervised approaches, originating from\na supervised classifier, are inappropriate and hardly detect new and unknown\nanomalies. We argue that semi-supervised anomaly detection needs to ground on\nthe unsupervised learning paradigm and devise a novel algorithm that meets this\nrequirement. Although being intrinsically non-convex, we further show that the\noptimization problem has a convex equivalent under relatively mild assumptions.\nAdditionally, we propose an active learning strategy to automatically filter\ncandidates for labeling. In an empirical study on network intrusion detection\ndata, we observe that the proposed learning methodology requires much less\nlabeled data than the state-of-the-art, while achieving higher detection\naccuracies.",
    "published": "2014-01-23T02:46:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Lower Bound for the Variance of Estimators for Nakagami m Distribution",
    "authors": [
      "Rangeet Mitra",
      "Amit Kumar Mishra",
      "Tarun Choubisa"
    ],
    "summary": "Recently, we have proposed a maximum likelihood iterative algorithm for\nestimation of the parameters of the Nakagami-m distribution. This technique\nperforms better than state of art estimation techniques for this distribution.\nThis could be of particular use in low data or block based estimation problems.\nIn these scenarios, the estimator should be able to give accurate estimates in\nthe mean square sense with less amounts of data. Also, the estimates should\nimprove with the increase in number of blocks received. In this paper, we see\nthrough our simulations, that our proposal is well designed for such\nrequirements. Further, it is well known in the literature that an efficient\nestimator does not exist for Nakagami-m distribution. In this paper, we derive\na theoretical expression for the variance of our proposed estimator. We find\nthat this expression clearly fits the experimental curve for the variance of\nthe proposed estimator. This expression is pretty close to the cramer-rao lower\nbound(CRLB).",
    "published": "2014-02-03T18:20:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Feature Subset Selection Algorithm Automatic Recommendation Method",
    "authors": [
      "Guangtao Wang",
      "Qinbao Song",
      "Heli Sun",
      "Xueying Zhang",
      "Baowen Xu",
      "Yuming Zhou"
    ],
    "summary": "Many feature subset selection (FSS) algorithms have been proposed, but not\nall of them are appropriate for a given feature selection problem. At the same\ntime, so far there is rarely a good way to choose appropriate FSS algorithms\nfor the problem at hand. Thus, FSS algorithm automatic recommendation is very\nimportant and practically useful. In this paper, a meta learning based FSS\nalgorithm automatic recommendation method is presented. The proposed method\nfirst identifies the data sets that are most similar to the one at hand by the\nk-nearest neighbor classification algorithm, and the distances among these data\nsets are calculated based on the commonly-used data set characteristics. Then,\nit ranks all the candidate FSS algorithms according to their performance on\nthese similar data sets, and chooses the algorithms with best performance as\nthe appropriate ones. The performance of the candidate FSS algorithms is\nevaluated by a multi-criteria metric that takes into account not only the\nclassification accuracy over the selected features, but also the runtime of\nfeature selection and the number of selected features. The proposed\nrecommendation method is extensively tested on 115 real world data sets with 22\nwell-known and frequently-used different FSS algorithms for five representative\nclassifiers. The results show the effectiveness of our proposed FSS algorithm\nrecommendation method.",
    "published": "2014-02-04T01:37:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey on Latent Tree Models and Applications",
    "authors": [
      "Raphaël Mourad",
      "Christine Sinoquet",
      "Nevin L. Zhang",
      "Tengfei Liu",
      "Philippe Leray"
    ],
    "summary": "In data analysis, latent variables play a central role because they help\nprovide powerful insights into a wide variety of phenomena, ranging from\nbiological to human sciences. The latent tree model, a particular type of\nprobabilistic graphical models, deserves attention. Its simple structure - a\ntree - allows simple and efficient inference, while its latent variables\ncapture complex relationships. In the past decade, the latent tree model has\nbeen subject to significant theoretical and methodological developments. In\nthis review, we propose a comprehensive study of this model. First we summarize\nkey ideas underlying the model. Second we explain how it can be efficiently\nlearned from data. Third we illustrate its use within three types of\napplications: latent structure discovery, multidimensional clustering, and\nprobabilistic inference. Finally, we conclude and give promising directions for\nfuture researches in this field.",
    "published": "2014-02-04T01:40:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Near-Optimally Teaching the Crowd to Classify",
    "authors": [
      "Adish Singla",
      "Ilija Bogunovic",
      "Gábor Bartók",
      "Amin Karbasi",
      "Andreas Krause"
    ],
    "summary": "How should we present training examples to learners to teach them\nclassification rules? This is a natural problem when training workers for\ncrowdsourcing labeling tasks, and is also motivated by challenges in\ndata-driven online education. We propose a natural stochastic model of the\nlearners, modeling them as randomly switching among hypotheses based on\nobserved feedback. We then develop STRICT, an efficient algorithm for selecting\nexamples to teach to workers. Our solution greedily maximizes a submodular\nsurrogate objective function in order to select examples to show to the\nlearners. We prove that our strategy is competitive with the optimal teaching\npolicy. Moreover, for the special case of linear separators, we prove that an\nexponential reduction in error probability can be achieved. Our experiments on\nsimulated workers as well as three real image annotation tasks on Amazon\nMechanical Turk show the effectiveness of our teaching algorithm.",
    "published": "2014-02-10T10:36:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Indian Buffet Process Deep Generative Models for Semi-Supervised\n  Classification",
    "authors": [
      "Sotirios P. Chatzis"
    ],
    "summary": "Deep generative models (DGMs) have brought about a major breakthrough, as\nwell as renewed interest, in generative latent variable models. However, DGMs\ndo not allow for performing data-driven inference of the number of latent\nfeatures needed to represent the observed data. Traditional linear formulations\naddress this issue by resorting to tools from the field of nonparametric\nstatistics. Indeed, linear latent variable models imposed an Indian Buffet\nProcess (IBP) prior have been extensively studied by the machine learning\ncommunity; inference for such models can been performed either via exact\nsampling or via approximate variational techniques. Based on this inspiration,\nin this paper we examine whether similar ideas from the field of Bayesian\nnonparametrics can be utilized in the context of modern DGMs in order to\naddress the latent variable dimensionality inference problem. To this end, we\npropose a novel DGM formulation, based on the imposition of an IBP prior. We\ndevise an efficient Black-Box Variational inference algorithm for our model,\nand exhibit its efficacy in a number of semi-supervised classification\nexperiments. In all cases, we use popular benchmark datasets, and compare to\nstate-of-the-art DGMs.",
    "published": "2014-02-14T10:44:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sparse Polynomial Learning and Graph Sketching",
    "authors": [
      "Murat Kocaoglu",
      "Karthikeyan Shanmugam",
      "Alexandros G. Dimakis",
      "Adam Klivans"
    ],
    "summary": "Let $f:\\{-1,1\\}^n$ be a polynomial with at most $s$ non-zero real\ncoefficients. We give an algorithm for exactly reconstructing f given random\nexamples from the uniform distribution on $\\{-1,1\\}^n$ that runs in time\npolynomial in $n$ and $2s$ and succeeds if the function satisfies the unique\nsign property: there is one output value which corresponds to a unique set of\nvalues of the participating parities. This sufficient condition is satisfied\nwhen every coefficient of f is perturbed by a small random noise, or satisfied\nwith high probability when s parity functions are chosen randomly or when all\nthe coefficients are positive. Learning sparse polynomials over the Boolean\ndomain in time polynomial in $n$ and $2s$ is considered notoriously hard in the\nworst-case. Our result shows that the problem is tractable for almost all\nsparse polynomials. Then, we show an application of this result to hypergraph\nsketching which is the problem of learning a sparse (both in the number of\nhyperedges and the size of the hyperedges) hypergraph from uniformly drawn\nrandom cuts. We also provide experimental results on a real world dataset.",
    "published": "2014-02-17T06:00:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Selective Sampling with Drift",
    "authors": [
      "Edward Moroshko",
      "Koby Crammer"
    ],
    "summary": "Recently there has been much work on selective sampling, an online active\nlearning setting, in which algorithms work in rounds. On each round an\nalgorithm receives an input and makes a prediction. Then, it can decide whether\nto query a label, and if so to update its model, otherwise the input is\ndiscarded. Most of this work is focused on the stationary case, where it is\nassumed that there is a fixed target model, and the performance of the\nalgorithm is compared to a fixed model. However, in many real-world\napplications, such as spam prediction, the best target function may drift over\ntime, or have shifts from time to time. We develop a novel selective sampling\nalgorithm for the drifting setting, analyze it under no assumptions on the\nmechanism generating the sequence of instances, and derive new mistake bounds\nthat depend on the amount of drift in the problem. Simulations on synthetic and\nreal-world datasets demonstrate the superiority of our algorithms as a\nselective sampling algorithm in the drifting setting.",
    "published": "2014-02-17T17:53:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the properties of $α$-unchaining single linkage hierarchical\n  clustering",
    "authors": [
      "A. Martínez-Pérez"
    ],
    "summary": "In the election of a hierarchical clustering method, theoretic properties may\ngive some insight to determine which method is the most suitable to treat a\nclustering problem. Herein, we study some basic properties of two hierarchical\nclustering methods: $\\alpha$-unchaining single linkage or $SL(\\alpha)$ and a\nmodified version of this one, $SL^*(\\alpha)$. We compare the results with the\nproperties satisfied by the classical linkage-based hierarchical clustering\nmethods.",
    "published": "2014-02-18T13:08:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning the Irreducible Representations of Commutative Lie Groups",
    "authors": [
      "Taco Cohen",
      "Max Welling"
    ],
    "summary": "We present a new probabilistic model of compact commutative Lie groups that\nproduces invariant-equivariant and disentangled representations of data. To\ndefine the notion of disentangling, we borrow a fundamental principle from\nphysics that is used to derive the elementary particles of a system from its\nsymmetries. Our model employs a newfound Bayesian conjugacy relation that\nenables fully tractable probabilistic inference over compact commutative Lie\ngroups -- a class that includes the groups that describe the rotation and\ncyclic translation of images. We train the model on pairs of transformed image\npatches, and show that the learned invariant representation is highly effective\nfor classification.",
    "published": "2014-02-18T18:47:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey on Semi-Supervised Learning Techniques",
    "authors": [
      "V. Jothi Prakash",
      "Dr. L. M. Nithya"
    ],
    "summary": "Semisupervised learning is a learning standard which deals with the study of\nhow computers and natural systems such as human beings acquire knowledge in the\npresence of both labeled and unlabeled data. Semisupervised learning based\nmethods are preferred when compared to the supervised and unsupervised learning\nbecause of the improved performance shown by the semisupervised approaches in\nthe presence of large volumes of data. Labels are very hard to attain while\nunlabeled data are surplus, therefore semisupervised learning is a noble\nindication to shrink human labor and improve accuracy. There has been a large\nspectrum of ideas on semisupervised learning. In this paper we bring out some\nof the key approaches for semisupervised learning.",
    "published": "2014-02-19T12:40:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Quasi-Newton Method for Large Scale Support Vector Machines",
    "authors": [
      "Aryan Mokhtari",
      "Alejandro Ribeiro"
    ],
    "summary": "This paper adapts a recently developed regularized stochastic version of the\nBroyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for the\nsolution of support vector machine classification problems. The proposed method\nis shown to converge almost surely to the optimal classifier at a rate that is\nlinear in expectation. Numerical results show that the proposed method exhibits\na convergence rate that degrades smoothly with the dimensionality of the\nfeature vectors.",
    "published": "2014-02-20T01:44:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "To go deep or wide in learning?",
    "authors": [
      "Gaurav Pandey",
      "Ambedkar Dukkipati"
    ],
    "summary": "To achieve acceptable performance for AI tasks, one can either use\nsophisticated feature extraction methods as the first layer in a two-layered\nsupervised learning model, or learn the features directly using a deep\n(multi-layered) model. While the first approach is very problem-specific, the\nsecond approach has computational overheads in learning multiple layers and\nfine-tuning of the model. In this paper, we propose an approach called wide\nlearning based on arc-cosine kernels, that learns a single layer of infinite\nwidth. We propose exact and inexact learning strategies for wide learning and\nshow that wide learning with single layer outperforms single layer as well as\ndeep architectures of finite width for some benchmark datasets.",
    "published": "2014-02-23T16:51:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bandits with concave rewards and convex knapsacks",
    "authors": [
      "Shipra Agrawal",
      "Nikhil R. Devanur"
    ],
    "summary": "In this paper, we consider a very general model for exploration-exploitation\ntradeoff which allows arbitrary concave rewards and convex constraints on the\ndecisions across time, in addition to the customary limitation on the time\nhorizon. This model subsumes the classic multi-armed bandit (MAB) model, and\nthe Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also\nconsider an extension of this model to allow linear contexts, similar to the\nlinear contextual extension of the MAB model. We demonstrate that a natural and\nsimple extension of the UCB family of algorithms for MAB provides a polynomial\ntime algorithm that has near-optimal regret guarantees for this substantially\nmore general model, and matches the bounds provided by Badanidiyuru et\nal.[2013] for the special case of BwK, which is quite surprising. We also\nprovide computationally more efficient algorithms by establishing interesting\nconnections between this problem and other well studied problems/algorithms\nsuch as the Blackwell approachability problem, online convex optimization, and\nthe Frank-Wolfe technique for convex optimization. We give examples of several\nconcrete applications, where this more general model of bandits allows for\nricher and/or more efficient formulations of the problem.",
    "published": "2014-02-24T09:27:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Renewable Energy Prediction using Weather Forecasts for Optimal\n  Scheduling in HPC Systems",
    "authors": [
      "Ankur Sahai"
    ],
    "summary": "The objective of the GreenPAD project is to use green energy (wind, solar and\nbiomass) for powering data-centers that are used to run HPC jobs. As a part of\nthis it is important to predict the Renewable (Wind) energy for efficient\nscheduling (executing jobs that require higher energy when there is more green\nenergy available and vice-versa). For predicting the wind energy we first\nanalyze the historical data to find a statistical model that gives relation\nbetween wind energy and weather attributes. Then we use this model based on the\nweather forecast data to predict the green energy availability in the future.\nUsing the green energy prediction obtained from the statistical model we are\nable to precompute job schedules for maximizing the green energy utilization in\nthe future. We propose a model which uses live weather data in addition to\nmachine learning techniques (which can predict future deviations in weather\nconditions based on current deviations from the forecast) to make on-the-fly\nchanges to the precomputed schedule (based on green energy prediction).\n  For this we first analyze the data using histograms and simple statistical\ntools such as correlation. In addition we build (correlation) regression model\nfor finding the relation between wind energy availability and weather\nattributes (temperature, cloud cover, air pressure, wind speed / direction,\nprecipitation and sunshine). We also analyze different algorithms and machine\nlearning techniques for optimizing the job schedules for maximizing the green\nenergy utilization.",
    "published": "2014-02-26T14:29:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Marginalizing Corrupted Features",
    "authors": [
      "Laurens van der Maaten",
      "Minmin Chen",
      "Stephen Tyree",
      "Kilian Weinberger"
    ],
    "summary": "The goal of machine learning is to develop predictors that generalize well to\ntest data. Ideally, this is achieved by training on an almost infinitely large\ntraining data set that captures all variations in the data distribution. In\npractical learning settings, however, we do not have infinite data and our\npredictors may overfit. Overfitting may be combatted, for example, by adding a\nregularizer to the training objective or by defining a prior over the model\nparameters and performing Bayesian inference. In this paper, we propose a\nthird, alternative approach to combat overfitting: we extend the training set\nwith infinitely many artificial training examples that are obtained by\ncorrupting the original training data. We show that this approach is practical\nand efficient for a range of predictors and corruption models. Our approach,\ncalled marginalized corrupted features (MCF), trains robust predictors by\nminimizing the expected value of the loss function under the corruption model.\nWe show empirically on a variety of data sets that MCF classifiers can be\ntrained efficiently, may generalize substantially better to test data, and are\nalso more robust to feature deletion at test time.",
    "published": "2014-02-27T18:31:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploiting the Statistics of Learning and Inference",
    "authors": [
      "Max Welling"
    ],
    "summary": "When dealing with datasets containing a billion instances or with simulations\nthat require a supercomputer to execute, computational resources become part of\nthe equation. We can improve the efficiency of learning and inference by\nexploiting their inherent statistical nature. We propose algorithms that\nexploit the redundancy of data relative to a model by subsampling data-cases\nfor every update and reasoning about the uncertainty created in this process.\nIn the context of learning we propose to test for the probability that a\nstochastically estimated gradient points more than 180 degrees in the wrong\ndirection. In the context of MCMC sampling we use stochastic gradients to\nimprove the efficiency of MCMC updates, and hypothesis tests based on adaptive\nmini-batches to decide whether to accept or reject a proposed parameter update.\nFinally, we argue that in the context of likelihood free MCMC one needs to\nstore all the information revealed by all simulations, for instance in a\nGaussian process. We conclude that Bayesian methods will remain to play a\ncrucial role in the era of big data and big simulations, but only if we\novercome a number of computational challenges.",
    "published": "2014-02-26T10:47:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sleep Analytics and Online Selective Anomaly Detection",
    "authors": [
      "Tahereh Babaie",
      "Sanjay Chawla",
      "Romesh Abeysuriya"
    ],
    "summary": "We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to\nmodel a specific scenario emerging from research in sleep science. Scientists\nhave segmented sleep into several stages and stage two is characterized by two\npatterns (or anomalies) in the EEG time series recorded on sleep subjects.\nThese two patterns are sleep spindle (SS) and K-complex. The OSAD problem was\nintroduced to design a residual system, where all anomalies (known and unknown)\nare detected but the system only triggers an alarm when non-SS anomalies\nappear. The solution of the OSAD problem required us to combine techniques from\nboth machine learning and control theory. Experiments on data from real\nsubjects attest to the effectiveness of our approach.",
    "published": "2014-03-02T04:14:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Structurally Smoothed Graphlet Kernel",
    "authors": [
      "Pinar Yanardag",
      "S. V. N. Vishwanathan"
    ],
    "summary": "A commonly used paradigm for representing graphs is to use a vector that\ncontains normalized frequencies of occurrence of certain motifs or sub-graphs.\nThis vector representation can be used in a variety of applications, such as,\nfor computing similarity between graphs. The graphlet kernel of Shervashidze et\nal. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj\n[28]) as motifs in the vector representation, and computes the kernel via a dot\nproduct between these vectors. One can easily show that this is a valid kernel\nbetween graphs. However, such a vector representation suffers from a few\ndrawbacks. As k becomes larger we encounter the sparsity problem; most higher\norder graphlets will not occur in a given graph. This leads to diagonal\ndominance, that is, a given graph is similar to itself but not to any other\ngraph in the dataset. On the other hand, since lower order graphlets tend to be\nmore numerous, using lower values of k does not provide enough discrimination\nability. We propose a smoothing technique to tackle the above problems. Our\nmethod is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing\ntechniques from natural language processing to graphs. We use the relationships\nbetween lower order and higher order graphlets in order to derive our method.\nConsequently, our smoothing algorithm not only respects the dependency between\nsub-graphs but also tackles the diagonal dominance problem by distributing the\nprobability mass across graphlets. In our experiments, the smoothed graphlet\nkernel outperforms graph kernels based on raw frequency counts.",
    "published": "2014-03-03T21:20:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax\n  Algorithms and Normal Approximations",
    "authors": [
      "H. Brendan McMahan",
      "Francesco Orabona"
    ],
    "summary": "We study algorithms for online linear optimization in Hilbert spaces,\nfocusing on the case where the player is unconstrained. We develop a novel\ncharacterization of a large class of minimax algorithms, recovering, and even\nimproving, several previous results as immediate corollaries. Moreover, using\nour tools, we develop an algorithm that provides a regret bound of\n$\\mathcal{O}\\Big(U \\sqrt{T \\log(U \\sqrt{T} \\log^2 T +1)}\\Big)$, where $U$ is\nthe $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to\nthe player. This bound is optimal up to $\\sqrt{\\log \\log T}$ terms. When $T$ is\nknown, we derive an algorithm with an optimal regret bound (up to constant\nfactors). For both the known and unknown $T$ case, a Normal approximation to\nthe conditional value of the game proves to be the key analysis tool.",
    "published": "2014-03-03T23:06:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Integer Programming Relaxations for Integrated Clustering and Outlier\n  Detection",
    "authors": [
      "Lionel Ott",
      "Linsey Pang",
      "Fabio Ramos",
      "David Howe",
      "Sanjay Chawla"
    ],
    "summary": "In this paper we present methods for exemplar based clustering with outlier\nselection based on the facility location formulation. Given a distance function\nand the number of outliers to be found, the methods automatically determine the\nnumber of clusters and outliers. We formulate the problem as an integer program\nto which we present relaxations that allow for solutions that scale to large\ndata sets. The advantages of combining clustering and outlier selection\ninclude: (i) the resulting clusters tend to be compact and semantically\ncoherent (ii) the clusters are more robust against data perturbations and (iii)\nthe outliers are contextualised by the clusters and more interpretable, i.e. it\nis easier to distinguish between outliers which are the result of data errors\nfrom those that may be indicative of a new pattern emergent in the data. We\npresent and contrast three relaxations to the integer program formulation: (i)\na linear programming formulation (LP) (ii) an extension of affinity propagation\nto outlier detection (APOC) and (iii) a Lagrangian duality based formulation\n(LD). Evaluation on synthetic as well as real data shows the quality and\nscalability of these different methods.",
    "published": "2014-03-06T02:42:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Predictive Overlapping Co-Clustering",
    "authors": [
      "Chandrima Sarkar",
      "Jaideep Srivastava"
    ],
    "summary": "In the past few years co-clustering has emerged as an important data mining\ntool for two way data analysis. Co-clustering is more advantageous over\ntraditional one dimensional clustering in many ways such as, ability to find\nhighly correlated sub-groups of rows and columns. However, one of the\noverlooked benefits of co-clustering is that, it can be used to extract\nmeaningful knowledge for various other knowledge extraction purposes. For\nexample, building predictive models with high dimensional data and\nheterogeneous population is a non-trivial task. Co-clusters extracted from such\ndata, which shows similar pattern in both the dimension, can be used for a more\naccurate predictive model building. Several applications such as finding\npatient-disease cohorts in health care analysis, finding user-genre groups in\nrecommendation systems and community detection problems can benefit from\nco-clustering technique that utilizes the predictive power of the data to\ngenerate co-clusters for improved data analysis.\n  In this paper, we present the novel idea of Predictive Overlapping\nCo-Clustering (POCC) as an optimization problem for a more effective and\nimproved predictive analysis. Our algorithm generates optimal co-clusters by\nmaximizing predictive power of the co-clusters subject to the constraints on\nthe number of row and column clusters. In this paper precision, recall and\nf-measure have been used as evaluation measures of the resulting co-clusters.\nResults of our algorithm has been compared with two other well-known techniques\n- K-means and Spectral co-clustering, over four real data set namely, Leukemia,\nInternet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate\nthe effectiveness and utility of our algorithm POCC in practice.",
    "published": "2014-03-08T07:07:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improving Performance of a Group of Classification Algorithms Using\n  Resampling and Feature Selection",
    "authors": [
      "Mehdi Naseriparsa",
      "Amir-masoud Bidgoli",
      "Touraj Varaee"
    ],
    "summary": "In recent years the importance of finding a meaningful pattern from huge\ndatasets has become more challenging. Data miners try to adopt innovative\nmethods to face this problem by applying feature selection methods. In this\npaper we propose a new hybrid method in which we use a combination of\nresampling, filtering the sample domain and wrapper subset evaluation method\nwith genetic search to reduce dimensions of Lung-Cancer dataset that we\nreceived from UCI Repository of Machine Learning databases. Finally, we apply\nsome well- known classification algorithms (Na\\\"ive Bayes, Logistic, Multilayer\nPerceptron, Best First Decision Tree and JRIP) to the resulting dataset and\ncompare the results and prediction rates before and after the application of\nour feature selection method on that dataset. The results show a substantial\nprogress in the average performance of five classification algorithms\nsimultaneously and the classification error for these classifiers decreases\nconsiderably. The experiments also show that this method outperforms other\nfeature selection methods with a lower cost.",
    "published": "2014-03-08T07:47:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Categorization Axioms for Clustering Results",
    "authors": [
      "Jian Yu",
      "Zongben Xu"
    ],
    "summary": "Cluster analysis has attracted more and more attention in the field of\nmachine learning and data mining. Numerous clustering algorithms have been\nproposed and are being developed due to diverse theories and various\nrequirements of emerging applications. Therefore, it is very worth establishing\nan unified axiomatic framework for data clustering. In the literature, it is an\nopen problem and has been proved very challenging. In this paper, clustering\nresults are axiomatized by assuming that an proper clustering result should\nsatisfy categorization axioms. The proposed axioms not only introduce\nclassification of clustering results and inequalities of clustering results,\nbut also are consistent with prototype theory and exemplar theory of\ncategorization models in cognitive science. Moreover, the proposed axioms lead\nto three principles of designing clustering algorithm and cluster validity\nindex, which follow many popular clustering algorithms and cluster validity\nindices.",
    "published": "2014-03-09T14:51:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Hybrid Feature Selection Method to Improve Performance of a Group of\n  Classification Algorithms",
    "authors": [
      "Mehdi Naseriparsa",
      "Amir-Masoud Bidgoli",
      "Touraj Varaee"
    ],
    "summary": "In this paper a hybrid feature selection method is proposed which takes\nadvantages of wrapper subset evaluation with a lower cost and improves the\nperformance of a group of classifiers. The method uses combination of sample\ndomain filtering and resampling to refine the sample domain and two feature\nsubset evaluation methods to select reliable features. This method utilizes\nboth feature space and sample domain in two phases. The first phase filters and\nresamples the sample domain and the second phase adopts a hybrid procedure by\ninformation gain, wrapper subset evaluation and genetic search to find the\noptimal feature space. Experiments carried out on different types of datasets\nfrom UCI Repository of Machine Learning databases and the results show a rise\nin the average performance of five classifiers (Naive Bayes, Logistic,\nMultilayer Perceptron, Best First Decision Tree and JRIP) simultaneously and\nthe classification error for these classifiers decreases considerably. The\nexperiments also show that this method outperforms other feature selection\nmethods with a lower cost.",
    "published": "2014-03-08T08:04:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cancer Prognosis Prediction Using Balanced Stratified Sampling",
    "authors": [
      "J S Saleema",
      "N Bhagawathi",
      "S Monica",
      "P Deepa Shenoy",
      "K R Venugopal",
      "L M Patnaik"
    ],
    "summary": "High accuracy in cancer prediction is important to improve the quality of the\ntreatment and to improve the rate of survivability of patients. As the data\nvolume is increasing rapidly in the healthcare research, the analytical\nchallenge exists in double. The use of effective sampling technique in\nclassification algorithms always yields good prediction accuracy. The SEER\npublic use cancer database provides various prominent class labels for\nprognosis prediction. The main objective of this paper is to find the effect of\nsampling techniques in classifying the prognosis variable and propose an ideal\nsampling method based on the outcome of the experimentation. In the first phase\nof this work the traditional random sampling and stratified sampling techniques\nhave been used. At the next level the balanced stratified sampling with\nvariations as per the choice of the prognosis class labels have been tested.\nMuch of the initial time has been focused on performing the pre_processing of\nthe SEER data set. The classification model for experimentation has been built\nusing the breast cancer, respiratory cancer and mixed cancer data sets with\nthree traditional classifiers namely Decision Tree, Naive Bayes and K-Nearest\nNeighbor. The three prognosis factors survival, stage and metastasis have been\nused as class labels for experimental comparisons. The results shows a steady\nincrease in the prediction accuracy of balanced stratified model as the sample\nsize increases, but the traditional approach fluctuates before the optimum\nresults.",
    "published": "2014-03-12T14:33:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey of Algorithms and Analysis for Adaptive Online Learning",
    "authors": [
      "H. Brendan McMahan"
    ],
    "summary": "We present tools for the analysis of Follow-The-Regularized-Leader (FTRL),\nDual Averaging, and Mirror Descent algorithms when the regularizer\n(equivalently, prox-function or learning rate schedule) is chosen adaptively\nbased on the data. Adaptivity can be used to prove regret bounds that hold on\nevery round, and also allows for data-dependent regret bounds as in\nAdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive\nper-coordinate learning rates). We present results from a large number of prior\nworks in a unified manner, using a modular and tight analysis that isolates the\nkey arguments in easily re-usable lemmas. This approach strengthens pre-viously\nknown FTRL analysis techniques to produce bounds as tight as those achieved by\npotential functions or primal-dual analysis. Further, we prove a general and\nexact equivalence between an arbitrary adaptive Mirror Descent algorithm and a\ncorrespond- ing FTRL update, which allows us to analyze any Mirror Descent\nalgorithm in the same framework. The key to bridging the gap between Dual\nAveraging and Mirror Descent algorithms lies in an analysis of the\nFTRL-Proximal algorithm family. Our regret bounds are proved in the most\ngeneral form, holding for arbitrary norms and non-smooth regularizers with\ntime-varying weight.",
    "published": "2014-03-14T00:25:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Making Risk Minimization Tolerant to Label Noise",
    "authors": [
      "Aritra Ghosh",
      "Naresh Manwani",
      "P. S. Sastry"
    ],
    "summary": "In many applications, the training data, from which one needs to learn a\nclassifier, is corrupted with label noise. Many standard algorithms such as SVM\nperform poorly in presence of label noise. In this paper we investigate the\nrobustness of risk minimization to label noise. We prove a sufficient condition\non a loss function for the risk minimization under that loss to be tolerant to\nuniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss and\nprobit loss satisfy this condition though none of the standard convex loss\nfunctions satisfy it. We also prove that, by choosing a sufficiently large\nvalue of a parameter in the loss function, the sigmoid loss, ramp loss and\nprobit loss can be made tolerant to non-uniform label noise also if we can\nassume the classes to be separable under noise-free data distribution. Through\nextensive empirical studies, we show that risk minimization under the $0-1$\nloss, the sigmoid loss and the ramp loss has much better robustness to label\nnoise when compared to the SVM algorithm.",
    "published": "2014-03-14T15:30:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Mixed-norm Regularization for Brain Decoding",
    "authors": [
      "Rémi Flamary",
      "Nisrine Jrad",
      "Ronald Phlypo",
      "Marco Congedo",
      "Alain Rakotomamonjy"
    ],
    "summary": "This work investigates the use of mixed-norm regularization for sensor\nselection in Event-Related Potential (ERP) based Brain-Computer Interfaces\n(BCI). The classification problem is cast as a discriminative optimization\nframework where sensor selection is induced through the use of mixed-norms.\nThis framework is extended to the multi-task learning situation where several\nsimilar classification tasks related to different subjects are learned\nsimultaneously. In this case, multi-task learning helps in leveraging data\nscarcity issue yielding to more robust classifiers. For this purpose, we have\nintroduced a regularizer that induces both sensor selection and classifier\nsimilarities. The different regularization approaches are compared on three ERP\ndatasets showing the interest of mixed-norm regularization in terms of sensor\nselection. The multi-task approaches are evaluated when a small number of\nlearning examples are available yielding to significant performance\nimprovements especially for subjects performing poorly.",
    "published": "2014-03-14T16:15:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Negative Mixture Models by Tensor Decompositions",
    "authors": [
      "Guillaume Rabusseau",
      "François Denis"
    ],
    "summary": "This work considers the problem of estimating the parameters of negative\nmixture models, i.e. mixture models that possibly involve negative weights. The\ncontributions of this paper are as follows. (i) We show that every rational\nprobability distributions on strings, a representation which occurs naturally\nin spectral learning, can be computed by a negative mixture of at most two\nprobabilistic automata (or HMMs). (ii) We propose a method to estimate the\nparameters of negative mixture models having a specific tensor structure in\ntheir low order observable moments. Building upon a recent paper on tensor\ndecompositions for learning latent variable models, we extend this work to the\nbroader setting of tensors having a symmetric decomposition with positive and\nnegative weights. We introduce a generalization of the tensor power method for\ncomplex valued tensors, and establish theoretical convergence guarantees. (iii)\nWe show how our approach applies to negative Gaussian mixture models, for which\nwe provide some experiments.",
    "published": "2014-03-17T19:35:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Spectral Clustering with Jensen-type kernels and their multi-point\n  extensions",
    "authors": [
      "Debarghya Ghoshdastidar",
      "Ambedkar Dukkipati",
      "Ajay P. Adsul",
      "Aparna S. Vijayan"
    ],
    "summary": "Motivated by multi-distribution divergences, which originate in information\ntheory, we propose a notion of `multi-point' kernels, and study their\napplications. We study a class of kernels based on Jensen type divergences and\nshow that these can be extended to measure similarity among multiple points. We\nstudy tensor flattening methods and develop a multi-point (kernel) spectral\nclustering (MSC) method. We further emphasize on a special case of the proposed\nkernels, which is a multi-point extension of the linear (dot-product) kernel\nand show the existence of cubic time tensor flattening algorithm in this case.\nFinally, we illustrate the usefulness of our contributions using standard data\nsets and image segmentation tasks.",
    "published": "2014-03-18T09:04:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unconfused Ultraconservative Multiclass Algorithms",
    "authors": [
      "Ugo Louche",
      "Liva Ralaivola"
    ],
    "summary": "We tackle the problem of learning linear classifiers from noisy datasets in a\nmulticlass setting. The two-class version of this problem was studied a few\nyears ago by, e.g. Bylander (1994) and Blum et al. (1996): in these\ncontributions, the proposed approaches to fight the noise revolve around a\nPerceptron learning scheme fed with peculiar examples computed through a\nweighted average of points from the noisy training set. We propose to build\nupon these approaches and we introduce a new algorithm called UMA (for\nUnconfused Multiclass additive Algorithm) which may be seen as a generalization\nto the multiclass setting of the previous approaches. In order to characterize\nthe noise we use the confusion matrix as a multiclass extension of the\nclassification noise studied in the aforementioned literature. Theoretically\nwell-founded, UMA furthermore displays very good empirical noise robustness, as\nevidenced by numerical simulations conducted on both synthetic and real data.\nKeywords: Multiclass classification, Perceptron, Noisy labels, Confusion Matrix",
    "published": "2014-03-20T12:46:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Local Learning via Semidefinite Programming",
    "authors": [
      "Paul Christiano"
    ],
    "summary": "In many online learning problems we are interested in predicting local\ninformation about some universe of items. For example, we may want to know\nwhether two items are in the same cluster rather than computing an assignment\nof items to clusters; we may want to know which of two teams will win a game\nrather than computing a ranking of teams. Although finding the optimal\nclustering or ranking is typically intractable, it may be possible to predict\nthe relationships between items as well as if you could solve the global\noptimization problem exactly.\n  Formally, we consider an online learning problem in which a learner\nrepeatedly guesses a pair of labels (l(x), l(y)) and receives an adversarial\npayoff depending on those labels. The learner's goal is to receive a payoff\nnearly as good as the best fixed labeling of the items. We show that a simple\nalgorithm based on semidefinite programming can obtain asymptotically optimal\nregret in the case where the number of possible labels is O(1), resolving an\nopen problem posed by Hazan, Kale, and Shalev-Schwartz. Our main technical\ncontribution is a novel use and analysis of the log determinant regularizer,\nexploiting the observation that log det(A + I) upper bounds the entropy of any\ndistribution with covariance matrix A.",
    "published": "2014-03-20T20:36:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Information-Theoretic Analysis of Thompson Sampling",
    "authors": [
      "Daniel Russo",
      "Benjamin Van Roy"
    ],
    "summary": "We provide an information-theoretic analysis of Thompson sampling that\napplies across a broad range of online optimization problems in which a\ndecision-maker must learn from partial feedback. This analysis inherits the\nsimplicity and elegance of information theory and leads to regret bounds that\nscale with the entropy of the optimal-action distribution. This strengthens\npreexisting results and yields new insight into how information improves\nperformance.",
    "published": "2014-03-21T01:42:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning to Optimize via Information-Directed Sampling",
    "authors": [
      "Daniel Russo",
      "Benjamin Van Roy"
    ],
    "summary": "We propose information-directed sampling -- a new approach to online\noptimization problems in which a decision-maker must balance between\nexploration and exploitation while learning from partial feedback. Each action\nis sampled in a manner that minimizes the ratio between squared expected\nsingle-period regret and a measure of information gain: the mutual information\nbetween the optimal action and the next observation. We establish an expected\nregret bound for information-directed sampling that applies across a very\ngeneral class of models and scales with the entropy of the optimal action\ndistribution. We illustrate through simple analytic examples how\ninformation-directed sampling accounts for kinds of information that\nalternative approaches do not adequately address and that this can lead to\ndramatic performance gains. For the widely studied Bernoulli, Gaussian, and\nlinear bandit problems, we demonstrate state-of-the-art simulation performance.",
    "published": "2014-03-21T02:02:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning of k-CNF Boolean Functions",
    "authors": [
      "Joel Veness",
      "Marcus Hutter"
    ],
    "summary": "This paper revisits the problem of learning a k-CNF Boolean function from\nexamples in the context of online learning under the logarithmic loss. In doing\nso, we give a Bayesian interpretation to one of Valiant's celebrated PAC\nlearning algorithms, which we then build upon to derive two efficient, online,\nprobabilistic, supervised learning algorithms for predicting the output of an\nunknown k-CNF Boolean function. We analyze the loss of our methods, and show\nthat the cumulative log-loss can be upper bounded, ignoring logarithmic\nfactors, by a polynomial function of the size of each example.",
    "published": "2014-03-26T21:17:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A study on cost behaviors of binary classification measures in\n  class-imbalanced problems",
    "authors": [
      "Bao-Gang Hu",
      "Wei-Ming Dong"
    ],
    "summary": "This work investigates into cost behaviors of binary classification measures\nin a background of class-imbalanced problems. Twelve performance measures are\nstudied, such as F measure, G-means in terms of accuracy rates, and of recall\nand precision, balance error rate (BER), Matthews correlation coefficient\n(MCC), Kappa coefficient, etc. A new perspective is presented for those\nmeasures by revealing their cost functions with respect to the class imbalance\nratio. Basically, they are described by four types of cost functions. The\nfunctions provides a theoretical understanding why some measures are suitable\nfor dealing with class-imbalanced problems. Based on their cost functions, we\nare able to conclude that G-means of accuracy rates and BER are suitable\nmeasures because they show \"proper\" cost behaviors in terms of \"a\nmisclassification from a small class will cause a greater cost than that from a\nlarge class\". On the contrary, F1 measure, G-means of recall and precision, MCC\nand Kappa coefficient measures do not produce such behaviors so that they are\nunsuitable to serve our goal in dealing with the problems properly.",
    "published": "2014-03-26T05:43:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Approximate Decentralized Bayesian Inference",
    "authors": [
      "Trevor Campbell",
      "Jonathan P. How"
    ],
    "summary": "This paper presents an approximate method for performing Bayesian inference\nin models with conditional independence over a decentralized network of\nlearning agents. The method first employs variational inference on each\nindividual learning agent to generate a local approximate posterior, the agents\ntransmit their local posteriors to other agents in the network, and finally\neach agent combines its set of received local posteriors. The key insight in\nthis work is that, for many Bayesian models, approximate inference schemes\ndestroy symmetry and dependencies in the model that are crucial to the correct\napplication of Bayes' rule when combining the local posteriors. The proposed\nmethod addresses this issue by including an additional optimization step in the\ncombination procedure that accounts for these broken dependencies. Experiments\non synthetic and real data demonstrate that the decentralized method provides\nadvantages in computational performance and predictive test likelihood over\nprevious batch and distributed methods.",
    "published": "2014-03-28T18:07:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Algorithms and Error Analysis for the Modified Nystrom Method",
    "authors": [
      "Shusen Wang",
      "Zhihua Zhang"
    ],
    "summary": "Many kernel methods suffer from high time and space complexities and are thus\nprohibitive in big-data applications. To tackle the computational challenge,\nthe Nystr\\\"om method has been extensively used to reduce time and space\ncomplexities by sacrificing some accuracy. The Nystr\\\"om method speedups\ncomputation by constructing an approximation of the kernel matrix using only a\nfew columns of the matrix. Recently, a variant of the Nystr\\\"om method called\nthe modified Nystr\\\"om method has demonstrated significant improvement over the\nstandard Nystr\\\"om method in approximation accuracy, both theoretically and\nempirically.\n  In this paper, we propose two algorithms that make the modified Nystr\\\"om\nmethod practical. First, we devise a simple column selection algorithm with a\nprovable error bound. Our algorithm is more efficient and easier to implement\nthan and nearly as accurate as the state-of-the-art algorithm. Second, with the\nselected columns at hand, we propose an algorithm that computes the\napproximation in lower time complexity than the approach in the previous work.\nFurthermore, we prove that the modified Nystr\\\"om method is exact under certain\nconditions, and we establish a lower error bound for the modified Nystr\\\"om\nmethod.",
    "published": "2014-04-01T06:26:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A probabilistic estimation and prediction technique for dynamic\n  continuous social science models: The evolution of the attitude of the Basque\n  Country population towards ETA as a case study",
    "authors": [
      "Juan-Carlos Cortés",
      "Francisco-J. Santonja",
      "Ana-C. Tarazona",
      "Rafael-J. Villanueva",
      "Javier Villanueva-Oller"
    ],
    "summary": "In this paper, we present a computational technique to deal with uncertainty\nin dynamic continuous models in Social Sciences. Considering data from surveys,\nthe method consists of determining the probability distribution of the survey\noutput and this allows to sample data and fit the model to the sampled data\nusing a goodness-of-fit criterion based on the chi-square-test. Taking the\nfitted parameters non-rejected by the chi-square-test, substituting them into\nthe model and computing their outputs, we build 95% confidence intervals in\neach time instant capturing uncertainty of the survey data (probabilistic\nestimation). Using the same set of obtained model parameters, we also provide a\nprediction over the next few years with 95% confidence intervals (probabilistic\nprediction). This technique is applied to a dynamic social model describing the\nevolution of the attitude of the Basque Country population towards the\nrevolutionary organization ETA.",
    "published": "2014-03-30T20:49:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Least Wrong Model Is Not in the Data",
    "authors": [
      "Oscar Stiffelman"
    ],
    "summary": "The true process that generated data cannot be determined when multiple\nexplanations are possible. Prediction requires a model of the probability that\na process, chosen randomly from the set of candidate explanations, generates\nsome future observation. The best model includes all of the information\ncontained in the minimal description of the data that is not contained in the\ndata. It is closely related to the Halting Problem and is logarithmic in the\nsize of the data. Prediction is difficult because the ideal model is not\ncomputable, and the best computable model is not \"findable.\" However, the error\nfrom any approximation can be bounded by the size of the description using the\nmodel.",
    "published": "2014-04-03T07:41:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bayes and Naive Bayes Classifier",
    "authors": [
      "Vikramkumar",
      "Vijaykumar B",
      "Trilochan"
    ],
    "summary": "The Bayesian Classification represents a supervised learning method as well\nas a statistical method for classification. Assumes an underlying probabilistic\nmodel and it allows us to capture uncertainty about the model in a principled\nway by determining probabilities of the outcomes. This Classification is named\nafter Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian\nclassification provides practical learning algorithms and prior knowledge and\nobserved data can be combined. Bayesian Classification provides a useful\nperspective for understanding and evaluating many learning algorithms. It\ncalculates explicit probabilities for hypothesis and it is robust to noise in\ninput data. In statistical classification the Bayes classifier minimises the\nprobability of misclassification. That was a visual intuition for a simple case\nof the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple\nBayes",
    "published": "2014-04-03T14:34:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Parallel Support Vector Machines in Practice",
    "authors": [
      "Stephen Tyree",
      "Jacob R. Gardner",
      "Kilian Q. Weinberger",
      "Kunal Agrawal",
      "John Tran"
    ],
    "summary": "In this paper, we evaluate the performance of various parallel optimization\nmethods for Kernel Support Vector Machines on multicore CPUs and GPUs. In\nparticular, we provide the first comparison of algorithms with explicit and\nimplicit parallelization. Most existing parallel implementations for multi-core\nor GPU architectures are based on explicit parallelization of Sequential\nMinimal Optimization (SMO)---the programmers identified parallelizable\ncomponents and hand-parallelized them, specifically tuned for a particular\narchitecture. We compare these approaches with each other and with implicitly\nparallelized algorithms---where the algorithm is expressed such that most of\nthe work is done within few iterations with large dense linear algebra\noperations. These can be computed with highly-optimized libraries, that are\ncarefully parallelized for a large variety of parallel platforms. We highlight\nthe advantages and disadvantages of both approaches and compare them on various\nbenchmark data sets. We find an approximate implicitly parallel algorithm which\nis surprisingly efficient, permits a much simpler implementation, and leads to\nunprecedented speedups in SVM training.",
    "published": "2014-04-03T19:49:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hierarchical Dirichlet Scaling Process",
    "authors": [
      "Dongwoo Kim",
      "Alice Oh"
    ],
    "summary": "We present the \\textit{hierarchical Dirichlet scaling process} (HDSP), a\nBayesian nonparametric mixed membership model. The HDSP generalizes the\nhierarchical Dirichlet process (HDP) to model the correlation structure between\nmetadata in the corpus and mixture components. We construct the HDSP based on\nthe normalized gamma representation of the Dirichlet process, and this\nconstruction allows incorporating a scaling function that controls the\nmembership probabilities of the mixture components. We develop two scaling\nmethods to demonstrate that different modeling assumptions can be expressed in\nthe HDSP. We also derive the corresponding approximate posterior inference\nalgorithms using variational Bayes. Through experiments on datasets of\nnewswire, medical journal articles, conference proceedings, and product\nreviews, we show that the HDSP results in a better predictive performance than\nlabeled LDA, partially labeled LDA, and author topic model and a better\nnegative review classification performance than the supervised topic model and\nSVM.",
    "published": "2014-03-22T06:25:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Efficient Feature Selection in Classification of Audio Files",
    "authors": [
      "Jayita Mitra",
      "Diganta Saha"
    ],
    "summary": "In this paper we have focused on an efficient feature selection method in\nclassification of audio files. The main objective is feature selection and\nextraction. We have selected a set of features for further analysis, which\nrepresents the elements in feature vector. By extraction method we can compute\na numerical representation that can be used to characterize the audio using the\nexisting toolbox. In this study Gain Ratio (GR) is used as a feature selection\nmeasure. GR is used to select splitting attribute which will separate the\ntuples into different classes. The pulse clarity is considered as a subjective\nmeasure and it is used to calculate the gain of features of audio files. The\nsplitting criterion is employed in the application to identify the class or the\nmusic genre of a specific audio file from testing database. Experimental\nresults indicate that by using GR the application can produce a satisfactory\nresult for music genre classification. After dimensionality reduction best\nthree features have been selected out of various features of audio file and in\nthis technique we will get more than 90% successful classification result.",
    "published": "2014-03-24T16:05:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Gradient-based Laplacian Feature Selection",
    "authors": [
      "Bo Wang",
      "Anna Goldenberg"
    ],
    "summary": "Analysis of high dimensional noisy data is of essence across a variety of\nresearch fields. Feature selection techniques are designed to find the relevant\nfeature subset that can facilitate classification or pattern detection.\nTraditional (supervised) feature selection methods utilize label information to\nguide the identification of relevant feature subsets. In this paper, however,\nwe consider the unsupervised feature selection problem. Without the label\ninformation, it is particularly difficult to identify a small set of relevant\nfeatures due to the noisy nature of real-world data which corrupts the\nintrinsic structure of the data. Our Gradient-based Laplacian Feature Selection\n(GLFS) selects important features by minimizing the variance of the Laplacian\nregularized least squares regression model. With $\\ell_1$ relaxation, GLFS can\nfind a sparse subset of features that is relevant to the Laplacian manifolds.\nExtensive experiments on simulated, three real-world object recognition and two\ncomputational biology datasets, have illustrated the power and superior\nperformance of our approach over multiple state-of-the-art unsupervised feature\nselection methods. Additionally, we show that GLFS selects a sparser set of\nmore relevant features in a supervised setting outperforming the popular\nelastic net methodology.",
    "published": "2014-04-10T20:49:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Pareto-Path Multi-Task Multiple Kernel Learning",
    "authors": [
      "Cong Li",
      "Michael Georgiopoulos",
      "Georgios C. Anagnostopoulos"
    ],
    "summary": "A traditional and intuitively appealing Multi-Task Multiple Kernel Learning\n(MT-MKL) method is to optimize the sum (thus, the average) of objective\nfunctions with (partially) shared kernel function, which allows information\nsharing amongst tasks. We point out that the obtained solution corresponds to a\nsingle point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO)\nproblem, which considers the concurrent optimization of all task objectives\ninvolved in the Multi-Task Learning (MTL) problem. Motivated by this last\nobservation and arguing that the former approach is heuristic, we propose a\nnovel Support Vector Machine (SVM) MT-MKL framework, that considers an\nimplicitly-defined set of conic combinations of task objectives. We show that\nsolving our framework produces solutions along a path on the aforementioned PF\nand that it subsumes the optimization of the average of objective functions as\na special case. Using algorithms we derived, we demonstrate through a series of\nexperimental results that the framework is capable of achieving better\nclassification performance, when compared to other similar MTL approaches.",
    "published": "2014-04-11T19:15:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Discovering and Exploiting Entailment Relationships in Multi-Label\n  Learning",
    "authors": [
      "Christina Papagiannopoulou",
      "Grigorios Tsoumakas",
      "Ioannis Tsamardinos"
    ],
    "summary": "This work presents a sound probabilistic method for enforcing adherence of\nthe marginal probabilities of a multi-label model to automatically discovered\ndeterministic relationships among labels. In particular we focus on discovering\ntwo kinds of relationships among the labels. The first one concerns pairwise\npositive entailement: pairs of labels, where the presence of one implies the\npresence of the other in all instances of a dataset. The second concerns\nexclusion: sets of labels that do not coexist in the same instances of the\ndataset. These relationships are represented with a Bayesian network. Marginal\nprobabilities are entered as soft evidence in the network and adjusted through\nprobabilistic inference. Our approach offers robust improvements in mean\naverage precision compared to the standard binary relavance approach across all\n12 datasets involved in our experiments. The discovery process helps\ninteresting implicit knowledge to emerge, which could be useful in itself.",
    "published": "2014-04-15T19:47:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Ensemble Classifiers and Their Applications: A Review",
    "authors": [
      "Akhlaqur Rahman",
      "Sumaira Tasnim"
    ],
    "summary": "Ensemble classifier refers to a group of individual classifiers that are\ncooperatively trained on data set in a supervised classification problem. In\nthis paper we present a review of commonly used ensemble classifiers in the\nliterature. Some ensemble classifiers are also developed targeting specific\napplications. We also present some application driven ensemble classifiers in\nthis paper.",
    "published": "2014-04-15T21:35:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Representation as a Service",
    "authors": [
      "Ouais Alsharif",
      "Philip Bachman",
      "Joelle Pineau"
    ],
    "summary": "Consider a Machine Learning Service Provider (MLSP) designed to rapidly\ncreate highly accurate learners for a never-ending stream of new tasks. The\nchallenge is to produce task-specific learners that can be trained from few\nlabeled samples, even if tasks are not uniquely identified, and the number of\ntasks and input dimensionality are large. In this paper, we argue that the MLSP\nshould exploit knowledge from previous tasks to build a good representation of\nthe environment it is in, and more precisely, that useful representations for\nsuch a service are ones that minimize generalization error for a new hypothesis\ntrained on a new task. We formalize this intuition with a novel method that\nminimizes an empirical proxy of the intra-task small-sample generalization\nerror. We present several empirical results showing state-of-the art\nperformance on single-task transfer, multitask learning, and the full lifelong\nlearning problem.",
    "published": "2014-02-24T15:17:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Structured Stochastic Variational Inference",
    "authors": [
      "Matthew D. Hoffman",
      "David M. Blei"
    ],
    "summary": "Stochastic variational inference makes it possible to approximate posterior\ndistributions induced by large datasets quickly using stochastic optimization.\nThe algorithm relies on the use of fully factorized variational distributions.\nHowever, this \"mean-field\" independence approximation limits the fidelity of\nthe posterior approximation, and introduces local optima. We show how to relax\nthe mean-field approximation to allow arbitrary dependencies between global\nparameters and local hidden variables, producing better parameter estimates by\nreducing bias, sensitivity to local optima, and sensitivity to hyperparameters.",
    "published": "2014-04-16T00:12:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dropout Training for Support Vector Machines",
    "authors": [
      "Ning Chen",
      "Jun Zhu",
      "Jianfei Chen",
      "Bo Zhang"
    ],
    "summary": "Dropout and other feature noising schemes have shown promising results in\ncontrolling over-fitting by artificially corrupting the training data. Though\nextensive theoretical and empirical studies have been performed for generalized\nlinear models, little work has been done for support vector machines (SVMs),\none of the most successful approaches for supervised learning. This paper\npresents dropout training for linear SVMs. To deal with the intractable\nexpectation of the non-smooth hinge loss under corrupting distributions, we\ndevelop an iteratively re-weighted least square (IRLS) algorithm by exploring\ndata augmentation techniques. Our algorithm iteratively minimizes the\nexpectation of a re-weighted least square problem, where the re-weights have\nclosed-form solutions. The similar ideas are applied to develop a new IRLS\nalgorithm for the expected logistic loss under corrupting distributions. Our\nalgorithms offer insights on the connection and difference between the hinge\nloss and logistic loss in dropout training. Empirical results on several real\ndatasets demonstrate the effectiveness of dropout training on significantly\nboosting the classification accuracy of linear SVMs.",
    "published": "2014-04-16T08:54:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Agent Behavior Prediction and Its Generalization Analysis",
    "authors": [
      "Fei Tian",
      "Haifang Li",
      "Wei Chen",
      "Tao Qin",
      "Enhong Chen",
      "Tie-Yan Liu"
    ],
    "summary": "Machine learning algorithms have been applied to predict agent behaviors in\nreal-world dynamic systems, such as advertiser behaviors in sponsored search\nand worker behaviors in crowdsourcing. The behavior data in these systems are\ngenerated by live agents: once the systems change due to the adoption of the\nprediction models learnt from the behavior data, agents will observe and\nrespond to these changes by changing their own behaviors accordingly. As a\nresult, the behavior data will evolve and will not be identically and\nindependently distributed, posing great challenges to the theoretical analysis\non the machine learning algorithms for behavior prediction. To tackle this\nchallenge, in this paper, we propose to use Markov Chain in Random Environments\n(MCRE) to describe the behavior data, and perform generalization analysis of\nthe machine learning algorithms on its basis. Since the one-step transition\nprobability matrix of MCRE depends on both previous states and the random\nenvironment, conventional techniques for generalization analysis cannot be\ndirectly applied. To address this issue, we propose a novel technique that\ntransforms the original MCRE into a higher-dimensional time-homogeneous Markov\nchain. The new Markov chain involves more variables but is more regular, and\nthus easier to deal with. We prove the convergence of the new Markov chain when\ntime approaches infinity. Then we prove a generalization bound for the machine\nlearning algorithms on the behavior data generated by the new Markov chain,\nwhich depends on both the Markovian parameters and the covering number of the\nfunction class compounded by the loss function for behavior prediction and the\nbehavior prediction model. To the best of our knowledge, this is the first work\nthat performs the generalization analysis on data generated by complex\nprocesses in real-world dynamic systems.",
    "published": "2014-04-19T14:57:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Target Regression via Random Linear Target Combinations",
    "authors": [
      "Grigorios Tsoumakas",
      "Eleftherios Spyromitros-Xioufis",
      "Aikaterini Vrekou",
      "Ioannis Vlahavas"
    ],
    "summary": "Multi-target regression is concerned with the simultaneous prediction of\nmultiple continuous target variables based on the same set of input variables.\nIt arises in several interesting industrial and environmental application\ndomains, such as ecological modelling and energy forecasting. This paper\npresents an ensemble method for multi-target regression that constructs new\ntarget variables via random linear combinations of existing targets. We discuss\nthe connection of our approach with multi-label classification algorithms, in\nparticular RA$k$EL, which originally inspired this work, and a family of recent\nmulti-label classification algorithms that involve output coding. Experimental\nresults on 12 multi-target datasets show that it performs significantly better\nthan a strong baseline that learns a single model for each target using\ngradient boosting and compares favourably to multi-objective random forest\napproach, which is a state-of-the-art approach. The experiments further show\nthat our approach improves more when stronger unconditional dependencies exist\namong the targets.",
    "published": "2014-04-20T19:17:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Coactive Learning for Locally Optimal Problem Solving",
    "authors": [
      "Robby Goetschalckx",
      "Alan Fern",
      "Prasad Tadepalli"
    ],
    "summary": "Coactive learning is an online problem solving setting where the solutions\nprovided by a solver are interactively improved by a domain expert, which in\nturn drives learning. In this paper we extend the study of coactive learning to\nproblems where obtaining a globally optimal or near-optimal solution may be\nintractable or where an expert can only be expected to make small, local\nimprovements to a candidate solution. The goal of learning in this new setting\nis to minimize the cost as measured by the expert effort over time. We first\nestablish theoretical bounds on the average cost of the existing coactive\nPerceptron algorithm. In addition, we consider new online algorithms that use\ncost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved\ntheoretical bounds. We provide an empirical evaluation of the learners in\nvarious domains, which show that the Perceptron based algorithms are quite\neffective and that unlike the case for online classification, the PA algorithms\ndo not yield significant performance gains.",
    "published": "2014-04-18T21:17:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Overlapping Trace Norms in Multi-View Learning",
    "authors": [
      "Behrouz Behmardi",
      "Cedric Archambeau",
      "Guillaume Bouchard"
    ],
    "summary": "Multi-view learning leverages correlations between different sources of data\nto make predictions in one view based on observations in another view. A\npopular approach is to assume that, both, the correlations between the views\nand the view-specific covariances have a low-rank structure, leading to\ninter-battery factor analysis, a model closely related to canonical correlation\nanalysis. We propose a convex relaxation of this model using structured norm\nregularization. Further, we extend the convex formulation to a robust version\nby adding an l1-penalized matrix to our estimator, similarly to convex robust\nPCA. We develop and compare scalable algorithms for several convex multi-view\nmodels. We show experimentally that the view-specific correlations are\nimproving data imputation performances, as well as labeling accuracy in\nreal-world multi-label prediction tasks.",
    "published": "2014-04-24T15:50:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multitask Learning for Sequence Labeling Tasks",
    "authors": [
      "Arvind Agarwal",
      "Saurabh Kataria"
    ],
    "summary": "In this paper, we present a learning method for sequence labeling tasks in\nwhich each example sequence has multiple label sequences. Our method learns\nmultiple models, one model for each label sequence. Each model computes the\njoint probability of all label sequences given the example sequence. Although\neach model considers all label sequences, its primary focus is only one label\nsequence, and therefore, each model becomes a task-specific model, for the task\nbelonging to that primary label. Such multiple models are learned {\\it\nsimultaneously} by facilitating the learning transfer among models through {\\it\nexplicit parameter sharing}. We experiment the proposed method on two\napplications and show that our method significantly outperforms the\nstate-of-the-art method.",
    "published": "2014-04-25T22:59:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Comparison of First-order Algorithms for Machine Learning",
    "authors": [
      "Yu Wei",
      "Pock Thomas"
    ],
    "summary": "Using an optimization algorithm to solve a machine learning problem is one of\nmainstreams in the field of science. In this work, we demonstrate a\ncomprehensive comparison of some state-of-the-art first-order optimization\nalgorithms for convex optimization problems in machine learning. We concentrate\non several smooth and non-smooth machine learning problems with a loss function\nplus a regularizer. The overall experimental results show the superiority of\nprimal-dual algorithms in solving a machine learning problem from the\nperspectives of the ease to construct, running time and accuracy.",
    "published": "2014-04-26T19:24:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Approximation of Rotations and Hessians matrices",
    "authors": [
      "Michael Mathieu",
      "Yann LeCun"
    ],
    "summary": "A new method to represent and approximate rotation matrices is introduced.\nThe method represents approximations of a rotation matrix $Q$ with linearithmic\ncomplexity, i.e. with $\\frac{1}{2}n\\lg(n)$ rotations over pairs of coordinates,\narranged in an FFT-like fashion. The approximation is \"learned\" using gradient\ndescent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is\na diagonal matrix. It can be used to approximate covariance matrix of Gaussian\nmodels in order to speed up inference, or to estimate and track the inverse\nHessian of an objective function by relating changes in parameters to changes\nin gradient along the trajectory followed by the optimization procedure.\nExperiments were conducted to approximate synthetic matrices, covariance\nmatrices of real data, and Hessian matrices of objective functions involved in\nmachine learning problems.",
    "published": "2014-04-29T00:08:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Meteorological time series forecasting based on MLP modelling using\n  heterogeneous transfer functions",
    "authors": [
      "Cyril Voyant",
      "Marie Laure Nivet",
      "Christophe Paoli",
      "Marc Muselli",
      "Gilles Notton"
    ],
    "summary": "In this paper, we propose to study four meteorological and seasonal time\nseries coupled with a multi-layer perceptron (MLP) modeling. We chose to\ncombine two transfer functions for the nodes of the hidden layer, and to use a\ntemporal indicator (time index as input) in order to take into account the\nseasonal aspect of the studied time series. The results of the prediction\nconcern two years of measurements and the learning step, eight independent\nyears. We show that this methodology can improve the accuracy of meteorological\ndata estimation compared to a classical MLP modelling with a homogenous\ntransfer function.",
    "published": "2014-04-29T06:43:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Implementing spectral methods for hidden Markov models with real-valued\n  emissions",
    "authors": [
      "Carl Mattfeld"
    ],
    "summary": "Hidden Markov models (HMMs) are widely used statistical models for modeling\nsequential data. The parameter estimation for HMMs from time series data is an\nimportant learning problem. The predominant methods for parameter estimation\nare based on local search heuristics, most notably the expectation-maximization\n(EM) algorithm. These methods are prone to local optima and oftentimes suffer\nfrom high computational and sample complexity. Recent years saw the emergence\nof spectral methods for the parameter estimation of HMMs, based on a method of\nmoments approach. Two spectral learning algorithms as proposed by Hsu, Kakade\nand Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012\n(arXiv:1203.0683) are assessed in this work. Using experiments with synthetic\ndata, the algorithms are compared with each other. Furthermore, the spectral\nmethods are compared to the Baum-Welch algorithm, a well-established method\napplying the EM algorithm to HMMs. The spectral algorithms are found to have a\nmuch more favorable computational and sample complexity. Even though the\nalgorithms readily handle high dimensional observation spaces, instability\nissues are encountered in this regime. In view of learning from real-world\nexperimental data, the representation of real-valued observations for the use\nin spectral methods is discussed, presenting possible methods to represent data\nfor the use in the learning algorithms.",
    "published": "2014-04-29T19:28:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Map of Update Constraints in Inductive Inference",
    "authors": [
      "Timo Kötzing",
      "Raphaela Palenta"
    ],
    "summary": "We investigate how different learning restrictions reduce learning power and\nhow the different restrictions relate to one another. We give a complete map\nfor nine different restrictions both for the cases of complete information\nlearning and set-driven learning. This completes the picture for these\nwell-studied \\emph{delayable} learning restrictions. A further insight is\ngained by different characterizations of \\emph{conservative} learning in terms\nof variants of \\emph{cautious} learning.\n  Our analyses greatly benefit from general theorems we give, for example\nshowing that learners with exclusively delayable restrictions can always be\nassumed total.",
    "published": "2014-04-29T20:54:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Exact Learning Monotone DNF from Membership Queries",
    "authors": [
      "Hasan Abasi",
      "Nader H. Bshouty",
      "Hanna Mazzawi"
    ],
    "summary": "In this paper, we study the problem of learning a monotone DNF with at most\n$s$ terms of size (number of variables in each term) at most $r$ ($s$ term\n$r$-MDNF) from membership queries. This problem is equivalent to the problem of\nlearning a general hypergraph using hyperedge-detecting queries, a problem\nmotivated by applications arising in chemical reactions and genome sequencing.\n  We first present new lower bounds for this problem and then present\ndeterministic and randomized adaptive algorithms with query complexities that\nare almost optimal. All the algorithms we present in this paper run in time\nlinear in the query complexity and the number of variables $n$. In addition,\nall of the algorithms we present in this paper are asymptotically tight for\nfixed $r$ and/or $s$.",
    "published": "2014-05-05T06:49:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptation Algorithm and Theory Based on Generalized Discrepancy",
    "authors": [
      "Corinna Cortes",
      "Mehryar Mohri",
      "Andres Muñoz Medina"
    ],
    "summary": "We present a new algorithm for domain adaptation improving upon a discrepancy\nminimization algorithm previously shown to outperform a number of algorithms\nfor this task. Unlike many previous algorithms for domain adaptation, our\nalgorithm does not consist of a fixed reweighting of the losses over the\ntraining sample. We show that our algorithm benefits from a solid theoretical\nfoundation and more favorable learning bounds than discrepancy minimization. We\npresent a detailed description of our algorithm and give several efficient\nsolutions for solving its optimization problem. We also report the results of\nseveral experiments showing that it outperforms discrepancy minimization.",
    "published": "2014-05-07T04:39:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Boolean Halfspaces with Small Weights from Membership Queries",
    "authors": [
      "Hasan Abasi",
      "Ali Z. Abdi",
      "Nader H. Bshouty"
    ],
    "summary": "We consider the problem of proper learning a Boolean Halfspace with integer\nweights $\\{0,1,\\ldots,t\\}$ from membership queries only. The best known\nalgorithm for this problem is an adaptive algorithm that asks $n^{O(t^5)}$\nmembership queries where the best lower bound for the number of membership\nqueries is $n^t$ [Learning Threshold Functions with Small Weights Using\nMembership Queries. COLT 1999]\n  In this paper we close this gap and give an adaptive proper learning\nalgorithm with two rounds that asks $n^{O(t)}$ membership queries. We also give\na non-adaptive proper learning algorithm that asks $n^{O(t^3)}$ membership\nqueries.",
    "published": "2014-05-07T09:06:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Learners for Multiclass Problems",
    "authors": [
      "Amit Daniely",
      "Shai Shalev-Shwartz"
    ],
    "summary": "The fundamental theorem of statistical learning states that for binary\nclassification problems, any Empirical Risk Minimization (ERM) learning rule\nhas close to optimal sample complexity. In this paper we seek for a generic\noptimal learner for multiclass prediction. We start by proving a surprising\nresult: a generic optimal multiclass learner must be improper, namely, it must\nhave the ability to output hypotheses which do not belong to the hypothesis\nclass, even though it knows that all the labels are generated by some\nhypothesis from the class. In particular, no ERM learner is optimal. This\nbrings back the fundmamental question of \"how to learn\"? We give a complete\nanswer to this question by giving a new analysis of the one-inclusion\nmulticlass learner of Rubinstein et al (2006) showing that its sample\ncomplexity is essentially optimal. Then, we turn to study the popular\nhypothesis class of generalized linear classifiers. We derive optimal learners\nthat, unlike the one-inclusion algorithm, are computationally efficient.\nFurthermore, we show that the sample complexity of these learners is better\nthan the sample complexity of the ERM rule, thus settling in negative an open\nquestion due to Collins (2005).",
    "published": "2014-05-10T11:23:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Canonical Semi-Deterministic Transducer",
    "authors": [
      "Achilles Beros",
      "Colin de la Higuera"
    ],
    "summary": "We prove the existence of a canonical form for semi-deterministic transducers\nwith incomparable sets of output strings. Based on this, we develop an\nalgorithm which learns semi-deterministic transducers given access to\ntranslation queries. We also prove that there is no learning algorithm for\nsemi-deterministic transducers that uses only domain knowledge.",
    "published": "2014-05-10T22:30:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Selecting Near-Optimal Approximate State Representations in\n  Reinforcement Learning",
    "authors": [
      "Ronald Ortner",
      "Odalric-Ambrym Maillard",
      "Daniil Ryabko"
    ],
    "summary": "We consider a reinforcement learning setting introduced in (Maillard et al.,\nNIPS 2011) where the learner does not have explicit access to the states of the\nunderlying Markov decision process (MDP). Instead, she has access to several\nmodels that map histories of past interactions to states. Here we improve over\nknown regret bounds in this setting, and more importantly generalize to the\ncase where the models given to the learner do not contain a true model\nresulting in an MDP representation but only approximations of it. We also give\nimproved error bounds for state aggregation.",
    "published": "2014-05-12T07:45:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Clustering, Hamming Embedding, Generalized LSH and the Max Norm",
    "authors": [
      "Behnam Neyshabur",
      "Yury Makarychev",
      "Nathan Srebro"
    ],
    "summary": "We study the convex relaxation of clustering and hamming embedding, focusing\non the asymmetric case (co-clustering and asymmetric hamming embedding),\nunderstanding their relationship to LSH as studied by (Charikar 2002) and to\nthe max-norm ball, and the differences between their symmetric and asymmetric\nversions.",
    "published": "2014-05-13T14:36:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reducing Dueling Bandits to Cardinal Bandits",
    "authors": [
      "Nir Ailon",
      "Thorsten Joachims",
      "Zohar Karnin"
    ],
    "summary": "We present algorithms for reducing the Dueling Bandits problem to the\nconventional (stochastic) Multi-Armed Bandits problem. The Dueling Bandits\nproblem is an online model of learning with ordinal feedback of the form \"A is\npreferred to B\" (as opposed to cardinal feedback like \"A has value 2.5\"),\ngiving it wide applicability in learning from implicit user feedback and\nrevealed and stated preferences. In contrast to existing algorithms for the\nDueling Bandits problem, our reductions -- named $\\Doubler$, $\\MultiSbm$ and\n$\\DoubleSbm$ -- provide a generic schema for translating the extensive body of\nknown results about conventional Multi-Armed Bandit algorithms to the Dueling\nBandits setting. For $\\Doubler$ and $\\MultiSbm$ we prove regret upper bounds in\nboth finite and infinite settings, and conjecture about the performance of\n$\\DoubleSbm$ which empirically outperforms the other two as well as previous\nalgorithms in our experiments. In addition, we provide the first almost optimal\nregret bound in terms of second order terms, such as the differences between\nthe values of the arms.",
    "published": "2014-05-14T08:03:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization",
    "authors": [
      "Elad Hazan",
      "Tomer Koren",
      "Kfir Y. Levy"
    ],
    "summary": "The logistic loss function is often advocated in machine learning and\nstatistics as a smooth and strictly convex surrogate for the 0-1 loss. In this\npaper we investigate the question of whether these smoothness and convexity\nproperties make the logistic loss preferable to other widely considered options\nsuch as the hinge loss. We show that in contrast to known asymptotic bounds, as\nlong as the number of prediction/optimization iterations is sub exponential,\nthe logistic loss provides no improvement over a generic non-smooth loss\nfunction such as the hinge loss. In particular we show that the convergence\nrate of stochastic logistic optimization is bounded from below by a polynomial\nin the diameter of the decision set and the number of prediction iterations,\nand provide a matching tight upper bound. This resolves the COLT open problem\nof McMahan and Streeter (2012).",
    "published": "2014-05-15T13:29:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A two-step learning approach for solving full and almost full cold start\n  problems in dyadic prediction",
    "authors": [
      "Tapio Pahikkala",
      "Michiel Stock",
      "Antti Airola",
      "Tero Aittokallio",
      "Bernard De Baets",
      "Willem Waegeman"
    ],
    "summary": "Dyadic prediction methods operate on pairs of objects (dyads), aiming to\ninfer labels for out-of-sample dyads. We consider the full and almost full cold\nstart problem in dyadic prediction, a setting that occurs when both objects in\nan out-of-sample dyad have not been observed during training, or if one of them\nhas been observed, but very few times. A popular approach for addressing this\nproblem is to train a model that makes predictions based on a pairwise feature\nrepresentation of the dyads, or, in case of kernel methods, based on a tensor\nproduct pairwise kernel. As an alternative to such a kernel approach, we\nintroduce a novel two-step learning algorithm that borrows ideas from the\nfields of pairwise learning and spectral filtering. We show theoretically that\nthe two-step method is very closely related to the tensor product kernel\napproach, and experimentally that it yields a slightly better predictive\nperformance. Moreover, unlike existing tensor product kernel methods, the\ntwo-step method allows closed-form solutions for training and parameter\nselection via cross-validation estimates both in the full and almost full cold\nstart settings, making the approach much more efficient and straightforward to\nimplement.",
    "published": "2014-05-17T18:20:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning with Composite Loss Functions",
    "authors": [
      "Ofer Dekel",
      "Jian Ding",
      "Tomer Koren",
      "Yuval Peres"
    ],
    "summary": "We study a new class of online learning problems where each of the online\nalgorithm's actions is assigned an adversarial value, and the loss of the\nalgorithm at each step is a known and deterministic function of the values\nassigned to its recent actions. This class includes problems where the\nalgorithm's loss is the minimum over the recent adversarial values, the maximum\nover the recent values, or a linear combination of the recent values. We\nanalyze the minimax regret of this class of problems when the algorithm\nreceives bandit feedback, and prove that when the minimum or maximum functions\nare used, the minimax regret is $\\tilde \\Omega(T^{2/3})$ (so called hard online\nlearning problems), and when a linear function is used, the minimax regret is\n$\\tilde O(\\sqrt{T})$ (so called easy learning problems). Previously, the only\nonline learning problem that was known to be provably hard was the multi-armed\nbandit with switching costs.",
    "published": "2014-05-18T08:47:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Distributed Algorithm for Training Nonlinear Kernel Machines",
    "authors": [
      "Dhruv Mahajan",
      "S. Sathiya Keerthi",
      "S. Sundararajan"
    ],
    "summary": "This paper concerns the distributed training of nonlinear kernel machines on\nMap-Reduce. We show that a re-formulation of Nystr\\\"om approximation based\nsolution which is solved using gradient based techniques is well suited for\nthis, especially when it is necessary to work with a large number of basis\npoints. The main advantages of this approach are: avoidance of computing the\npseudo-inverse of the kernel sub-matrix corresponding to the basis points;\nsimplicity and efficiency of the distributed part of the computations; and,\nfriendliness to stage-wise addition of basis points. We implement the method\nusing an AllReduce tree on Hadoop and demonstrate its value on a few large\nbenchmark datasets.",
    "published": "2014-05-18T19:54:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A distributed block coordinate descent method for training $l_1$\n  regularized linear classifiers",
    "authors": [
      "Dhruv Mahajan",
      "S. Sathiya Keerthi",
      "S. Sundararajan"
    ],
    "summary": "Distributed training of $l_1$ regularized classifiers has received great\nattention recently. Most existing methods approach this problem by taking steps\nobtained from approximating the objective by a quadratic approximation that is\ndecoupled at the individual variable level. These methods are designed for\nmulticore and MPI platforms where communication costs are low. They are\ninefficient on systems such as Hadoop running on a cluster of commodity\nmachines where communication costs are substantial. In this paper we design a\ndistributed algorithm for $l_1$ regularization that is much better suited for\nsuch systems than existing algorithms. A careful cost analysis is used to\nsupport these points and motivate our method. The main idea of our algorithm is\nto do block optimization of many variables on the actual objective function\nwithin each computing node; this increases the computational cost per step that\nis matched with the communication cost, and decreases the number of outer\niterations, thus yielding a faster overall method. Distributed Gauss-Seidel and\nGauss-Southwell greedy schemes are used for choosing variables to update in\neach step. We establish global convergence theory for our algorithm, including\nQ-linear rate of convergence. Experiments on two benchmark problems show our\nmethod to be much faster than existing methods.",
    "published": "2014-05-18T20:07:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Lipschitz Bandits: Regret Lower Bounds and Optimal Algorithms",
    "authors": [
      "Stefan Magureanu",
      "Richard Combes",
      "Alexandre Proutiere"
    ],
    "summary": "We consider stochastic multi-armed bandit problems where the expected reward\nis a Lipschitz function of the arm, and where the set of arms is either\ndiscrete or continuous. For discrete Lipschitz bandits, we derive asymptotic\nproblem specific lower bounds for the regret satisfied by any algorithm, and\npropose OSLB and CKL-UCB, two algorithms that efficiently exploit the Lipschitz\nstructure of the problem. In fact, we prove that OSLB is asymptotically\noptimal, as its asymptotic regret matches the lower bound. The regret analysis\nof our algorithms relies on a new concentration inequality for weighted sums of\nKL divergences between the empirical distributions of rewards and their true\ndistributions. For continuous Lipschitz bandits, we propose to first discretize\nthe action space, and then apply OSLB or CKL-UCB, algorithms that provably\nexploit the structure efficiently. This approach is shown, through numerical\nexperiments, to significantly outperform existing algorithms that directly deal\nwith the continuous set of arms. Finally the results and algorithms are\nextended to contextual bandits with similarities.",
    "published": "2014-05-19T14:56:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Linear Optimization via Smoothing",
    "authors": [
      "Jacob Abernethy",
      "Chansoo Lee",
      "Abhinav Sinha",
      "Ambuj Tewari"
    ],
    "summary": "We present a new optimization-theoretic approach to analyzing\nFollow-the-Leader style algorithms, particularly in the setting where\nperturbations are used as a tool for regularization. We show that adding a\nstrongly convex penalty function to the decision rule and adding stochastic\nperturbations to data correspond to deterministic and stochastic smoothing\noperations, respectively. We establish an equivalence between \"Follow the\nRegularized Leader\" and \"Follow the Perturbed Leader\" up to the smoothness\nproperties. This intuition leads to a new generic analysis framework that\nrecovers and improves the previous known regret bounds of the class of\nalgorithms commonly known as Follow the Perturbed Leader.",
    "published": "2014-05-23T14:33:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Visualizing Random Forest with Self-Organising Map",
    "authors": [
      "Piotr Płoński",
      "Krzysztof Zaremba"
    ],
    "summary": "Random Forest (RF) is a powerful ensemble method for classification and\nregression tasks. It consists of decision trees set. Although, a single tree is\nwell interpretable for human, the ensemble of trees is a black-box model. The\npopular technique to look inside the RF model is to visualize a RF proximity\nmatrix obtained on data samples with Multidimensional Scaling (MDS) method.\nHerein, we present a novel method based on Self-Organising Maps (SOM) for\nrevealing intrinsic relationships in data that lay inside the RF used for\nclassification tasks. We propose an algorithm to learn the SOM with the\nproximity matrix obtained from the RF. The visualization of RF proximity matrix\nwith MDS and SOM is compared. What is more, the SOM learned with the RF\nproximity matrix has better classification accuracy in comparison to SOM\nlearned with Euclidean distance. Presented approach enables better\nunderstanding of the RF and additionally improves accuracy of the SOM.",
    "published": "2014-05-26T19:00:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Proximal Reinforcement Learning: A New Theory of Sequential Decision\n  Making in Primal-Dual Spaces",
    "authors": [
      "Sridhar Mahadevan",
      "Bo Liu",
      "Philip Thomas",
      "Will Dabney",
      "Steve Giguere",
      "Nicholas Jacek",
      "Ian Gemp",
      "Ji Liu"
    ],
    "summary": "In this paper, we set forth a new vision of reinforcement learning developed\nby us over the past few years, one that yields mathematically rigorous\nsolutions to longstanding important questions that have remained unresolved:\n(i) how to design reliable, convergent, and robust reinforcement learning\nalgorithms (ii) how to guarantee that reinforcement learning satisfies\npre-specified \"safety\" guarantees, and remains in a stable region of the\nparameter space (iii) how to design \"off-policy\" temporal difference learning\nalgorithms in a reliable and stable manner, and finally (iv) how to integrate\nthe study of reinforcement learning into the rich theory of stochastic\noptimization. In this paper, we provide detailed answers to all these questions\nusing the powerful framework of proximal operators.\n  The key idea that emerges is the use of primal dual spaces connected through\nthe use of a Legendre transform. This allows temporal difference updates to\noccur in dual spaces, allowing a variety of important technical advantages. The\nLegendre transform elegantly generalizes past algorithms for solving\nreinforcement learning problems, such as natural gradient methods, which we\nshow relate closely to the previously unconnected framework of mirror descent\nmethods. Equally importantly, proximal operator theory enables the systematic\ndevelopment of operator splitting methods that show how to safely and reliably\ndecompose complex products of gradients that occur in recent variants of\ngradient-based temporal difference learning. This key technical innovation\nmakes it possible to finally design \"true\" stochastic gradient methods for\nreinforcement learning. Finally, Legendre transforms enable a variety of other\nbenefits, including modeling sparsity and domain geometry. Our work builds\nextensively on recent work on the convergence of saddle-point algorithms, and\non the theory of monotone operators.",
    "published": "2014-05-26T23:11:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization,\n  Experimental Design and Bandits",
    "authors": [
      "Ruben Martinez-Cantin"
    ],
    "summary": "BayesOpt is a library with state-of-the-art Bayesian optimization methods to\nsolve nonlinear optimization, stochastic bandits or sequential experimental\ndesign problems. Bayesian optimization is sample efficient by building a\nposterior distribution to capture the evidence and prior knowledge for the\ntarget function. Built in standard C++, the library is extremely efficient\nwhile being portable and flexible. It includes a common interface for C, C++,\nPython, Matlab and Octave.",
    "published": "2014-05-29T00:37:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Effect of Different Distance Measures on the Performance of K-Means\n  Algorithm: An Experimental Study in Matlab",
    "authors": [
      "Mr. Dibya Jyoti Bora",
      "Dr. Anil Kumar Gupta"
    ],
    "summary": "K-means algorithm is a very popular clustering algorithm which is famous for\nits simplicity. Distance measure plays a very important rule on the performance\nof this algorithm. We have different distance measure techniques available. But\nchoosing a proper technique for distance calculation is totally dependent on\nthe type of the data that we are going to cluster. In this paper an\nexperimental study is done in Matlab to cluster the iris and wine data sets\nwith different distance measures and thereby observing the variation of the\nperformances shown.",
    "published": "2014-05-29T05:59:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Simultaneous Feature and Expert Selection within Mixture of Experts",
    "authors": [
      "Billy Peralta"
    ],
    "summary": "A useful strategy to deal with complex classification scenarios is the\n\"divide and conquer\" approach. The mixture of experts (MOE) technique makes use\nof this strategy by joinly training a set of classifiers, or experts, that are\nspecialized in different regions of the input space. A global model, or gate\nfunction, complements the experts by learning a function that weights their\nrelevance in different parts of the input space. Local feature selection\nappears as an attractive alternative to improve the specialization of experts\nand gate function, particularly, for the case of high dimensional data. Our\nmain intuition is that particular subsets of dimensions, or subspaces, are\nusually more appropriate to classify instances located in different regions of\nthe input space. Accordingly, this work contributes with a regularized variant\nof MoE that incorporates an embedded process for local feature selection using\n$L1$ regularization, with a simultaneous expert selection. The experiments are\nstill pending.",
    "published": "2014-05-29T17:32:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Flip-Flop Sublinear Models for Graphs: Proof of Theorem 1",
    "authors": [
      "Brijnesh Jain"
    ],
    "summary": "We prove that there is no class-dual for almost all sublinear models on\ngraphs.",
    "published": "2014-05-30T15:50:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Holistic Measures for Evaluating Prediction Models in Smart Grids",
    "authors": [
      "Saima Aman",
      "Yogesh Simmhan",
      "Viktor K. Prasanna"
    ],
    "summary": "The performance of prediction models is often based on \"abstract metrics\"\nthat estimate the model's ability to limit residual errors between the observed\nand predicted values. However, meaningful evaluation and selection of\nprediction models for end-user domains requires holistic and\napplication-sensitive performance measures. Inspired by energy consumption\nprediction models used in the emerging \"big data\" domain of Smart Power Grids,\nwe propose a suite of performance measures to rationally compare models along\nthe dimensions of scale independence, reliability, volatility and cost. We\ninclude both application independent and dependent measures, the latter\nparameterized to allow customization by domain experts to fit their scenario.\nWhile our measures are generalizable to other domains, we offer an empirical\nanalysis using real energy use data for three Smart Grid applications:\nplanning, customer education and demand response, which are relevant for energy\nsustainability. Our results underscore the value of the proposed measures to\noffer a deeper insight into models' behavior and their impact on real\napplications, which benefit both data mining researchers and practitioners.",
    "published": "2014-06-02T00:34:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning the Information Divergence",
    "authors": [
      "Onur Dikmen",
      "Zhirong Yang",
      "Erkki Oja"
    ],
    "summary": "Information divergence that measures the difference between two nonnegative\nmatrices or tensors has found its use in a variety of machine learning\nproblems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic\nNeighbor Embedding, topic models, and Bayesian network optimization. The\nsuccess of such a learning task depends heavily on a suitable divergence. A\nlarge variety of divergences have been suggested and analyzed, but very few\nresults are available for an objective choice of the optimal divergence for a\ngiven task. Here we present a framework that facilitates automatic selection of\nthe best divergence among a given family, based on standard maximum likelihood\nestimation. We first propose an approximated Tweedie distribution for the\nbeta-divergence family. Selecting the best beta then becomes a machine learning\nproblem solved by maximum likelihood. Next, we reformulate alpha-divergence in\nterms of beta-divergence, which enables automatic selection of alpha by maximum\nlikelihood with reuse of the learning principle for beta-divergence.\nFurthermore, we show the connections between gamma and beta-divergences as well\nas R\\'enyi and alpha-divergences, such that our automatic selection framework\nis extended to non-separable divergences. Experiments on both synthetic and\nreal-world data demonstrate that our method can quite accurately select\ninformation divergence across different learning problems and various\ndivergence families.",
    "published": "2014-06-05T13:44:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning to Discover Efficient Mathematical Identities",
    "authors": [
      "Wojciech Zaremba",
      "Karol Kurach",
      "Rob Fergus"
    ],
    "summary": "In this paper we explore how machine learning techniques can be applied to\nthe discovery of efficient mathematical identities. We introduce an attribute\ngrammar framework for representing symbolic expressions. Given a set of grammar\nrules we build trees that combine different rules, looking for branches which\nyield compositions that are analytically equivalent to a target expression, but\nof lower computational complexity. However, as the size of the trees grows\nexponentially with the complexity of the target expression, brute force search\nis impractical for all but the simplest of expressions. Consequently, we\nintroduce two novel learning approaches that are able to learn from simpler\nexpressions to guide the tree search. The first of these is a simple n-gram\nmodel, the other being a recursive neural-network. We show how these approaches\nenable us to derive complex identities, beyond reach of brute-force search, or\nhuman derivation.",
    "published": "2014-06-06T05:28:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Logarithmic Time Online Multiclass prediction",
    "authors": [
      "Anna Choromanska",
      "John Langford"
    ],
    "summary": "We study the problem of multiclass classification with an extremely large\nnumber of classes (k), with the goal of obtaining train and test time\ncomplexity logarithmic in the number of classes. We develop top-down tree\nconstruction approaches for constructing logarithmic depth trees. On the\ntheoretical front, we formulate a new objective function, which is optimized at\neach node of the tree and creates dynamic partitions of the data which are both\npure (in terms of class labels) and balanced. We demonstrate that under\nfavorable conditions, we can construct logarithmic depth trees that have leaves\nwith low label entropy. However, the objective function at the nodes is\nchallenging to optimize computationally. We address the empirical problem with\na new online decision tree construction procedure. Experiments demonstrate that\nthis online algorithm quickly achieves improvement in test error compared to\nmore common logarithmic training time approaches, which makes it a plausible\nmethod in computationally constrained large-k applications.",
    "published": "2014-06-06T21:52:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Credit Assignment Compiler for Joint Prediction",
    "authors": [
      "Kai-Wei Chang",
      "He He",
      "Hal Daumé III",
      "John Langford",
      "Stephane Ross"
    ],
    "summary": "Many machine learning applications involve jointly predicting multiple\nmutually dependent output variables. Learning to search is a family of methods\nwhere the complex decision problem is cast into a sequence of decisions via a\nsearch space. Although these methods have shown promise both in theory and in\npractice, implementing them has been burdensomely awkward. In this paper, we\nshow the search space can be defined by an arbitrary imperative program,\nturning learning to search into a credit assignment compiler. Altogether with\nthe algorithmic improvements for the compiler, we radically reduce the\ncomplexity of programming and the running time. We demonstrate the feasibility\nof our approach on multiple joint prediction tasks. In all cases, we obtain\naccuracies as high as alternative approaches, at drastically reduced execution\nand programming time.",
    "published": "2014-06-07T00:24:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Drifting-Games Analysis for Online Learning and Applications to\n  Boosting",
    "authors": [
      "Haipeng Luo",
      "Robert E. Schapire"
    ],
    "summary": "We provide a general mechanism to design online learning algorithms based on\na minimax analysis within a drifting-games framework. Different online learning\nsettings (Hedge, multi-armed bandit problems and online convex optimization)\nare studied by converting into various kinds of drifting games. The original\nminimax analysis for drifting games is then used and generalized by applying a\nseries of relaxations, starting from choosing a convex surrogate of the 0-1\nloss function. With different choices of surrogates, we not only recover\nexisting algorithms, but also propose new algorithms that are totally\nparameter-free and enjoy other useful properties. Moreover, our drifting-games\nframework naturally allows us to study high probability bounds without\nresorting to any concentration results, and also a generalized notion of regret\nthat measures how good the algorithm is compared to all but the top small\nfraction of candidates. Finally, we translate our new Hedge algorithm into a\nnew adaptive boosting algorithm that is computationally faster as shown in\nexperiments, since it ignores a large number of examples on each round.",
    "published": "2014-06-07T03:11:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reweighted Wake-Sleep",
    "authors": [
      "Jörg Bornschein",
      "Yoshua Bengio"
    ],
    "summary": "Training deep directed graphical models with many hidden variables and\nperforming inference remains a major challenge. Helmholtz machines and deep\nbelief networks are such models, and the wake-sleep algorithm has been proposed\nto train them. The wake-sleep algorithm relies on training not just the\ndirected generative model but also a conditional generative model (the\ninference network) that runs backward from visible to latent, estimating the\nposterior distribution of latent given visible. We propose a novel\ninterpretation of the wake-sleep algorithm which suggests that better\nestimators of the gradient can be obtained by sampling latent variables\nmultiple times from the inference network. This view is based on importance\nsampling as an estimator of the likelihood, with the approximate inference\nnetwork as a proposal distribution. This interpretation is confirmed\nexperimentally, showing that better likelihood can be achieved with this\nreweighted wake-sleep procedure. Based on this interpretation, we propose that\na sigmoidal belief network is not sufficiently powerful for the layers of the\ninference network in order to recover a good estimator of the posterior\ndistribution of latent variables. Our experiments show that using a more\npowerful layer model, such as NADE, yields substantially better generative\nmodels.",
    "published": "2014-06-11T00:44:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Kalman Temporal Differences",
    "authors": [
      "Matthieu Geist",
      "Olivier Pietquin"
    ],
    "summary": "Because reinforcement learning suffers from a lack of scalability, online\nvalue (and Q-) function approximation has received increasing interest this\nlast decade. This contribution introduces a novel approximation scheme, namely\nthe Kalman Temporal Differences (KTD) framework, that exhibits the following\nfeatures: sample-efficiency, non-linear approximation, non-stationarity\nhandling and uncertainty management. A first KTD-based algorithm is provided\nfor deterministic Markov Decision Processes (MDP) which produces biased\nestimates in the case of stochastic transitions. Than the eXtended KTD\nframework (XKTD), solving stochastic MDP, is described. Convergence is analyzed\nfor special cases for both deterministic and stochastic transitions. Related\nalgorithms are experimented on classical benchmarks. They compare favorably to\nthe state of the art while exhibiting the announced features.",
    "published": "2014-01-16T05:02:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Restricted Boltzmann Machine for Classification with Hierarchical\n  Correlated Prior",
    "authors": [
      "Gang Chen",
      "Sargur H. Srihari"
    ],
    "summary": "Restricted Boltzmann machines (RBM) and its variants have become hot research\ntopics recently, and widely applied to many classification problems, such as\ncharacter recognition and document categorization. Often, classification RBM\nignores the interclass relationship or prior knowledge of sharing information\namong classes. In this paper, we are interested in RBM with the hierarchical\nprior over classes. We assume parameters for nearby nodes are correlated in the\nhierarchical tree, and further the parameters at each node of the tree be\northogonal to those at its ancestors. We propose a hierarchical correlated RBM\nfor classification problem, which generalizes the classification RBM with\nsharing information among different classes. In order to reduce the redundancy\nbetween node parameters in the hierarchy, we also introduce orthogonal\nrestrictions to our objective function. We test our method on challenge\ndatasets, and show promising results compared to competitive baselines.",
    "published": "2014-06-13T02:19:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evaluation of Machine Learning Techniques for Green Energy Prediction",
    "authors": [
      "Ankur Sahai"
    ],
    "summary": "We evaluate the following Machine Learning techniques for Green Energy (Wind,\nSolar) Prediction: Bayesian Inference, Neural Networks, Support Vector\nMachines, Clustering techniques (PCA). Our objective is to predict green energy\nusing weather forecasts, predict deviations from forecast green energy, find\ncorrelation amongst different weather parameters and green energy availability,\nrecover lost or missing energy (/ weather) data. We use historical weather data\nand weather forecasts for the same.",
    "published": "2014-06-14T13:08:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Resource Allocation with Semi-Bandit Feedback",
    "authors": [
      "Tor Lattimore",
      "Koby Crammer",
      "Csaba Szepesvári"
    ],
    "summary": "We study a sequential resource allocation problem involving a fixed number of\nrecurring jobs. At each time-step the manager should distribute available\nresources among the jobs in order to maximise the expected number of completed\njobs. Allocating more resources to a given job increases the probability that\nit completes, but with a cut-off. Specifically, we assume a linear model where\nthe probability increases linearly until it equals one, after which allocating\nadditional resources is wasteful. We assume the difficulty of each job is\nunknown and present the first algorithm for this problem and prove upper and\nlower bounds on its regret. Despite its apparent simplicity, the problem has a\nrich structure: we show that an appropriate optimistic algorithm can improve\nits learning speed dramatically beyond the results one normally expects for\nsimilar problems as the problem becomes resource-laden.",
    "published": "2014-06-15T18:41:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Sober Look at Spectral Learning",
    "authors": [
      "Han Zhao",
      "Pascal Poupart"
    ],
    "summary": "Spectral learning recently generated lots of excitement in machine learning,\nlargely because it is the first known method to produce consistent estimates\n(under suitable conditions) for several latent variable models. In contrast,\nmaximum likelihood estimates may get trapped in local optima due to the\nnon-convex nature of the likelihood function of latent variable models. In this\npaper, we do an empirical evaluation of spectral learning (SL) and expectation\nmaximization (EM), which reveals an important gap between the theory and the\npractice. First, SL often leads to negative probabilities. Second, EM often\nyields better estimates than spectral learning and it does not seem to get\nstuck in local optima. We discuss how the rank of the model parameters and the\namount of training data can yield negative probabilities. We also question the\ncommon belief that maximum likelihood estimators are necessarily inconsistent.",
    "published": "2014-06-18T08:25:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Experimental Evaluation of Nearest Neighbour Time Series\n  Classification",
    "authors": [
      "Anthony Bagnall",
      "Jason Lines"
    ],
    "summary": "Data mining research into time series classification (TSC) has focussed on\nalternative distance measures for nearest neighbour classifiers. It is standard\npractice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a\nstraw man for comparison. As part of a wider investigation into elastic\ndistance measures for TSC~\\cite{lines14elastic}, we perform a series of\nexperiments to test whether this standard practice is valid.\n  Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to\nstandard classifiers, examine whether the performance of 1-NN Euclidean\napproaches that of 1-NN DTW as the number of cases increases, assess whether\nthere is any benefit of setting $k$ for $k$-NN through cross validation whether\nit is worth setting the warping path for DTW through cross validation and\nfinally is it better to use a window or weighting for DTW. Based on experiments\non 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to\nbeat but 1-NN with DTW is not, if window size is set through cross validation.",
    "published": "2014-06-18T15:09:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning computationally efficient dictionaries and their implementation\n  as fast transforms",
    "authors": [
      "Luc Le Magoarou",
      "Rémi Gribonval"
    ],
    "summary": "Dictionary learning is a branch of signal processing and machine learning\nthat aims at finding a frame (called dictionary) in which some training data\nadmits a sparse representation. The sparser the representation, the better the\ndictionary. The resulting dictionary is in general a dense matrix, and its\nmanipulation can be computationally costly both at the learning stage and later\nin the usage of this dictionary, for tasks such as sparse coding. Dictionary\nlearning is thus limited to relatively small-scale problems. In this paper,\ninspired by usual fast transforms, we consider a general dictionary structure\nthat allows cheaper manipulation, and propose an algorithm to learn such\ndictionaries --and their fast implementation-- over training data. The approach\nis demonstrated experimentally with the factorization of the Hadamard matrix\nand with synthetic dictionary learning experiments.",
    "published": "2014-06-20T13:52:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "From conformal to probabilistic prediction",
    "authors": [
      "Vladimir Vovk",
      "Ivan Petej",
      "Valentina Fedorova"
    ],
    "summary": "This paper proposes a new method of probabilistic prediction, which is based\non conformal prediction. The method is applied to the standard USPS data set\nand gives encouraging results.",
    "published": "2014-06-21T11:47:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "SPSD Matrix Approximation vis Column Selection: Theories, Algorithms,\n  and Extensions",
    "authors": [
      "Shusen Wang",
      "Luo Luo",
      "Zhihua Zhang"
    ],
    "summary": "Symmetric positive semidefinite (SPSD) matrix approximation is an important\nproblem with applications in kernel methods. However, existing SPSD matrix\napproximation methods such as the Nystr\\\"om method only have weak error bounds.\nIn this paper we conduct in-depth studies of an SPSD matrix approximation model\nand establish strong relative-error bounds. We call it the prototype model for\nit has more efficient and effective extensions, and some of its extensions have\nhigh scalability. Though the prototype model itself is not suitable for\nlarge-scale data, it is still useful to study its properties, on which the\nanalysis of its extensions relies.\n  This paper offers novel theoretical analysis, efficient algorithms, and a\nhighly accurate extension. First, we establish a lower error bound for the\nprototype model and improve the error bound of an existing column selection\nalgorithm to match the lower bound. In this way, we obtain the first optimal\ncolumn selection algorithm for the prototype model. We also prove that the\nprototype model is exact under certain conditions. Second, we develop a simple\ncolumn selection algorithm with a provable error bound. Third, we propose a\nso-called spectral shifting model to make the approximation more accurate when\nthe eigenvalues of the matrix decay slowly, and the improvement is\ntheoretically quantified. The spectral shifting method can also be applied to\nimprove other SPSD matrix approximation models.",
    "published": "2014-06-22T05:02:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stationary Mixing Bandits",
    "authors": [
      "Julien Audiffren",
      "Liva Ralaivola"
    ],
    "summary": "We study the bandit problem where arms are associated with stationary\nphi-mixing processes and where rewards are therefore dependent: the question\nthat arises from this setting is that of recovering some independence by\nignoring the value of some rewards. As we shall see, the bandit problem we\ntackle requires us to address the exploration/exploitation/independence\ntrade-off. To do so, we provide a UCB strategy together with a general regret\nanalysis for the case where the size of the independence blocks (the ignored\nrewards) is fixed and we go a step beyond by providing an algorithm that is\nable to compute the size of the independence blocks from the data. Finally, we\ngive an analysis of our bandit problem in the restless case, i.e., in the\nsituation where the time counters for all mixing processes simultaneously\nevolve.",
    "published": "2014-06-23T18:48:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier\n  Transform",
    "authors": [
      "Sakthithasan Sripirakas",
      "Russel Pears"
    ],
    "summary": "In this research we address the problem of capturing recurring concepts in a\ndata stream environment. Recurrence capture enables the re-use of previously\nlearned classifiers without the need for re-learning while providing for better\naccuracy during the concept recurrence interval. We capture concepts by\napplying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to\nobtain highly compressed versions of the trees at concept drift points in the\nstream and store such trees in a repository for future use. Our empirical\nresults on real world and synthetic data exhibiting varying degrees of\nrecurrence show that the Fourier compressed trees are more robust to noise and\nare able to capture recurring concepts with higher precision than a meta\nlearning approach that chooses to re-use classifiers in their originally\noccurring form.",
    "published": "2014-06-24T00:48:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generalized Mixability via Entropic Duality",
    "authors": [
      "Mark D. Reid",
      "Rafael M. Frongillo",
      "Robert C. Williamson",
      "Nishant Mehta"
    ],
    "summary": "Mixability is a property of a loss which characterizes when fast convergence\nis possible in the game of prediction with expert advice. We show that a key\nproperty of mixability generalizes, and the exp and log operations present in\nthe usual theory are not as special as one might have thought. In doing this we\nintroduce a more general notion of $\\Phi$-mixability where $\\Phi$ is a general\nentropy (\\ie, any convex function on probabilities). We show how a property\nshared by the convex dual of any such entropy yields a natural algorithm (the\nminimizer of a regret bound) which, analogous to the classical aggregating\nalgorithm, is guaranteed a constant regret when used with $\\Phi$-mixable\nlosses. We characterize precisely which $\\Phi$ have $\\Phi$-mixable losses and\nput forward a number of conjectures about the optimality and relationships\nbetween different choices of entropy.",
    "published": "2014-06-24T03:31:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Composite Likelihood Estimation for Restricted Boltzmann machines",
    "authors": [
      "Muneki Yasuda",
      "Shun Kataoka",
      "Yuji Waizumi",
      "Kazuyuki Tanaka"
    ],
    "summary": "Learning the parameters of graphical models using the maximum likelihood\nestimation is generally hard which requires an approximation. Maximum composite\nlikelihood estimations are statistical approximations of the maximum likelihood\nestimation which are higher-order generalizations of the maximum\npseudo-likelihood estimation. In this paper, we propose a composite likelihood\nmethod and investigate its property. Furthermore, we apply our composite\nlikelihood method to restricted Boltzmann machines.",
    "published": "2014-06-24T09:32:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Incremental Clustering: The Case for Extra Clusters",
    "authors": [
      "Margareta Ackerman",
      "Sanjoy Dasgupta"
    ],
    "summary": "The explosion in the amount of data available for analysis often necessitates\na transition from batch to incremental clustering methods, which process one\nelement at a time and typically store only a small subset of the data. In this\npaper, we initiate the formal analysis of incremental clustering methods\nfocusing on the types of cluster structure that they are able to detect. We\nfind that the incremental setting is strictly weaker than the batch model,\nproving that a fundamental class of cluster structures that can readily be\ndetected in the batch setting is impossible to identify using any incremental\nmethod. Furthermore, we show how the limitations of incremental clustering can\nbe overcome by allowing additional clusters.",
    "published": "2014-06-24T21:41:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comparison of SVM Optimization Techniques in the Primal",
    "authors": [
      "Jonathan Katzman",
      "Diane Duros"
    ],
    "summary": "This paper examines the efficacy of different optimization techniques in a\nprimal formulation of a support vector machine (SVM). Three main techniques are\ncompared. The dataset used to compare all three techniques was the Sentiment\nAnalysis on Movie Reviews dataset, from kaggle.com.",
    "published": "2014-06-28T18:59:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Contrastive Feature Induction for Efficient Structure Learning of\n  Conditional Random Fields",
    "authors": [
      "Ni Lao",
      "Jun Zhu"
    ],
    "summary": "Structure learning of Conditional Random Fields (CRFs) can be cast into an\nL1-regularized optimization problem. To avoid optimizing over a fully linked\nmodel, gain-based or gradient-based feature selection methods start from an\nempty model and incrementally add top ranked features to it. However, for\nhigh-dimensional problems like statistical relational learning, training time\nof these incremental methods can be dominated by the cost of evaluating the\ngain or gradient of a large collection of candidate features. In this study we\npropose a fast feature evaluation algorithm called Contrastive Feature\nInduction (CFI), which only evaluates a subset of features that involve both\nvariables with high signals (deviation from mean) and variables with high\nerrors (residue). We prove that the gradient of candidate features can be\nrepresented solely as a function of signals and errors, and that CFI is an\nefficient approximation of gradient-based evaluation methods. Experiments on\nsynthetic and real data sets show competitive learning speed and accuracy of\nCFI on pairwise CRFs, compared to state-of-the-art structure learning methods\nsuch as full optimization over all features, and Grafting.",
    "published": "2014-06-28T22:13:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unimodal Bandits without Smoothness",
    "authors": [
      "Richard Combes",
      "Alexandre Proutiere"
    ],
    "summary": "We consider stochastic bandit problems with a continuous set of arms and\nwhere the expected reward is a continuous and unimodal function of the arm. No\nfurther assumption is made regarding the smoothness and the structure of the\nexpected reward function. For these problems, we propose the Stochastic\nPentachotomy (SP) algorithm, and derive finite-time upper bounds on its regret\nand optimization error. In particular, we show that, for any expected reward\nfunction $\\mu$ that behaves as $\\mu(x)=\\mu(x^\\star)-C|x-x^\\star|^\\xi$ locally\naround its maximizer $x^\\star$ for some $\\xi, C>0$, the SP algorithm is\norder-optimal. Namely its regret and optimization error scale as\n$O(\\sqrt{T\\log(T)})$ and $O(\\sqrt{\\log(T)/T})$, respectively, when the time\nhorizon $T$ grows large. These scalings are achieved without the knowledge of\n$\\xi$ and $C$. Our algorithm is based on asymptotically optimal sequential\nstatistical tests used to successively trim an interval that contains the best\narm with high probability. To our knowledge, the SP algorithm constitutes the\nfirst sequential arm selection rule that achieves a regret and optimization\nerror scaling as $O(\\sqrt{T})$ and $O(1/\\sqrt{T})$, respectively, up to a\nlogarithmic factor for non-smooth expected reward functions, as well as for\nsmooth functions with unknown smoothness.",
    "published": "2014-06-28T23:45:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Randomized Block Coordinate Descent for Online and Stochastic\n  Optimization",
    "authors": [
      "Huahua Wang",
      "Arindam Banerjee"
    ],
    "summary": "Two types of low cost-per-iteration gradient descent methods have been\nextensively studied in parallel. One is online or stochastic gradient descent\n(OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this\npaper, we combine the two types of methods together and propose online\nrandomized block coordinate descent (ORBCD). At each iteration, ORBCD only\ncomputes the partial gradient of one block coordinate of one mini-batch\nsamples. ORBCD is well suited for the composite minimization problem where one\nfunction is the average of the losses of a large number of samples and the\nother is a simple regularizer defined on high dimensional variables. We show\nthat the iteration complexity of ORBCD has the same order as OGD or SGD. For\nstrongly convex functions, by reducing the variance of stochastic gradients, we\nshow that ORBCD can converge at a geometric rate in expectation, matching the\nconvergence rate of SGD with variance reduction and RBCD.",
    "published": "2014-07-01T05:57:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Submodular Maximization under a Matroid Constraint with\n  Application to Learning Assignments",
    "authors": [
      "Daniel Golovin",
      "Andreas Krause",
      "Matthew Streeter"
    ],
    "summary": "Which ads should we display in sponsored search in order to maximize our\nrevenue? How should we dynamically rank information sources to maximize the\nvalue of the ranking? These applications exhibit strong diminishing returns:\nRedundancy decreases the marginal utility of each ad or information source. We\nshow that these and other problems can be formalized as repeatedly selecting an\nassignment of items to positions to maximize a sequence of monotone submodular\nfunctions that arrive one by one. We present an efficient algorithm for this\ngeneral problem and analyze it in the no-regret model. Our algorithm possesses\nstrong theoretical guarantees, such as a performance ratio that converges to\nthe optimal constant of 1 - 1/e. We empirically evaluate our algorithm on two\nreal-world online optimization problems on the web: ad allocation with\nsubmodular utilities, and dynamically ranking blogs to detect information\ncascades. Finally, we present a second algorithm that handles the more general\ncase in which the feasible sets are given by a matroid constraint, while still\nmaintaining a 1 - 1/e asymptotic performance ratio.",
    "published": "2014-07-03T23:06:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large-Scale Multi-Label Learning with Incomplete Label Assignments",
    "authors": [
      "Xiangnan Kong",
      "Zhaoming Wu",
      "Li-Jia Li",
      "Ruofei Zhang",
      "Philip S. Yu",
      "Hang Wu",
      "Wei Fan"
    ],
    "summary": "Multi-label learning deals with the classification problems where each\ninstance can be assigned with multiple labels simultaneously. Conventional\nmulti-label learning approaches mainly focus on exploiting label correlations.\nIt is usually assumed, explicitly or implicitly, that the label sets for\ntraining instances are fully labeled without any missing labels. However, in\nmany real-world multi-label datasets, the label assignments for training\ninstances can be incomplete. Some ground-truth labels can be missed by the\nlabeler from the label set. This problem is especially typical when the number\ninstances is very large, and the labeling cost is very high, which makes it\nalmost impossible to get a fully labeled training set. In this paper, we study\nthe problem of large-scale multi-label learning with incomplete label\nassignments. We propose an approach, called MPU, based upon positive and\nunlabeled stochastic gradient descent and stacked models. Unlike prior works,\nour method can effectively and efficiently consider missing labels and label\ncorrelations simultaneously, and is very scalable, that has linear time\ncomplexities over the size of the data. Extensive experiments on two real-world\nmulti-label datasets show that our MPU model consistently outperform other\ncommonly-used baselines.",
    "published": "2014-07-06T20:13:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Deep Structured Models",
    "authors": [
      "Liang-Chieh Chen",
      "Alexander G. Schwing",
      "Alan L. Yuille",
      "Raquel Urtasun"
    ],
    "summary": "Many problems in real-world applications involve predicting several random\nvariables which are statistically related. Markov random fields (MRFs) are a\ngreat mathematical tool to encode such relationships. The goal of this paper is\nto combine MRFs with deep learning algorithms to estimate complex\nrepresentations while taking into account the dependencies between the output\nrandom variables. Towards this goal, we propose a training algorithm that is\nable to learn structured models jointly with deep features that form the MRF\npotentials. Our approach is efficient as it blends learning and inference and\nmakes use of GPU acceleration. We demonstrate the effectiveness of our\nalgorithm in the tasks of predicting words from noisy images, as well as\nmulti-class classification of Flickr photographs. We show that joint learning\nof the deep features and the MRF parameters results in significant performance\ngains.",
    "published": "2014-07-09T15:54:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A multi-instance learning algorithm based on a stacked ensemble of lazy\n  learners",
    "authors": [
      "Ramasubramanian Sundararajan",
      "Hima Patel",
      "Manisha Srivastava"
    ],
    "summary": "This document describes a novel learning algorithm that classifies \"bags\" of\ninstances rather than individual instances. A bag is labeled positive if it\ncontains at least one positive instance (which may or may not be specifically\nidentified), and negative otherwise. This class of problems is known as\nmulti-instance learning problems, and is useful in situations where the class\nlabel at an instance level may be unavailable or imprecise or difficult to\nobtain, or in situations where the problem is naturally posed as one of\nclassifying instance groups. The algorithm described here is an ensemble-based\nmethod, wherein the members of the ensemble are lazy learning classifiers\nlearnt using the Citation Nearest Neighbour method. Diversity among the\nensemble members is achieved by optimizing their parameters using a\nmulti-objective optimization method, with the objectives being to maximize\nClass 1 accuracy and minimize false positive rate. The method has been found to\nbe effective on the Musk1 benchmark dataset.",
    "published": "2014-07-10T09:39:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Subspace Restricted Boltzmann Machine",
    "authors": [
      "Jakub M. Tomczak",
      "Adam Gonczarek"
    ],
    "summary": "The subspace Restricted Boltzmann Machine (subspaceRBM) is a third-order\nBoltzmann machine where multiplicative interactions are between one visible and\ntwo hidden units. There are two kinds of hidden units, namely, gate units and\nsubspace units. The subspace units reflect variations of a pattern in data and\nthe gate unit is responsible for activating the subspace units. Additionally,\nthe gate unit can be seen as a pooling feature. We evaluate the behavior of\nsubspaceRBM through experiments with MNIST digit recognition task, measuring\nreconstruction error and classification error.",
    "published": "2014-07-16T18:50:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A feature construction framework based on outlier detection and\n  discriminative pattern mining",
    "authors": [
      "Albrecht Zimmermann"
    ],
    "summary": "No matter the expressive power and sophistication of supervised learning\nalgorithms, their effectiveness is restricted by the features describing the\ndata. This is not a new insight in ML and many methods for feature selection,\ntransformation, and construction have been developed. But while this is\non-going for general techniques for feature selection and transformation, i.e.\ndimensionality reduction, work on feature construction, i.e. enriching the\ndata, is by now mainly the domain of image, particularly character,\nrecognition, and NLP.\n  In this work, we propose a new general framework for feature construction.\nThe need for feature construction in a data set is indicated by class outliers\nand discriminative pattern mining used to derive features on their\nk-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, and\nevaluate the usefulness of the derived features on a diverse collection of UCI\ndata sets. The derived features are more often useful than ones derived by\nDC-Fringe, and our approach is much less likely to overfit. But while a weak\nlearner, Naive Bayes, benefits strongly from the feature construction, the\neffect is less pronounced for C4.5, and almost vanishes for an SVM leaner.\n  Keywords: feature construction, classification, outlier detection",
    "published": "2014-07-17T13:51:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploiting Smoothness in Statistical Learning, Sequential Prediction,\n  and Stochastic Optimization",
    "authors": [
      "Mehrdad Mahdavi"
    ],
    "summary": "In the last several years, the intimate connection between convex\noptimization and learning problems, in both statistical and sequential\nframeworks, has shifted the focus of algorithmic machine learning to examine\nthis interplay. In particular, on one hand, this intertwinement brings forward\nnew challenges in reassessment of the performance of learning algorithms\nincluding generalization and regret bounds under the assumptions imposed by\nconvexity such as analytical properties of loss functions (e.g., Lipschitzness,\nstrong convexity, and smoothness). On the other hand, emergence of datasets of\nan unprecedented size, demands the development of novel and more efficient\noptimization algorithms to tackle large-scale learning problems.\n  The overarching goal of this thesis is to reassess the smoothness of loss\nfunctions in statistical learning, sequential prediction/online learning, and\nstochastic optimization and explicate its consequences. In particular we\nexamine how smoothness of loss function could be beneficial or detrimental in\nthese settings in terms of sample complexity, statistical consistency, regret\nanalysis, and convergence rate, and investigate how smoothness can be leveraged\nto devise more efficient learning algorithms.",
    "published": "2014-07-19T15:16:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Fast Synchronization Clustering Algorithm",
    "authors": [
      "Xinquan Chen"
    ],
    "summary": "This paper presents a Fast Synchronization Clustering algorithm (FSynC),\nwhich is an improved version of SynC algorithm. In order to decrease the time\ncomplexity of the original SynC algorithm, we combine grid cell partitioning\nmethod and Red-Black tree to construct the near neighbor point set of every\npoint. By simulated experiments of some artificial data sets and several real\ndata sets, we observe that FSynC algorithm can often get less time than SynC\nalgorithm for many kinds of data sets. At last, it gives some research\nexpectations to popularize this algorithm.",
    "published": "2014-07-23T09:14:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Chasing Ghosts: Competing with Stateful Policies",
    "authors": [
      "Uriel Feige",
      "Tomer Koren",
      "Moshe Tennenholtz"
    ],
    "summary": "We consider sequential decision making in a setting where regret is measured\nwith respect to a set of stateful reference policies, and feedback is limited\nto observing the rewards of the actions performed (the so called \"bandit\"\nsetting). If either the reference policies are stateless rather than stateful,\nor the feedback includes the rewards of all actions (the so called \"expert\"\nsetting), previous work shows that the optimal regret grows like\n$\\Theta(\\sqrt{T})$ in terms of the number of decision rounds $T$.\n  The difficulty in our setting is that the decision maker unavoidably loses\ntrack of the internal states of the reference policies, and thus cannot\nreliably attribute rewards observed in a certain round to any of the reference\npolicies. In fact, in this setting it is impossible for the algorithm to\nestimate which policy gives the highest (or even approximately highest) total\nreward. Nevertheless, we design an algorithm that achieves expected regret that\nis sublinear in $T$, of the form $O( T/\\log^{1/4}{T})$. Our algorithm is based\non a certain local repetition lemma that may be of independent interest. We\nalso show that no algorithm can guarantee expected regret better than $O(\nT/\\log^{3/2} T)$.",
    "published": "2014-07-29T06:17:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Hash-based Co-Clustering Algorithm for Categorical Data",
    "authors": [
      "Fabricio Olivetti de França"
    ],
    "summary": "Many real-life data are described by categorical attributes without a\npre-classification. A common data mining method used to extract information\nfrom this type of data is clustering. This method group together the samples\nfrom the data that are more similar than all other samples. But, categorical\ndata pose a challenge when extracting information because: the calculation of\ntwo objects similarity is usually done by measuring the number of common\nfeatures, but ignore a possible importance weighting; if the data may be\ndivided differently according to different subsets of the features, the\nalgorithm may find clusters with different meanings from each other,\ndifficulting the post analysis. Data Co-Clustering of categorical data is the\ntechnique that tries to find subsets of samples that share a subset of features\nin common. By doing so, not only a sample may belong to more than one cluster\nbut, the feature selection of each cluster describe its own characteristics. In\nthis paper a novel Co-Clustering technique for categorical data is proposed by\nusing Locality Sensitive Hashing technique in order to preprocess a list of\nCo-Clusters seeds based on a previous research. Results indicate this technique\nis capable of finding high quality Co-Clusters in many different categorical\ndata sets and scales linearly with the data set size.",
    "published": "2014-07-29T15:23:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "How Auto-Encoders Could Provide Credit Assignment in Deep Networks via\n  Target Propagation",
    "authors": [
      "Yoshua Bengio"
    ],
    "summary": "We propose to exploit {\\em reconstruction} as a layer-local training signal\nfor deep learning. Reconstructions can be propagated in a form of target\npropagation playing a role similar to back-propagation but helping to reduce\nthe reliance on derivatives in order to perform credit assignment across many\nlevels of possibly strong non-linearities (which is difficult for\nback-propagation). A regularized auto-encoder tends produce a reconstruction\nthat is a more likely version of its input, i.e., a small move in the direction\nof higher likelihood. By generalizing gradients, target propagation may also\nallow to train deep networks with discrete hidden units. If the auto-encoder\ntakes both a representation of input and target (or of any side information) in\ninput, then its reconstruction of input representation provides a target\ntowards a representation that is more likely, conditioned on all the side\ninformation. A deep auto-encoder decoding path generalizes gradient propagation\nin a learned way that can could thus handle not just infinitesimal changes but\nlarger, discrete changes, hopefully allowing credit assignment through a long\nchain of non-linear operations. In addition to each layer being a good\nauto-encoder, the encoder also learns to please the upper layers by\ntransforming the data into a space where it is easier to model by them,\nflattening manifolds and disentangling factors. The motivations and theoretical\njustifications for this approach are laid down in this paper, along with\nconjectures that will have to be verified either mathematically or\nexperimentally, including a hypothesis stating that such auto-encoder mediated\ntarget propagation could play in brains the role of credit assignment through\nmany non-linear, noisy and discrete transformations.",
    "published": "2014-07-29T23:32:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "DuSK: A Dual Structure-preserving Kernel for Supervised Tensor Learning\n  with Applications to Neuroimages",
    "authors": [
      "Lifang He",
      "Xiangnan Kong",
      "Philip S. Yu",
      "Ann B. Ragin",
      "Zhifeng Hao",
      "Xiaowei Yang"
    ],
    "summary": "With advances in data collection technologies, tensor data is assuming\nincreasing prominence in many applications and the problem of supervised tensor\nlearning has emerged as a topic of critical significance in the data mining and\nmachine learning community. Conventional methods for supervised tensor learning\nmainly focus on learning kernels by flattening the tensor into vectors or\nmatrices, however structural information within the tensors will be lost. In\nthis paper, we introduce a new scheme to design structure-preserving kernels\nfor supervised tensor learning. Specifically, we demonstrate how to leverage\nthe naturally available structure within the tensorial representation to encode\nprior knowledge in the kernel. We proposed a tensor kernel that can preserve\ntensor structures based upon dual-tensorial mapping. The dual-tensorial mapping\nfunction can map each tensor instance in the input space to another tensor in\nthe feature space while preserving the tensorial structure. Theoretically, our\napproach is an extension of the conventional kernels in the vector space to\ntensor space. We applied our novel kernel in conjunction with SVM to real-world\ntensor classification problems including brain fMRI classification for three\ndifferent diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV).\nExtensive empirical studies demonstrate that our proposed approach can\neffectively boost tensor classification performances, particularly with small\nsample sizes.",
    "published": "2014-07-31T06:33:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically\n  Triggered Arms",
    "authors": [
      "Wei Chen",
      "Yajun Wang",
      "Yang Yuan",
      "Qinshi Wang"
    ],
    "summary": "We define a general framework for a large class of combinatorial multi-armed\nbandit (CMAB) problems, where subsets of base arms with unknown distributions\nform super arms. In each round, a super arm is played and the base arms\ncontained in the super arm are played and their outcomes are observed. We\nfurther consider the extension in which more based arms could be\nprobabilistically triggered based on the outcomes of already triggered arms.\nThe reward of the super arm depends on the outcomes of all played arms, and it\nonly needs to satisfy two mild assumptions, which allow a large class of\nnonlinear reward instances. We assume the availability of an offline\n(\\alpha,\\beta)-approximation oracle that takes the means of the outcome\ndistributions of arms and outputs a super arm that with probability {\\beta}\ngenerates an {\\alpha} fraction of the optimal expected reward. The objective of\nan online learning algorithm for CMAB is to minimize\n(\\alpha,\\beta)-approximation regret, which is the difference between the\n\\alpha{\\beta} fraction of the expected reward when always playing the optimal\nsuper arm, and the expected reward of playing super arms according to the\nalgorithm. We provide CUCB algorithm that achieves O(log n)\ndistribution-dependent regret, where n is the number of rounds played, and we\nfurther provide distribution-independent bounds for a large class of reward\nfunctions. Our regret analysis is tight in that it matches the bound of UCB1\nalgorithm (up to a constant factor) for the classical MAB problem, and it\nsignificantly improves the regret bound in a earlier paper on combinatorial\nbandits with linear rewards. We apply our CMAB framework to two new\napplications, probabilistic maximum coverage and social influence maximization,\nboth having nonlinear reward structures. In particular, application to social\ninfluence maximization requires our extension on probabilistically triggered\narms.",
    "published": "2014-07-31T10:09:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Consistency of Ordinal Regression Methods",
    "authors": [
      "Fabian Pedregosa",
      "Francis Bach",
      "Alexandre Gramfort"
    ],
    "summary": "Many of the ordinal regression models that have been proposed in the\nliterature can be seen as methods that minimize a convex surrogate of the\nzero-one, absolute, or squared loss functions. A key property that allows to\nstudy the statistical implications of such approximations is that of Fisher\nconsistency. Fisher consistency is a desirable property for surrogate loss\nfunctions and implies that in the population setting, i.e., if the probability\ndistribution that generates the data were available, then optimization of the\nsurrogate would yield the best possible model. In this paper we will\ncharacterize the Fisher consistency of a rich family of surrogate loss\nfunctions used in the context of ordinal regression, including support vector\nordinal regression, ORBoosting and least absolute deviation. We will see that,\nfor a family of surrogate loss functions that subsumes support vector ordinal\nregression and ORBoosting, consistency can be fully characterized by the\nderivative of a real-valued function at zero, as happens for convex\nmargin-based surrogates in binary classification. We also derive excess risk\nbounds for a surrogate of the absolute error that generalize existing risk\nbounds for binary classification. Finally, our analysis suggests a novel\nsurrogate of the squared error loss. We compare this novel surrogate with\ncompeting approaches on 9 different datasets. Our method shows to be highly\ncompetitive in practice, outperforming the least squares loss on 7 out of 9\ndatasets.",
    "published": "2014-08-11T06:52:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Complexity of Bandit Linear Optimization",
    "authors": [
      "Ohad Shamir"
    ],
    "summary": "We study the attainable regret for online linear optimization problems with\nbandit feedback, where unlike the full-information setting, the player can only\nobserve its own loss rather than the full loss vector. We show that the price\nof bandit information in this setting can be as large as $d$, disproving the\nwell-known conjecture that the regret for bandit linear optimization is at most\n$\\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using\n\"trivial\" modifications of standard domains, which have no effect in the\nfull-information setting. This and other results we present highlight some\ninteresting differences between full-information and bandit learning, which\nwere not considered in previous literature.",
    "published": "2014-08-11T10:40:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning a hyperplane classifier by minimizing an exact bound on the VC\n  dimension",
    "authors": [
      "Jayadeva"
    ],
    "summary": "The VC dimension measures the capacity of a learning machine, and a low VC\ndimension leads to good generalization. While SVMs produce state-of-the-art\nlearning performance, it is well known that the VC dimension of a SVM can be\nunbounded; despite good results in practice, there is no guarantee of good\ngeneralization. In this paper, we show how to learn a hyperplane classifier by\nminimizing an exact, or \\boldmath{$\\Theta$} bound on its VC dimension. The\nproposed approach, termed as the Minimal Complexity Machine (MCM), involves\nsolving a simple linear programming problem. Experimental results show, that on\na number of benchmark datasets, the proposed approach learns classifiers with\nerror rates much less than conventional SVMs, while often using fewer support\nvectors. On many benchmark datasets, the number of support vectors is less than\none-tenth the number used by SVMs, indicating that the MCM does indeed learn\nsimpler representations.",
    "published": "2014-08-12T18:57:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Robust OS-ELM with a novel selective ensemble based on particle swarm\n  optimization",
    "authors": [
      "Yang Liu",
      "Bo He",
      "Diya Dong",
      "Yue Shen",
      "Tianhong Yan",
      "Rui Nian",
      "Amaury Lendase"
    ],
    "summary": "In this paper, a robust online sequential extreme learning machine (ROS-ELM)\nis proposed. It is based on the original OS-ELM with an adaptive selective\nensemble framework. Two novel insights are proposed in this paper. First, a\nnovel selective ensemble algorithm referred to as particle swarm optimization\nselective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a general\nselective ensemble method which is applicable to any learning algorithms,\nincluding batch learning and online learning. Second, an adaptive selective\nensemble framework for online learning is designed to balance the robustness\nand complexity of the algorithm. Experiments for both regression and\nclassification problems with UCI data sets are carried out. Comparisons between\nOS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empirically\nshow that ROS-ELM significantly improves the robustness and stability.",
    "published": "2014-08-13T00:41:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Linear Contour Learning: A Method for Supervised Dimension Reduction",
    "authors": [
      "Bing Li",
      "Hongyuan Zha",
      "Francesca Chiaromonte"
    ],
    "summary": "We propose a novel approach to sufficient dimension reduction in regression,\nbased on estimating contour directions of negligible variation for the response\nsurface. These directions span the orthogonal complement of the minimal space\nrelevant for the regression, and can be extracted according to a measure of the\nvariation in the response, leading to General Contour Regression(GCR). In\ncomparison to exiisting sufficient dimension reduction techniques, this\nsontour-based mothology guarantees exhaustive estimation of the central space\nunder ellipticity of the predictoor distribution and very mild additional\nassumptions, while maintaining vn-consisytency and somputational ease.\nMoreover, it proves to be robust to departures from ellipticity. We also\nestablish some useful population properties for GCR. Simulations to compare\nperformance with that of standard techniques such as ordinary least squares,\nsliced inverse regression, principal hessian directions, and sliced average\nvariance estimation confirm the advntages anticipated by theoretical analyses.\nWe also demonstrate the use of contour-based methods on a data set concerning\ngrades of students from Massachusetts colleges.",
    "published": "2014-08-13T05:14:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-Sensor Event Detection using Shape Histograms",
    "authors": [
      "Ehtesham Hassan",
      "Gautam Shroff",
      "Puneet Agarwal"
    ],
    "summary": "Vehicular sensor data consists of multiple time-series arising from a number\nof sensors. Using such multi-sensor data we would like to detect occurrences of\nspecific events that vehicles encounter, e.g., corresponding to particular\nmaneuvers that a vehicle makes or conditions that it encounters. Events are\ncharacterized by similar waveform patterns re-appearing within one or more\nsensors. Further such patterns can be of variable duration. In this work, we\npropose a method for detecting such events in time-series data using a novel\nfeature descriptor motivated by similar ideas in image processing. We define\nthe shape histogram: a constant dimension descriptor that nevertheless captures\npatterns of variable duration. We demonstrate the efficacy of using shape\nhistograms as features to detect events in an SVM-based, multi-sensor,\nsupervised learning scenario, i.e., multiple time-series are used to detect an\nevent. We present results on real-life vehicular sensor data and show that our\ntechnique performs better than available pattern detection implementations on\nour data, and that it can also be used to combine features from multiple\nsensors resulting in better accuracy than using any single sensor. Since\nprevious work on pattern detection in time-series has been in the single series\ncontext, we also present results using our technique on multiple standard\ntime-series datasets and show that it is the most versatile in terms of how it\nranks compared to other published results.",
    "published": "2014-08-16T11:11:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "AFP Algorithm and a Canonical Normal Form for Horn Formulas",
    "authors": [
      "Ruhollah Majdoddin"
    ],
    "summary": "AFP Algorithm is a learning algorithm for Horn formulas. We show that it does\nnot improve the complexity of AFP Algorithm, if after each negative\ncounterexample more that just one refinements are performed. Moreover, a\ncanonical normal form for Horn formulas is presented, and it is proved that the\noutput formula of AFP Algorithm is in this normal form.",
    "published": "2014-08-20T14:29:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Conic Multi-Task Classification",
    "authors": [
      "Cong Li",
      "Michael Georgiopoulos",
      "Georgios C. Anagnostopoulos"
    ],
    "summary": "Traditionally, Multi-task Learning (MTL) models optimize the average of\ntask-related objective functions, which is an intuitive approach and which we\nwill be referring to as Average MTL. However, a more general framework,\nreferred to as Conic MTL, can be formulated by considering conic combinations\nof the objective functions instead; in this framework, Average MTL arises as a\nspecial case, when all combination coefficients equal 1. Although the advantage\nof Conic MTL over Average MTL has been shown experimentally in previous works,\nno theoretical justification has been provided to date. In this paper, we\nderive a generalization bound for the Conic MTL method, and demonstrate that\nthe tightest bound is not necessarily achieved, when all combination\ncoefficients equal 1; hence, Average MTL may not always be the optimal choice,\nand it is important to consider Conic MTL. As a byproduct of the generalization\nbound, it also theoretically explains the good experimental results of previous\nrelevant works. Finally, we propose a new Conic MTL model, whose conic\ncombination coefficients minimize the generalization bound, instead of choosing\nthem heuristically as has been done in previous methods. The rationale and\nadvantage of our model is demonstrated and verified via a series of experiments\nby comparing with several other methods.",
    "published": "2014-08-20T16:23:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improved Distributed Principal Component Analysis",
    "authors": [
      "Maria-Florina Balcan",
      "Vandana Kanchanapally",
      "Yingyu Liang",
      "David Woodruff"
    ],
    "summary": "We study the distributed computing setting in which there are multiple\nservers, each holding a set of points, who wish to compute functions on the\nunion of their point sets. A key task in this setting is Principal Component\nAnalysis (PCA), in which the servers would like to compute a low dimensional\nsubspace capturing as much of the variance of the union of their point sets as\npossible. Given a procedure for approximate PCA, one can use it to\napproximately solve $\\ell_2$-error fitting problems such as $k$-means\nclustering and subspace clustering. The essential properties of an approximate\ndistributed PCA algorithm are its communication cost and computational\nefficiency for a given desired accuracy in downstream applications. We give new\nalgorithms and analyses for distributed PCA which lead to improved\ncommunication and computational costs for $k$-means clustering and related\nproblems. Our empirical study on real world data shows a speedup of orders of\nmagnitude, preserving communication with only a negligible degradation in\nsolution quality. Some of these techniques we develop, such as a general\ntransformation from a constant success probability subspace embedding to a high\nsuccess probability subspace embedding with a dimension and sparsity\nindependent of the success probability, may be of independent interest.",
    "published": "2014-08-25T16:24:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Label Distribution Learning",
    "authors": [
      "Xin Geng"
    ],
    "summary": "Although multi-label learning can deal with many problems with label\nambiguity, it does not fit some real applications well where the overall\ndistribution of the importance of the labels matters. This paper proposes a\nnovel learning paradigm named \\emph{label distribution learning} (LDL) for such\nkind of applications. The label distribution covers a certain number of labels,\nrepresenting the degree to which each label describes the instance. LDL is a\nmore general learning framework which includes both single-label and\nmulti-label learning as its special cases. This paper proposes six working LDL\nalgorithms in three ways: problem transformation, algorithm adaptation, and\nspecialized algorithm design. In order to compare the performance of the LDL\nalgorithms, six representative and diverse evaluation measures are selected via\na clustering analysis, and the first batch of label distribution datasets are\ncollected and made publicly available. Experimental results on one artificial\nand fifteen real-world datasets show clear advantages of the specialized\nalgorithms, which indicates the importance of special design for the\ncharacteristics of the LDL problem.",
    "published": "2014-08-26T06:48:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large Scale Purchase Prediction with Historical User Actions on B2C\n  Online Retail Platform",
    "authors": [
      "Yuyu Zhang",
      "Liang Pang",
      "Lei Shi",
      "Bin Wang"
    ],
    "summary": "This paper describes the solution of Bazinga Team for Tmall Recommendation\nPrize 2014. With real-world user action data provided by Tmall, one of the\nlargest B2C online retail platforms in China, this competition requires to\npredict future user purchases on Tmall website. Predictions are judged on\nF1Score, which considers both precision and recall for fair evaluation. The\ndata set provided by Tmall contains more than half billion action records from\nover ten million distinct users. Such massive data volume poses a big\nchallenge, and drives competitors to write every single program in MapReduce\nfashion and run it on distributed cluster. We model the purchase prediction\nproblem as standard machine learning problem, and mainly employ regression and\nclassification methods as single models. Individual models are then aggregated\nin a two-stage approach, using linear regression for blending, and finally a\nlinear ensemble of blended models. The competition is approaching the end but\nstill in running during writing this paper. In the end, our team achieves\nF1Score 6.11 and ranks 7th (out of 7,276 teams in total).",
    "published": "2014-08-27T06:32:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Task-group Relatedness and Generalization Bounds for Regularized\n  Multi-task Learning",
    "authors": [
      "Chao Zhang",
      "Dacheng Tao",
      "Tao Hu",
      "Xiang Li"
    ],
    "summary": "In this paper, we study the generalization performance of regularized\nmulti-task learning (RMTL) in a vector-valued framework, where MTL is\nconsidered as a learning process for vector-valued functions. We are mainly\nconcerned with two theoretical questions: 1) under what conditions does RMTL\nperform better with a smaller task sample size than STL? 2) under what\nconditions is RMTL generalizable and can guarantee the consistency of each task\nduring simultaneous learning?\n  In particular, we investigate two types of task-group relatedness: the\nobserved discrepancy-dependence measure (ODDM) and the empirical\ndiscrepancy-dependence measure (EDDM), both of which detect the dependence\nbetween two groups of multiple related tasks (MRTs). We then introduce the\nCartesian product-based uniform entropy number (CPUEN) to measure the\ncomplexities of vector-valued function classes. By applying the specific\ndeviation and the symmetrization inequalities to the vector-valued framework,\nwe obtain the generalization bound for RMTL, which is the upper bound of the\njoint probability of the event that there is at least one task with a large\nempirical discrepancy between the expected and empirical risks. Finally, we\npresent a sufficient condition to guarantee the consistency of each task in the\nsimultaneous learning process, and we discuss how task relatedness affects the\ngeneralization performance of RMTL. Our theoretical findings answer the\naforementioned two questions.",
    "published": "2014-08-28T03:27:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training\n  Structural SVMs with a Costly max-Oracle",
    "authors": [
      "Neel Shah",
      "Vladimir Kolmogorov",
      "Christoph H. Lampert"
    ],
    "summary": "Structural support vector machines (SSVMs) are amongst the best performing\nmodels for structured computer vision tasks, such as semantic image\nsegmentation or human pose estimation. Training SSVMs, however, is\ncomputationally costly, because it requires repeated calls to a structured\nprediction subroutine (called \\emph{max-oracle}), which has to solve an\noptimization problem itself, e.g. a graph cut.\n  In this work, we introduce a new algorithm for SSVM training that is more\nefficient than earlier techniques when the max-oracle is computationally\nexpensive, as it is frequently the case in computer vision tasks. The main idea\nis to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm\nwith efficient hyperplane caching, and (ii) use an automatic selection rule for\ndeciding whether to call the exact max-oracle or to rely on an approximate one\nbased on the cached hyperplanes.\n  We show experimentally that this strategy leads to faster convergence to the\noptimum with respect to the number of requires oracle calls, and that this\ntranslates into faster convergence with respect to the total runtime when the\nmax-oracle is slow compared to the other steps of the algorithm.\n  A publicly available C++ implementation is provided at\nhttp://pub.ist.ac.at/~vnk/papers/SVM.html .",
    "published": "2014-08-28T18:38:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Data classification using the Dempster-Shafer method",
    "authors": [
      "Qi Chen",
      "Amanda Whitbrook",
      "Uwe Aickelin",
      "Chris Roadknight"
    ],
    "summary": "In this paper, the Dempster-Shafer method is employed as the theoretical\nbasis for creating data classification systems. Testing is carried out using\nthree popular (multiple attribute) benchmark datasets that have two, three and\nfour classes. In each case, a subset of the available data is used for training\nto establish thresholds, limits or likelihoods of class membership for each\nattribute, and hence create mass functions that establish probability of class\nmembership for each attribute of the test data. Classification of each data\nitem is achieved by combination of these probabilities via Dempster's Rule of\nCombination. Results for the first two datasets show extremely high\nclassification accuracy that is competitive with other popular methods. The\nthird dataset is non-numerical and difficult to classify, but good results can\nbe achieved provided the system and mass functions are designed carefully and\nthe right attributes are chosen for combination. In all cases the\nDempster-Shafer method provides comparable performance to other more popular\nalgorithms, but the overhead of generating accurate mass functions increases\nthe complexity with the addition of new attributes. Overall, the results\nsuggest that the D-S approach provides a suitable framework for the design of\nclassification systems and that automating the mass function design and\ncalculation would increase the viability of the algorithm for complex\nclassification problems.",
    "published": "2014-09-02T15:49:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an\n  Ensemble Learning Approach",
    "authors": [
      "Ahmad Basheer Hassanat",
      "Mohammad Ali Abbadi",
      "Ghada Awad Altarawneh",
      "Ahmad Ali Alhasanat"
    ],
    "summary": "This paper presents a new solution for choosing the K parameter in the\nk-nearest neighbor (KNN) algorithm, the solution depending on the idea of\nensemble learning, in which a weak KNN classifier is used each time with a\ndifferent K, starting from one to the square root of the size of the training\nset. The results of the weak classifiers are combined using the weighted sum\nrule. The proposed solution was tested and compared to other solutions using a\ngroup of experiments in real life problems. The experimental results show that\nthe proposed classifier outperforms the traditional KNN classifier that uses a\ndifferent number of neighbors, is competitive with other classifiers, and is a\npromising classifier with strong potential for a wide range of applications.",
    "published": "2014-09-02T23:28:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dimensionality Invariant Similarity Measure",
    "authors": [
      "Ahmad Basheer Hassanat"
    ],
    "summary": "This paper presents a new similarity measure to be used for general tasks\nincluding supervised learning, which is represented by the K-nearest neighbor\nclassifier (KNN). The proposed similarity measure is invariant to large\ndifferences in some dimensions in the feature space. The proposed metric is\nproved mathematically to be a metric. To test its viability for different\napplications, the KNN used the proposed metric for classifying test examples\nchosen from a number of real datasets. Compared to some other well known\nmetrics, the experimental results show that the proposed metric is a promising\ndistance measure for the KNN classifier with strong potential for a wide range\nof applications.",
    "published": "2014-09-02T23:45:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Domain Transfer Structured Output Learning",
    "authors": [
      "Jim Jing-Yan Wang"
    ],
    "summary": "In this paper, we propose the problem of domain transfer structured output\nlearn- ing and the first solution to solve it. The problem is defined on two\ndifferent data domains sharing the same input and output spaces, named as\nsource domain and target domain. The outputs are structured, and for the data\nsamples of the source domain, the corresponding outputs are available, while\nfor most data samples of the target domain, the corresponding outputs are\nmissing. The input distributions of the two domains are significantly\ndifferent. The problem is to learn a predictor for the target domain to predict\nthe structured outputs from the input. Due to the limited number of outputs\navailable for the samples form the target domain, it is difficult to directly\nlearn the predictor from the target domain, thus it is necessary to use the\noutput information available in source domain. We propose to learn the target\ndomain predictor by adapting a auxiliary predictor trained by using source\ndomain data to the target domain. The adaptation is implemented by adding a\ndelta function on the basis of the auxiliary predictor. An algorithm is\ndeveloped to learn the parameter of the delta function to minimize loss\nfunctions associat- ed with the predicted outputs against the true outputs of\nthe data samples with available outputs of the target domain.",
    "published": "2014-09-03T19:18:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Novel Methods for Activity Classification and Occupany Prediction\n  Enabling Fine-grained HVAC Control",
    "authors": [
      "Rajib Rana",
      "Brano Kusy",
      "Josh Wall",
      "Wen Hu"
    ],
    "summary": "Much of the energy consumption in buildings is due to HVAC systems, which has\nmotivated several recent studies on making these systems more energy-\nefficient. Occupancy and activity are two important aspects, which need to be\ncorrectly estimated for optimal HVAC control. However, state-of-the-art methods\nto estimate occupancy and classify activity require infrastructure and/or\nwearable sensors which suffers from lower acceptability due to higher cost.\nEncouragingly, with the advancement of the smartphones, these are becoming more\nachievable. Most of the existing occupancy estimation tech- niques have the\nunderlying assumption that the phone is always carried by its user. However,\nphones are often left at desk while attending meeting or other events, which\ngenerates estimation error for the existing phone based occupancy algorithms.\nSimilarly, in the recent days the emerging theory of Sparse Random Classifier\n(SRC) has been applied for activity classification on smartphone, however,\nthere are rooms to improve the on-phone process- ing. We propose a novel sensor\nfusion method which offers almost 100% accuracy for occupancy estimation. We\nalso propose an activity classifica- tion algorithm, which offers similar\naccuracy as of the state-of-the-art SRC algorithms while offering 50% reduction\nin processing.",
    "published": "2014-09-05T16:17:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Non-Convex Boosting Overcomes Random Label Noise",
    "authors": [
      "Sunsern Cheamanunkul",
      "Evan Ettinger",
      "Yoav Freund"
    ],
    "summary": "The sensitivity of Adaboost to random label noise is a well-studied problem.\nLogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be\nless sensitive to noise than AdaBoost. We present the results of experiments\nevaluating these algorithms on both synthetic and real datasets. We compare the\nperformance on each of datasets when the labels are corrupted by different\nlevels of independent label noise. In presence of random label noise, we found\nthat BrownBoost and RobustBoost perform significantly better than AdaBoost and\nLogitBoost, while the difference between each pair of algorithms is\ninsignificant. We provide an explanation for the difference based on the margin\ndistributions of the algorithms.",
    "published": "2014-09-09T21:36:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Metric Learning for Temporal Sequence Alignment",
    "authors": [
      "Damien Garreau",
      "Rémi Lajugie",
      "Sylvain Arlot",
      "Francis Bach"
    ],
    "summary": "In this paper, we propose to learn a Mahalanobis distance to perform\nalignment of multivariate time series. The learning examples for this task are\ntime series for which the true alignment is known. We cast the alignment\nproblem as a structured prediction task, and propose realistic losses between\nalignments for which the optimization is tractable. We provide experiments on\nreal data in the audio to audio context, where we show that the learning of a\nsimilarity measure leads to improvements in the performance of the alignment\ntask. We also propose to use this metric learning framework to perform feature\nselection and, from basic audio features, build a combination of these with\nbetter performance for the alignment.",
    "published": "2014-09-10T16:10:33Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Consensus-Based Modelling using Distributed Feature Construction",
    "authors": [
      "Haimonti Dutta",
      "Ashwin Srinivasan"
    ],
    "summary": "A particularly successful role for Inductive Logic Programming (ILP) is as a\ntool for discovering useful relational features for subsequent use in a\npredictive model. Conceptually, the case for using ILP to construct relational\nfeatures rests on treating these features as functions, the automated discovery\nof which necessarily requires some form of first-order learning. Practically,\nthere are now several reports in the literature that suggest that augmenting\nany existing features with ILP-discovered relational features can substantially\nimprove the predictive power of a model. While the approach is straightforward\nenough, much still needs to be done to scale it up to explore more fully the\nspace of possible features that can be constructed by an ILP system. This is in\nprinciple, infinite and in practice, extremely large. Applications have been\nconfined to heuristic or random selections from this space. In this paper, we\naddress this computational difficulty by allowing features to be constructed in\na distributed manner. That is, there is a network of computational units, each\nof which employs an ILP engine to construct some small number of features and\nthen builds a (local) model. We then employ a consensus-based algorithm, in\nwhich neighboring nodes share information to update local models. For a\ncategory of models (those with convex loss functions), it can be shown that the\nalgorithm will result in all nodes converging to a consensus model. In\npractice, it may be slow to achieve this convergence. Nevertheless, our results\non synthetic and real datasets that suggests that in relatively short time the\n\"best\" node in the network reaches a model whose predictive accuracy is\ncomparable to that obtained using more computational effort in a\nnon-distributed setting (the best node is identified as the one whose weights\nconverge first).",
    "published": "2014-09-11T14:11:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Active Metric Learning from Relative Comparisons",
    "authors": [
      "Sicheng Xiong",
      "Rómer Rosales",
      "Yuanli Pei",
      "Xiaoli Z. Fern"
    ],
    "summary": "This work focuses on active learning of distance metrics from relative\ncomparison information. A relative comparison specifies, for a data point\ntriplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to\n$x_k$. Such constraints, when available, have been shown to be useful toward\ndefining appropriate distance metrics. In real-world applications, acquiring\nconstraints often require considerable human effort. This motivates us to study\nhow to select and query the most useful relative comparisons to achieve\neffective metric learning with minimum user effort. Given an underlying class\nconcept that is employed by the user to provide such constraints, we present an\ninformation-theoretic criterion that selects the triplet whose answer leads to\nthe highest expected gain in information about the classes of a set of\nexamples. Directly applying the proposed criterion requires examining $O(n^3)$\ntriplets with $n$ instances, which is prohibitive even for datasets of moderate\nsize. We show that a randomized selection strategy can be used to reduce the\nselection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size\nproblems. Experiments show that the proposed method consistently outperforms\ntwo baseline policies.",
    "published": "2014-09-15T04:37:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Mixtures-of-Experts Framework for Multi-Label Classification",
    "authors": [
      "Charmgil Hong",
      "Iyad Batal",
      "Milos Hauskrecht"
    ],
    "summary": "We develop a novel probabilistic approach for multi-label classification that\nis based on the mixtures-of-experts architecture combined with recently\nintroduced conditional tree-structured Bayesian networks. Our approach captures\ndifferent input-output relations from multi-label data using the efficient\ntree-structured classifiers, while the mixtures-of-experts architecture aims to\ncompensate for the tree-structured restrictions and build a more accurate\nmodel. We develop and present algorithms for learning the model from data and\nfor performing multi-label predictions on future data instances. Experiments on\nmultiple benchmark datasets demonstrate that our approach achieves highly\ncompetitive results and outperforms the existing state-of-the-art multi-label\nclassification methods.",
    "published": "2014-09-16T16:52:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Predictive Capacity of Meteorological Data - Will it rain tomorrow",
    "authors": [
      "Bilal Ahmed"
    ],
    "summary": "With the availability of high precision digital sensors and cheap storage\nmedium, it is not uncommon to find large amounts of data collected on almost\nall measurable attributes, both in nature and man-made habitats. Weather in\nparticular has been an area of keen interest for researchers to develop more\naccurate and reliable prediction models. This paper presents a set of\nexperiments which involve the use of prevalent machine learning techniques to\nbuild models to predict the day of the week given the weather data for that\nparticular day i.e. temperature, wind, rain etc., and test their reliability\nacross four cities in Australia {Brisbane, Adelaide, Perth, Hobart}. The\nresults provide a comparison of accuracy of these machine learning techniques\nand their reliability to predict the day of the week by analysing the weather\ndata. We then apply the models to predict weather conditions based on the\navailable data.",
    "published": "2014-09-16T13:09:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning and approximation capability of orthogonal super greedy\n  algorithm",
    "authors": [
      "Jian Fang",
      "Shaobo Lin",
      "Zongben Xu"
    ],
    "summary": "We consider the approximation capability of orthogonal super greedy\nalgorithms (OSGA) and its applications in supervised learning. OSGA is\nconcerned with selecting more than one atoms in each iteration step, which, of\ncourse, greatly reduces the computational burden when compared with the\nconventional orthogonal greedy algorithm (OGA). We prove that even for function\nclasses that are not the convex hull of the dictionary, OSGA does not degrade\nthe approximation capability of OGA provided the dictionary is incoherent.\nBased on this, we deduce a tight generalization error bound for OSGA learning.\nOur results show that in the realm of supervised learning, OSGA provides a\npossibility to further reduce the computational burden of OGA in the premise of\nmaintaining its prominent generalization capability.",
    "published": "2014-09-18T15:09:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Feature Group Sequencing for Anytime Linear Prediction",
    "authors": [
      "Hanzhang Hu",
      "Alexander Grubb",
      "J. Andrew Bagnell",
      "Martial Hebert"
    ],
    "summary": "We consider \\textit{anytime} linear prediction in the common machine learning\nsetting, where features are in groups that have costs. We achieve anytime (or\ninterruptible) predictions by sequencing the computation of feature groups and\nreporting results using the computed features at interruption. We extend\nOrthogonal Matching Pursuit (OMP) and Forward Regression (FR) to learn the\nsequencing greedily under this group setting with costs. We theoretically\nguarantee that our algorithms achieve near-optimal linear predictions at each\nbudget when a feature group is chosen. With a novel analysis of OMP, we improve\nits theoretical bound to the same strength as that of FR. In addition, we\ndevelop a novel algorithm that consumes cost $4B$ to approximate the optimal\nperformance of \\textit{any} cost $B$, and prove that with cost less than $4B$,\nsuch an approximation is impossible. To our knowledge, these are the first\nanytime bounds at \\textit{all} budgets. We test our algorithms on two\nreal-world data-sets and evaluate them in terms of anytime linear prediction\nperformance against cost-weighted Group Lasso and alternative greedy\nalgorithms.",
    "published": "2014-09-19T00:37:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey on Soft Subspace Clustering",
    "authors": [
      "Zhaohong Deng",
      "Kup-Sze Choi",
      "Yizhang Jiang",
      "Jun Wang",
      "Shitong Wang"
    ],
    "summary": "Subspace clustering (SC) is a promising clustering technology to identify\nclusters based on their associations with subspaces in high dimensional spaces.\nSC can be classified into hard subspace clustering (HSC) and soft subspace\nclustering (SSC). While HSC algorithms have been extensively studied and well\naccepted by the scientific community, SSC algorithms are relatively new but\ngaining more attention in recent years due to better adaptability. In the\npaper, a comprehensive survey on existing SSC algorithms and the recent\ndevelopment are presented. The SSC algorithms are classified systematically\ninto three main categories, namely, conventional SSC (CSSC), independent SSC\n(ISSC) and extended SSC (XSSC). The characteristics of these algorithms are\nhighlighted and the potential future development of SSC is also discussed.",
    "published": "2014-09-19T12:01:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Transfer Prototype-based Fuzzy Clustering",
    "authors": [
      "Zhaohong Deng",
      "Yizhang Jiang",
      "Fu-Lai Chung",
      "Hisao Ishibuchi",
      "Kup-Sze Choi",
      "Shitong Wang"
    ],
    "summary": "The traditional prototype based clustering methods, such as the well-known\nfuzzy c-mean (FCM) algorithm, usually need sufficient data to find a good\nclustering partition. If the available data is limited or scarce, most of the\nexisting prototype based clustering algorithms will no longer be effective.\nWhile the data for the current clustering task may be scarce, there is usually\nsome useful knowledge available in the related scenes/domains. In this study,\nthe concept of transfer learning is applied to prototype based fuzzy clustering\n(PFC). Specifically, the idea of leveraging knowledge from the source domain is\nexploited to develop a set of transfer prototype based fuzzy clustering (TPFC)\nalgorithms. Three prototype based fuzzy clustering algorithms, namely, FCM,\nfuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have been\nchosen to incorporate with knowledge leveraging mechanism to develop the\ncorresponding transfer clustering algorithms. Novel objective functions are\nproposed to integrate the knowledge of source domain with the data of target\ndomain for clustering in the target domain. The proposed algorithms have been\nvalidated on different synthetic and real-world datasets and the results\ndemonstrate their effectiveness when compared with both the original prototype\nbased fuzzy clustering algorithms and the related clustering algorithms like\nmulti-task clustering and co-clustering.",
    "published": "2014-09-19T14:58:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Information Theoretically Efficient Model (ITEM): A model for\n  computerized analysis of large datasets",
    "authors": [
      "Tyler Ward"
    ],
    "summary": "This document discusses the Information Theoretically Efficient Model (ITEM),\na computerized system to generate an information theoretically efficient\nmultinomial logistic regression from a general dataset. More specifically, this\nmodel is designed to succeed even where the logit transform of the dependent\nvariable is not necessarily linear in the independent variables. This research\nshows that for large datasets, the resulting models can be produced on modern\ncomputers in a tractable amount of time. These models are also resistant to\noverfitting, and as such they tend to produce interpretable models with only a\nlimited number of features, all of which are designed to be well behaved.",
    "published": "2014-09-22T03:39:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Best-Arm Identification in Linear Bandits",
    "authors": [
      "Marta Soare",
      "Alessandro Lazaric",
      "Rémi Munos"
    ],
    "summary": "We study the best-arm identification problem in linear bandit, where the\nrewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the\nobjective is to return the arm with the largest reward. We characterize the\ncomplexity of the problem and introduce sample allocation strategies that pull\narms to identify the best arm with a fixed confidence, while minimizing the\nsample budget. In particular, we show the importance of exploiting the global\nlinear structure to improve the estimate of the reward of near-optimal arms. We\nanalyze the proposed strategies and compare their empirical performance.\nFinally, as a by-product of our analysis, we point out the connection to the\n$G$-optimality criterion used in optimal experimental design.",
    "published": "2014-09-22T08:41:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Boosting Framework on Grounds of Online Learning",
    "authors": [
      "Tofigh Naghibi",
      "Beat Pfister"
    ],
    "summary": "By exploiting the duality between boosting and online learning, we present a\nboosting framework which proves to be extremely powerful thanks to employing\nthe vast knowledge available in the online learning area. Using this framework,\nwe develop various algorithms to address multiple practically and theoretically\ninteresting questions including sparse boosting, smooth-distribution boosting,\nagnostic learning and some generalization to double-projection online learning\nalgorithms, as a by-product.",
    "published": "2014-09-25T10:02:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Semidefinite Programming Based Search Strategy for Feature Selection\n  with Mutual Information Measure",
    "authors": [
      "Tofigh Naghibi",
      "Sarah Hoffmann",
      "Beat Pfister"
    ],
    "summary": "Feature subset selection, as a special case of the general subset selection\nproblem, has been the topic of a considerable number of studies due to the\ngrowing importance of data-mining applications. In the feature subset selection\nproblem there are two main issues that need to be addressed: (i) Finding an\nappropriate measure function than can be fairly fast and robustly computed for\nhigh-dimensional data. (ii) A search strategy to optimize the measure over the\nsubset space in a reasonable amount of time. In this article mutual information\nbetween features and class labels is considered to be the measure function. Two\nseries expansions for mutual information are proposed, and it is shown that\nmost heuristic criteria suggested in the literature are truncated\napproximations of these expansions. It is well-known that searching the whole\nsubset space is an NP-hard problem. Here, instead of the conventional\nsequential search algorithms, we suggest a parallel search strategy based on\nsemidefinite programming (SDP) that can search through the subset space in\npolynomial time. By exploiting the similarities between the proposed algorithm\nand an instance of the maximum-cut problem in graph theory, the approximation\nratio of this algorithm is derived and is compared with the approximation ratio\nof the backward elimination method. The experiments show that it can be\nmisleading to judge the quality of a measure solely based on the classification\naccuracy, without taking the effect of the non-optimum search strategy into\naccount.",
    "published": "2014-09-25T11:57:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Maximum mutual information regularized classification",
    "authors": [
      "Jim Jing-Yan Wang",
      "Yi Wang",
      "Shiguang Zhao",
      "Xin Gao"
    ],
    "summary": "In this paper, a novel pattern classification approach is proposed by\nregularizing the classifier learning to maximize mutual information between the\nclassification response and the true class label. We argue that, with the\nlearned classifier, the uncertainty of the true class label of a data sample\nshould be reduced by knowing its classification response as much as possible.\nThe reduced uncertainty is measured by the mutual information between the\nclassification response and the true class label. To this end, when learning a\nlinear classifier, we propose to maximize the mutual information between\nclassification responses and true class labels of training samples, besides\nminimizing the classification error and reduc- ing the classifier complexity.\nAn objective function is constructed by modeling mutual information with\nentropy estimation, and it is optimized by a gradi- ent descend method in an\niterative algorithm. Experiments on two real world pattern classification\nproblems show the significant improvements achieved by maximum mutual\ninformation regularization.",
    "published": "2014-09-27T07:45:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cognitive Learning of Statistical Primary Patterns via Bayesian Network",
    "authors": [
      "Weijia Han",
      "Huiyan Sang",
      "Min Sheng",
      "Jiandong Li",
      "Shuguang Cui"
    ],
    "summary": "In cognitive radio (CR) technology, the trend of sensing is no longer to only\ndetect the presence of active primary users. A large number of applications\ndemand for more comprehensive knowledge on primary user behaviors in spatial,\ntemporal, and frequency domains. To satisfy such requirements, we study the\nstatistical relationship among primary users by introducing a Bayesian network\n(BN) based framework. How to learn such a BN structure is a long standing\nissue, not fully understood even in the statistical learning community.\nBesides, another key problem in this learning scenario is that the CR has to\nidentify how many variables are in the BN, which is usually considered as prior\nknowledge in statistical learning applications. To solve such two issues\nsimultaneously, this paper proposes a BN structure learning scheme consisting\nof an efficient structure learning algorithm and a blind variable\nidentification scheme. The proposed approach incurs significantly lower\ncomputational complexity compared with previous ones, and is capable of\ndetermining the structure without assuming much prior knowledge about\nvariables. With this result, cognitive users could efficiently understand the\nstatistical pattern of primary networks, such that more efficient cognitive\nprotocols could be designed across different network layers.",
    "published": "2014-09-28T16:36:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient multivariate sequence classification",
    "authors": [
      "Pavel P. Kuksa"
    ],
    "summary": "Kernel-based approaches for sequence classification have been successfully\napplied to a variety of domains, including the text categorization, image\nclassification, speech analysis, biological sequence analysis, time series and\nmusic classification, where they show some of the most accurate results.\n  Typical kernel functions for sequences in these domains (e.g., bag-of-words,\nmismatch, or subsequence kernels) are restricted to {\\em discrete univariate}\n(i.e. one-dimensional) string data, such as sequences of words in the text\nanalysis, codeword sequences in the image analysis, or nucleotide or amino acid\nsequences in the DNA and protein sequence analysis. However, original sequence\ndata are often of real-valued multivariate nature, i.e. are not univariate and\ndiscrete as required by typical $k$-mer based sequence kernel functions.\n  In this work, we consider the problem of the {\\em multivariate} sequence\nclassification such as classification of multivariate music sequences, or\nmultidimensional protein sequence representations. To this end, we extend {\\em\nunivariate} kernel functions typically used in sequence analysis and propose\nefficient {\\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a\ndirect feature quantization (DFQ) of each sequence dimension in the original\n{\\em real-valued} multivariate sequences and (2) applying novel multivariate\ndiscrete kernel measures on these multivariate discrete DFQ sequence\nrepresentations to more accurately capture similarity relationships among\nsequences and improve classification performance.\n  Experiments using the proposed MVDFQ-SK kernel method show excellent\nclassification performance on three challenging music classification tasks as\nwell as protein sequence classification with significant 25-40% improvements\nover univariate kernel methods and existing state-of-the-art sequence\nclassification methods.",
    "published": "2014-09-29T18:03:22Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generalized Laguerre Reduction of the Volterra Kernel for Practical\n  Identification of Nonlinear Dynamic Systems",
    "authors": [
      "Brett W. Israelsen",
      "Dale A. Smith"
    ],
    "summary": "The Volterra series can be used to model a large subset of nonlinear, dynamic\nsystems. A major drawback is the number of coefficients required model such\nsystems. In order to reduce the number of required coefficients, Laguerre\npolynomials are used to estimate the Volterra kernels. Existing literature\nproposes algorithms for a fixed number of Volterra kernels, and Laguerre\nseries. This paper presents a novel algorithm for generalized calculation of\nthe finite order Volterra-Laguerre (VL) series for a MIMO system. An example\naddresses the utility of the algorithm in practical application.",
    "published": "2014-10-03T01:59:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Ranking with Top-1 Feedback",
    "authors": [
      "Sougata Chaudhuri",
      "Ambuj Tewari"
    ],
    "summary": "We consider a setting where a system learns to rank a fixed set of $m$ items.\nThe goal is produce good item rankings for users with diverse interests who\ninteract online with the system for $T$ rounds. We consider a novel top-$1$\nfeedback model: at the end of each round, the relevance score for only the top\nranked object is revealed. However, the performance of the system is judged on\nthe entire ranked list. We provide a comprehensive set of results regarding\nlearnability under this challenging setting. For PairwiseLoss and DCG, two\npopular ranking measures, we prove that the minimax regret is\n$\\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using an\nefficient strategy that only spends $O(m \\log m)$ time per round. The same\nefficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$.\nSurprisingly, we show that for normalized versions of these ranking measures,\ni.e., AUC, NDCG \\& MAP, no online ranking algorithm can have sublinear regret.",
    "published": "2014-10-05T00:51:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Stochastic Discriminative EM",
    "authors": [
      "Andres R. Masegosa"
    ],
    "summary": "Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for\ndiscriminative training of probabilistic generative models belonging to the\nexponential family. In this work, we introduce and justify this algorithm as a\nstochastic natural gradient descent method, i.e. a method which accounts for\nthe information geometry in the parameter space of the statistical model. We\nshow how this learning algorithm can be used to train probabilistic generative\nmodels by minimizing different discriminative loss functions, such as the\nnegative conditional log-likelihood and the Hinge loss. The resulting models\ntrained by sdEM are always generative (i.e. they define a joint probability\ndistribution) and, in consequence, allows to deal with missing data and latent\nvariables in a principled way either when being learned or when making\npredictions. The performance of this method is illustrated by several text\nclassification problems for which a multinomial naive Bayes and a latent\nDirichlet allocation based classifier are learned using different\ndiscriminative loss functions.",
    "published": "2014-10-02T12:10:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning manifold to regularize nonnegative matrix factorization",
    "authors": [
      "Jim Jing-Yan Wang",
      "Xin Gao"
    ],
    "summary": "Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize\nnonegative matrix factorization (NMF) for data representation problems.\nNMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank\nnonnegative matrices, has been a popular method for data representation due to\nits ability to explore the latent part-based structure of data. Recent study\nshows that lots of data distributions have manifold structures, and we should\nrespect the manifold structure when the data are represented. Recently,\nmanifold regularized NMF used a nearest neighbor graph to regulate the learning\nof factorization parameter matrices and has shown its advantage over\ntraditional NMF methods for data representation problems. However, how to\nconstruct an optimal graph to present the manifold prop- erly remains a\ndifficultproblem due to the graph modelselection, noisy features, and nonlinear\ndistributed data. In this chapter, we introduce three effective methods to\nsolve these problems of graph construction for manifold regularized NMF.\nMultiple graph learning is proposed to solve the problem of graph model\nselection, adaptive graph learning via feature selection is proposed to solve\nthe problem of constructing a graph from noisy features, while multi-kernel\nlearning-based graph construction is used to solve the problem of learning a\ngraph from nonlinearly distributed data.",
    "published": "2014-10-03T09:25:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Logic-based Approach to Generatively Defined Discriminative Modeling",
    "authors": [
      "Taisuke Sato",
      "Keiichi Kubota",
      "Yoshitaka Kameya"
    ],
    "summary": "Conditional random fields (CRFs) are usually specified by graphical models\nbut in this paper we propose to use probabilistic logic programs and specify\nthem generatively. Our intension is first to provide a unified approach to CRFs\nfor complex modeling through the use of a Turing complete language and second\nto offer a convenient way of realizing generative-discriminative pairs in\nmachine learning to compare generative and discriminative models and choose the\nbest model. We implemented our approach as the D-PRISM language by modifying\nPRISM, a logic-based probabilistic modeling language for generative modeling,\nwhile exploiting its dynamic programming mechanism for efficient probability\ncomputation. We tested D-PRISM with logistic regression, a linear-chain CRF and\na CRF-CFG and empirically confirmed their excellent discriminative performance\ncompared to their generative counterparts, i.e.\\ naive Bayes, an HMM and a\nPCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF\nversions of Bayesian network classifiers and probabilistic left-corner grammars\nrespectively and easily implementable in D-PRISM. We empirically showed that\nthey outperform their generative counterparts as expected.",
    "published": "2014-10-15T06:01:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of\n  Convex Sets",
    "authors": [
      "Jie Wang",
      "Jieping Ye"
    ],
    "summary": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique\nfor simultaneously discovering group and within-group sparse patterns by using\na combination of the $\\ell_1$ and $\\ell_2$ norms. However, in large-scale\napplications, the complexity of the regularizers entails great computational\nchallenges. In this paper, we propose a novel Two-Layer Feature REduction\nmethod (TLFre) for SGL via a decomposition of its dual feasible set. The\ntwo-layer reduction is able to quickly identify the inactive groups and the\ninactive features, respectively, which are guaranteed to be absent from the\nsparse representation and can be removed from the optimization. Existing\nfeature reduction methods are only applicable for sparse models with one\nsparsity-inducing regularizer. To our best knowledge, TLFre is the first one\nthat is capable of dealing with multiple sparsity-inducing regularizers.\nMoreover, TLFre has a very low computational cost and can be integrated with\nany existing solvers. We also develop a screening method---called DPC\n(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments\non both synthetic and real data sets show that TLFre and DPC improve the\nefficiency of SGL and nonnegative Lasso by several orders of magnitude.",
    "published": "2014-10-15T20:08:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning a hyperplane regressor by minimizing an exact bound on the VC\n  dimension",
    "authors": [
      "Jayadeva",
      "Suresh Chandra",
      "Siddarth Sabharwal",
      "Sanjit S. Batra"
    ],
    "summary": "The capacity of a learning machine is measured by its Vapnik-Chervonenkis\ndimension, and learning machines with a low VC dimension generalize better. It\nis well known that the VC dimension of SVMs can be very large or unbounded,\neven though they generally yield state-of-the-art learning performance. In this\npaper, we show how to learn a hyperplane regressor by minimizing an exact, or\n\\boldmath{$\\Theta$} bound on its VC dimension. The proposed approach, termed as\nthe Minimal Complexity Machine (MCM) Regressor, involves solving a simple\nlinear programming problem. Experimental results show, that on a number of\nbenchmark datasets, the proposed approach yields regressors with error rates\nmuch less than those obtained with conventional SVM regresssors, while often\nusing fewer support vectors. On some benchmark datasets, the number of support\nvectors is less than one tenth the number used by SVMs, indicating that the MCM\ndoes indeed learn simpler representations.",
    "published": "2014-10-16T20:04:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Naive Bayes and Text Classification I - Introduction and Theory",
    "authors": [
      "Sebastian Raschka"
    ],
    "summary": "Naive Bayes classifiers, a family of classifiers that are based on the\npopular Bayes' probability theorem, are known for creating simple yet well\nperforming models, especially in the fields of document classification and\ndisease prediction. In this article, we will look at the main concepts of naive\nBayes classification in the context of document categorization.",
    "published": "2014-10-16T22:11:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Overview of General Performance Metrics of Binary Classifier Systems",
    "authors": [
      "Sebastian Raschka"
    ],
    "summary": "This document provides a brief overview of different metrics and terminology\nthat is used to measure the performance of binary classification systems.",
    "published": "2014-10-17T00:50:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Feature Selection Based on Confidence Machine",
    "authors": [
      "Chang Liu",
      "Yi Xu"
    ],
    "summary": "In machine learning and pattern recognition, feature selection has been a hot\ntopic in the literature. Unsupervised feature selection is challenging due to\nthe loss of labels which would supply the related information.How to define an\nappropriate metric is the key for feature selection. We propose a filter method\nfor unsupervised feature selection which is based on the Confidence Machine.\nConfidence Machine offers an estimation of confidence on a feature'reliability.\nIn this paper, we provide the math model of Confidence Machine in the context\nof feature selection, which maximizes the relevance and minimizes the\nredundancy of the selected feature. We compare our method against classic\nfeature selection methods Laplacian Score, Pearson Correlation and Principal\nComponent Analysis on benchmark data sets. The experimental results demonstrate\nthe efficiency and effectiveness of our method.",
    "published": "2014-10-20T21:32:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cosine Similarity Measure According to a Convex Cost Function",
    "authors": [
      "Osman Gunay",
      "Cem Emre Akbas",
      "A. Enis Cetin"
    ],
    "summary": "In this paper, we describe a new vector similarity measure associated with a\nconvex cost function. Given two vectors, we determine the surface normals of\nthe convex function at the vectors. The angle between the two surface normals\nis the similarity measure. Convex cost function can be the negative entropy\nfunction, total variation (TV) function and filtered variation function. The\nconvex cost function need not be differentiable everywhere. In general, we need\nto compute the gradient of the cost function to compute the surface normals. If\nthe gradient does not exist at a given vector, it is possible to use the\nsubgradients and the normal producing the smallest angle between the two\nvectors is used to compute the similarity measure.",
    "published": "2014-10-22T16:13:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Differentially- and non-differentially-private random decision trees",
    "authors": [
      "Mariusz Bojarski",
      "Anna Choromanska",
      "Krzysztof Choromanski",
      "Yann LeCun"
    ],
    "summary": "We consider supervised learning with random decision trees, where the tree\nconstruction is completely random. The method is popularly used and works well\nin practice despite the simplicity of the setting, but its statistical\nmechanism is not yet well-understood. In this paper we provide strong\ntheoretical guarantees regarding learning with random decision trees. We\nanalyze and compare three different variants of the algorithm that have minimal\nmemory requirements: majority voting, threshold averaging and probabilistic\naveraging. The random structure of the tree enables us to adapt these methods\nto a differentially-private setting thus we also propose differentially-private\nversions of all three schemes. We give upper-bounds on the generalization error\nand mathematically explain how the accuracy depends on the number of random\ndecision trees. Furthermore, we prove that only logarithmic (in the size of the\ndataset) number of independently selected random decision trees suffice to\ncorrectly classify most of the data, even when differential-privacy guarantees\nmust be maintained. We empirically show that majority voting and threshold\naveraging give the best accuracy, also for conservative users requiring high\nprivacy guarantees. Furthermore, we demonstrate that a simple majority voting\nrule is an especially good candidate for the differentially-private classifier\nsince it is much less sensitive to the choice of forest parameters than other\nmethods.",
    "published": "2014-10-26T00:16:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Notes on using Determinantal Point Processes for Clustering with\n  Applications to Text Clustering",
    "authors": [
      "Apoorv Agarwal",
      "Anna Choromanska",
      "Krzysztof Choromanski"
    ],
    "summary": "In this paper, we compare three initialization schemes for the KMEANS\nclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and\n3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of k\nneeds to be set by the user of the algorithms. (Kang 2013) recently proposed a\nnovel use of determinantal point processes for sampling the initial centroids\nfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provide\nany evaluation establishing that KMEANSD++ is better than other algorithms. In\nthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++\n(both of which are better than KMEANSRAND) with KMEANSD++ having an additional\nthat it can automatically approximate the value of k.",
    "published": "2014-10-26T00:59:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Feature Selection through Minimization of the VC dimension",
    "authors": [
      "Jayadeva",
      "Sanjit S. Batra",
      "Siddharth Sabharwal"
    ],
    "summary": "Feature selection involes identifying the most relevant subset of input\nfeatures, with a view to improving generalization of predictive models by\nreducing overfitting. Directly searching for the most relevant combination of\nattributes is NP-hard. Variable selection is of critical importance in many\napplications, such as micro-array data analysis, where selecting a small number\nof discriminative features is crucial to developing useful models of disease\nmechanisms, as well as for prioritizing targets for drug discovery. The\nrecently proposed Minimal Complexity Machine (MCM) provides a way to learn a\nhyperplane classifier by minimizing an exact (\\boldmath{$\\Theta$}) bound on its\nVC dimension. It is well known that a lower VC dimension contributes to good\ngeneralization. For a linear hyperplane classifier in the input space, the VC\ndimension is upper bounded by the number of features; hence, a linear\nclassifier with a small VC dimension is parsimonious in the set of features it\nemploys. In this paper, we use the linear MCM to learn a classifier in which a\nlarge number of weights are zero; features with non-zero weights are the ones\nthat are chosen. Selected features are used to learn a kernel SVM classifier.\nOn a number of benchmark datasets, the features chosen by the linear MCM yield\ncomparable or better test set accuracy than when methods such as ReliefF and\nFCBF are used for the task. The linear MCM typically chooses one-tenth the\nnumber of attributes chosen by the other methods; on some very high dimensional\ndatasets, the MCM chooses about $0.6\\%$ of the features; in comparison, ReliefF\nand FCBF choose 70 to 140 times more features, thus demonstrating that\nminimizing the VC dimension may provide a new, and very effective route for\nfeature selection and for learning sparse representations.",
    "published": "2014-10-27T19:46:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Learning of Relational Dependency Networks",
    "authors": [
      "Oliver Schulte",
      "Zhensong Qian",
      "Arthur E. Kirkpatrick",
      "Xiaoqian Yin",
      "Yan Sun"
    ],
    "summary": "A Relational Dependency Network (RDN) is a directed graphical model widely\nused for multi-relational data. These networks allow cyclic dependencies,\nnecessary to represent relational autocorrelations. We describe an approach for\nlearning both the RDN's structure and its parameters, given an input relational\ndatabase: First learn a Bayesian network (BN), then transform the Bayesian\nnetwork to an RDN. Thus fast Bayes net learning can provide fast RDN learning.\nThe BN-to-RDN transform comprises a simple, local adjustment of the Bayes net\nstructure and a closed-form transform of the Bayes net parameters. This method\ncan learn an RDN for a dataset with a million tuples in minutes. We empirically\ncompare our approach to state-of-the art RDN learning methods that use\nfunctional gradient boosting, on five benchmark datasets. Learning RDNs via BNs\nscales much better to large datasets than learning RDNs with boosting, and\nprovides competitive accuracy in predictions.",
    "published": "2014-10-28T23:14:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Global Bandits with Holder Continuity",
    "authors": [
      "Onur Atan",
      "Cem Tekin",
      "Mihaela van der Schaar"
    ],
    "summary": "Standard Multi-Armed Bandit (MAB) problems assume that the arms are\nindependent. However, in many application scenarios, the information obtained\nby playing an arm provides information about the remainder of the arms. Hence,\nin such applications, this informativeness can and should be exploited to\nenable faster convergence to the optimal solution. In this paper, we introduce\nand formalize the Global MAB (GMAB), in which arms are globally informative\nthrough a global parameter, i.e., choosing an arm reveals information about all\nthe arms. We propose a greedy policy for the GMAB which always selects the arm\nwith the highest estimated expected reward, and prove that it achieves bounded\nparameter-dependent regret. Hence, this policy selects suboptimal arms only\nfinitely many times, and after a finite number of initial time steps, the\noptimal arm is selected in all of the remaining time steps with probability\none. In addition, we also study how the informativeness of the arms about each\nother's rewards affects the speed of learning. Specifically, we prove that the\nparameter-free (worst-case) regret is sublinear in time, and decreases with the\ninformativeness of the arms. We also prove a sublinear in time Bayesian risk\nbound for the GMAB which reduces to the well-known Bayesian risk bound for\nlinearly parameterized bandits when the arms are fully informative. GMABs have\napplications ranging from drug and treatment discovery to dynamic pricing.",
    "published": "2014-10-29T07:05:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Notes on Noise Contrastive Estimation and Negative Sampling",
    "authors": [
      "Chris Dyer"
    ],
    "summary": "Estimating the parameters of probabilistic models of language such as maxent\nmodels and probabilistic neural models is computationally difficult since it\ninvolves evaluating partition functions by summing over an entire vocabulary,\nwhich may be millions of word types in size. Two closely related\nstrategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih and\nKavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al.,\n2012; Goldberg and Levy, 2014)---have emerged as popular solutions to this\ncomputational problem, but some confusion remains as to which is more\nappropriate and when. This document explicates their relationships to each\nother and to other estimation techniques. The analysis shows that, although\nthey are superficially similar, NCE is a general parameter estimation technique\nthat is asymptotically unbiased, while negative sampling is best understood as\na family of binary classification models that are useful for learning word\nrepresentations but not as a general-purpose estimator.",
    "published": "2014-10-30T04:33:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "NICE: Non-linear Independent Components Estimation",
    "authors": [
      "Laurent Dinh",
      "David Krueger",
      "Yoshua Bengio"
    ],
    "summary": "We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.",
    "published": "2014-10-30T19:44:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Mixtures of Ranking Models",
    "authors": [
      "Pranjal Awasthi",
      "Avrim Blum",
      "Or Sheffet",
      "Aravindan Vijayaraghavan"
    ],
    "summary": "This work concerns learning probabilistic models for ranking data in a\nheterogeneous population. The specific problem we study is learning the\nparameters of a Mallows Mixture Model. Despite being widely studied, current\nheuristics for this problem do not have theoretical guarantees and can get\nstuck in bad local optima. We present the first polynomial time algorithm which\nprovably learns the parameters of a mixture of two Mallows models. A key\ncomponent of our algorithm is a novel use of tensor decomposition techniques to\nlearn the top-k prefix in both the rankings. Before this work, even the\nquestion of identifiability in the case of a mixture of two Mallows models was\nunresolved.",
    "published": "2014-10-31T14:31:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Factorbird - a Parameter Server Approach to Distributed Matrix\n  Factorization",
    "authors": [
      "Sebastian Schelter",
      "Venu Satuluri",
      "Reza Zadeh"
    ],
    "summary": "We present Factorbird, a prototype of a parameter server approach for\nfactorizing large matrices with Stochastic Gradient Descent-based algorithms.\nWe designed Factorbird to meet the following desiderata: (a) scalability to\ntall and wide matrices with dozens of billions of non-zeros, (b) extensibility\nto different kinds of models and loss functions as long as they can be\noptimized using Stochastic Gradient Descent (SGD), and (c) adaptability to both\nbatch and streaming scenarios. Factorbird uses a parameter server in order to\nscale to models that exceed the memory of an individual machine, and employs\nlock-free Hogwild!-style learning with a special partitioning scheme to\ndrastically reduce conflicting updates. We also discuss other aspects of the\ndesign of our system such as how to efficiently grid search for hyperparameters\nat scale. We present experiments of Factorbird on a matrix built from a subset\nof Twitter's interaction graph, consisting of more than 38 billion non-zeros\nand about 200 million rows and columns, which is to the best of our knowledge\nthe largest matrix on which factorization results have been reported in the\nliterature.",
    "published": "2014-11-03T18:49:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "CUR Algorithm for Partially Observed Matrices",
    "authors": [
      "Miao Xu",
      "Rong Jin",
      "Zhi-Hua Zhou"
    ],
    "summary": "CUR matrix decomposition computes the low rank approximation of a given\nmatrix by using the actual rows and columns of the matrix. It has been a very\nuseful tool for handling large matrices. One limitation with the existing\nalgorithms for CUR matrix decomposition is that they need an access to the {\\it\nfull} matrix, a requirement that can be difficult to fulfill in many real world\napplications. In this work, we alleviate this limitation by developing a CUR\ndecomposition algorithm for partially observed matrices. In particular, the\nproposed algorithm computes the low rank approximation of the target matrix\nbased on (i) the randomly sampled rows and columns, and (ii) a subset of\nobserved entries that are randomly sampled from the matrix. Our analysis shows\nthe relative error bound, measured by spectral norm, for the proposed algorithm\nwhen the target matrix is of full rank. We also show that only $O(n r\\ln r)$\nobserved entries are needed by the proposed algorithm to perfectly recover a\nrank $r$ matrix of size $n\\times n$, which improves the sample complexity of\nthe existing algorithms for matrix completion. Empirical studies on both\nsynthetic and real-world datasets verify our theoretical claims and demonstrate\nthe effectiveness of the proposed algorithm.",
    "published": "2014-11-04T11:03:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Eigenvectors of Orthogonally Decomposable Functions",
    "authors": [
      "Mikhail Belkin",
      "Luis Rademacher",
      "James Voss"
    ],
    "summary": "The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by\nthe spectral theorem is a foundational result in applied mathematics. Motivated\nby a shared structure found in inferential problems of recent interest---namely\northogonal tensor decompositions, Independent Component Analysis (ICA), topic\nmodels, spectral clustering, and Gaussian mixture learning---we generalize the\neigendecomposition from quadratic forms to a broad class of \"orthogonally\ndecomposable\" functions. We identify a key role of convexity in our extension,\nand we generalize two traditional characterizations of eigenvectors: First, the\neigenvectors of a quadratic form arise from the optima structure of the\nquadratic form on the sphere. Second, the eigenvectors are the fixed points of\nthe power iteration.\n  In our setting, we consider a simple first order generalization of the power\nmethod which we call gradient iteration. It leads to efficient and easily\nimplementable methods for basis recovery. It includes influential Machine\nLearning methods such as cumulant-based FastICA and the tensor power iteration\nfor orthogonally decomposable tensors as special cases.\n  We provide a complete theoretical analysis of gradient iteration using the\nstructure theory of discrete dynamical systems to show almost sure convergence\nand fast (super-linear) convergence rates. The analysis also extends to the\ncase when the observed function is only approximately orthogonally\ndecomposable, with bounds that are polynomial in dimension and other relevant\nparameters, such as perturbation size. Our perturbation results can be\nconsidered as a non-linear version of the classical Davis-Kahan theorem for\nperturbations of eigenvectors of symmetric matrices.",
    "published": "2014-11-05T21:07:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Information Theoretic Limits of Learning Ising Models",
    "authors": [
      "Karthikeyan Shanmugam",
      "Rashish Tandon",
      "Alexandros G. Dimakis",
      "Pradeep Ravikumar"
    ],
    "summary": "We provide a general framework for computing lower-bounds on the sample\ncomplexity of recovering the underlying graphs of Ising models, given i.i.d\nsamples. While there have been recent results for specific graph classes, these\ninvolve fairly extensive technical arguments that are specialized to each\nspecific graph class. In contrast, we isolate two key graph-structural\ningredients that can then be used to specify sample complexity lower-bounds.\nPresence of these structural properties makes the graph class hard to learn. We\nderive corollaries of our main result that not only recover existing recent\nresults, but also provide lower bounds for novel graph classes not considered\npreviously. We also extend our framework to the random graph setting and derive\ncorollaries for Erd\\H{o}s-R\\'{e}nyi graphs in a certain dense setting.",
    "published": "2014-11-05T22:28:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Representations for Life-Long Learning and Autoencoding",
    "authors": [
      "Maria-Florina Balcan",
      "Avrim Blum",
      "Santosh Vempala"
    ],
    "summary": "It has been a long-standing goal in machine learning, as well as in AI more\ngenerally, to develop life-long learning systems that learn many different\ntasks over time, and reuse insights from tasks learned, \"learning to learn\" as\nthey do so. In this work we pose and provide efficient algorithms for several\nnatural theoretical formulations of this goal. Specifically, we consider the\nproblem of learning many different target functions over time, that share\ncertain commonalities that are initially unknown to the learning algorithm. Our\naim is to learn new internal representations as the algorithm learns new target\nfunctions, that capture this commonality and allow subsequent learning tasks to\nbe solved more efficiently and from less data. We develop efficient algorithms\nfor two very different kinds of commonalities that target functions might\nshare: one based on learning common low-dimensional and unions of\nlow-dimensional subspaces and one based on learning nonlinear Boolean\ncombinations of features. Our algorithms for learning Boolean feature\ncombinations additionally have a dual interpretation, and can be viewed as\ngiving an efficient procedure for constructing near-optimal sparse Boolean\nautoencoders under a natural \"anchor-set\" assumption.",
    "published": "2014-11-06T03:51:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Hybrid Recurrent Neural Network For Music Transcription",
    "authors": [
      "Siddharth Sigtia",
      "Emmanouil Benetos",
      "Nicolas Boulanger-Lewandowski",
      "Tillman Weyde",
      "Artur S. d'Avila Garcez",
      "Simon Dixon"
    ],
    "summary": "We investigate the problem of incorporating higher-level symbolic score-like\ninformation into Automatic Music Transcription (AMT) systems to improve their\nperformance. We use recurrent neural networks (RNNs) and their variants as\nmusic language models (MLMs) and present a generative architecture for\ncombining these models with predictions from a frame level acoustic classifier.\nWe also compare different neural network architectures for acoustic modeling.\nThe proposed model computes a distribution over possible output sequences given\nthe acoustic input signal and we present an algorithm for performing a global\nsearch for good candidate transcriptions. The performance of the proposed model\nis evaluated on piano music from the MAPS dataset and we observe that the\nproposed model consistently outperforms existing transcription methods.",
    "published": "2014-11-06T14:18:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Collaborative-Filtering on Graphs",
    "authors": [
      "Siddhartha Banerjee",
      "Sujay Sanghavi",
      "Sanjay Shakkottai"
    ],
    "summary": "A common phenomena in modern recommendation systems is the use of feedback\nfrom one user to infer the `value' of an item to other users. This results in\nan exploration vs. exploitation trade-off, in which items of possibly low value\nhave to be presented to users in order to ascertain their value. Existing\napproaches to solving this problem focus on the case where the number of items\nare small, or admit some underlying structure -- it is unclear, however, if\ngood recommendation is possible when dealing with content-rich settings with\nunstructured content.\n  We consider this problem under a simple natural model, wherein the number of\nitems and the number of item-views are of the same order, and an `access-graph'\nconstrains which user is allowed to see which item. Our main insight is that\nthe presence of the access-graph in fact makes good recommendation possible --\nhowever this requires the exploration policy to be designed to take advantage\nof the access-graph. Our results demonstrate the importance of `serendipity' in\nexploration, and how higher graph-expansion translates to a higher quality of\nrecommendations; it also suggests a reason why in some settings, simple\npolicies like Twitter's `Latest-First' policy achieve a good performance.\n  From a technical perspective, our model presents a way to study\nexploration-exploitation tradeoffs in settings where the number of `trials' and\n`strategies' are large (potentially infinite), and more importantly, of the\nsame order. Our algorithms admit competitive-ratio guarantees which hold for\nthe worst-case user, under both finite-population and infinite-horizon\nsettings, and are parametrized in terms of properties of the underlying graph.\nConversely, we also demonstrate that improperly-designed policies can be highly\nsub-optimal, and that in many settings, our results are order-wise optimal.",
    "published": "2014-11-07T22:52:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A chain rule for the expected suprema of Gaussian processes",
    "authors": [
      "Andreas Maurer"
    ],
    "summary": "The expected supremum of a Gaussian process indexed by the image of an index\nset under a function class is bounded in terms of separate properties of the\nindex set and the function class. The bound is relevant to the estimation of\nnonlinear transformations or the analysis of learning algorithms whenever\nhypotheses are chosen from composite classes, as is the case for multi-layer\nmodels.",
    "published": "2014-11-10T21:41:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounded Regret for Finite-Armed Structured Bandits",
    "authors": [
      "Tor Lattimore",
      "Remi Munos"
    ],
    "summary": "We study a new type of K-armed bandit problem where the expected return of\none arm may depend on the returns of other arms. We present a new algorithm for\nthis general class of problems and show that under certain circumstances it is\npossible to achieve finite expected cumulative regret. We also give\nproblem-dependent lower bounds on the cumulative regret showing that at least\nin special cases the new algorithm is nearly optimal.",
    "published": "2014-11-11T18:55:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Greedy metrics in orthogonal greedy learning",
    "authors": [
      "Lin Xu",
      "Shaobo Lin",
      "Jinshan Zeng",
      "Zongben Xu"
    ],
    "summary": "Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a\nnew atom from a dictionary via the steepest gradient descent and build the\nestimator via orthogonal projecting the target function to the space spanned by\nthe selected atoms in each greedy step. Here, \"greed\" means choosing a new atom\naccording to the steepest gradient descent principle. OGL then avoids the\noverfitting/underfitting by selecting an appropriate iteration number. In this\npaper, we point out that the overfitting/underfitting can also be avoided via\nredefining \"greed\" in OGL. To this end, we introduce a new greedy metric,\ncalled $\\delta$-greedy thresholds, to refine \"greed\" and theoretically verifies\nits feasibility. Furthermore, we reveals that such a greedy metric can bring an\nadaptive termination rule on the premise of maintaining the prominent learning\nperformance of OGL. Our results show that the steepest gradient descent is not\nthe unique greedy metric of OGL and some other more suitable metric may lessen\nthe hassle of model-selection of OGL.",
    "published": "2014-11-13T14:26:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Minimal Realization Problems for Hidden Markov Models",
    "authors": [
      "Qingqing Huang",
      "Rong Ge",
      "Sham Kakade",
      "Munther Dahleh"
    ],
    "summary": "Consider a stationary discrete random process with alphabet size d, which is\nassumed to be the output process of an unknown stationary Hidden Markov Model\n(HMM). Given the joint probabilities of finite length strings of the process,\nwe are interested in finding a finite state generative model to describe the\nentire process. In particular, we focus on two classes of models: HMMs and\nquasi-HMMs, which is a strictly larger class of models containing HMMs. In the\nmain theorem, we show that if the random process is generated by an HMM of\norder less or equal than k, and whose transition and observation probability\nmatrix are in general position, namely almost everywhere on the parameter\nspace, both the minimal quasi-HMM realization and the minimal HMM realization\ncan be efficiently computed based on the joint probabilities of all the length\nN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to\ncompare and connect the two lines of literature: realization theory of HMMs,\nand the recent development in learning latent variable models with tensor\ndecomposition techniques.",
    "published": "2014-11-13T20:30:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sample-targeted clinical trial adaptation",
    "authors": [
      "Ognjen Arandjelovic"
    ],
    "summary": "Clinical trial adaptation refers to any adjustment of the trial protocol\nafter the onset of the trial. The main goal is to make the process of\nintroducing new medical interventions to patients more efficient by reducing\nthe cost and the time associated with evaluating their safety and efficacy. The\nprincipal question is how should adaptation be performed so as to minimize the\nchance of distorting the outcome of the trial. We propose a novel method for\nachieving this. Unlike previous work our approach focuses on trial adaptation\nby sample size adjustment. We adopt a recently proposed stratification\nframework based on collected auxiliary data and show that this information\ntogether with the primary measured variables can be used to make a\nprobabilistically informed choice of the particular sub-group a sample should\nbe removed from. Experiments on simulated data are used to illustrate the\neffectiveness of our method and its application in practice.",
    "published": "2014-11-14T14:30:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Differentially Private Algorithms for Empirical Machine Learning",
    "authors": [
      "Ben Stoddard",
      "Yan Chen",
      "Ashwin Machanavajjhala"
    ],
    "summary": "An important use of private data is to build machine learning classifiers.\nWhile there is a burgeoning literature on differentially private classification\nalgorithms, we find that they are not practical in real applications due to two\nreasons. First, existing differentially private classifiers provide poor\naccuracy on real world datasets. Second, there is no known differentially\nprivate algorithm for empirically evaluating the private classifier on a\nprivate test dataset.\n  In this paper, we develop differentially private algorithms that mirror real\nworld empirical machine learning workflows. We consider the private classifier\ntraining algorithm as a blackbox. We present private algorithms for selecting\nfeatures that are input to the classifier. Though adding a preprocessing step\ntakes away some of the privacy budget from the actual classification process\n(thus potentially making it noisier and less accurate), we show that our novel\npreprocessing techniques significantly increase classifier accuracy on three\nreal-world datasets. We also present the first private algorithms for\nempirically constructing receiver operating characteristic (ROC) curves on a\nprivate test set.",
    "published": "2014-11-20T03:10:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "No-Regret Learnability for Piecewise Linear Losses",
    "authors": [
      "Arthur Flajolet",
      "Patrick Jaillet"
    ],
    "summary": "In the convex optimization approach to online regret minimization, many\nmethods have been developed to guarantee a $O(\\sqrt{T})$ bound on regret for\nsubdifferentiable convex loss functions with bounded subgradients, by using a\nreduction to linear loss functions. This suggests that linear loss functions\ntend to be the hardest ones to learn against, regardless of the underlying\ndecision spaces. We investigate this question in a systematic fashion looking\nat the interplay between the set of possible moves for both the decision maker\nand the adversarial environment. This allows us to highlight sharp distinctive\nbehaviors about the learnability of piecewise linear loss functions. On the one\nhand, when the decision set of the decision maker is a polyhedron, we establish\n$\\Omega(\\sqrt{T})$ lower bounds on regret for a large class of piecewise linear\nloss functions with important applications in online linear optimization,\nrepeated zero-sum Stackelberg games, online prediction with side information,\nand online two-stage optimization. On the other hand, we exhibit $o(\\sqrt{T})$\nlearning rates, achieved by the Follow-The-Leader algorithm, in online linear\noptimization when the boundary of the decision maker's decision set is curved\nand when $0$ does not lie in the convex hull of the environment's decision set.\nHence, the curvature of the decision maker's decision set is a determining\nfactor for the optimal learning rate. These results hold in a completely\nadversarial setting.",
    "published": "2014-11-20T19:38:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Compound Rank-k Projections for Bilinear Analysis",
    "authors": [
      "Xiaojun Chang",
      "Feiping Nie",
      "Sen Wang",
      "Yi Yang",
      "Xiaofang Zhou",
      "Chengqi Zhang"
    ],
    "summary": "In many real-world applications, data are represented by matrices or\nhigh-order tensors. Despite the promising performance, the existing\ntwo-dimensional discriminant analysis algorithms employ a single projection\nmodel to exploit the discriminant information for projection, making the model\nless flexible. In this paper, we propose a novel Compound Rank-k Projection\n(CRP) algorithm for bilinear analysis. CRP deals with matrices directly without\ntransforming them into vectors, and it therefore preserves the correlations\nwithin the matrix and decreases the computation complexity. Different from the\nexisting two dimensional discriminant analysis algorithms, objective function\nvalues of CRP increase monotonically.In addition, CRP utilizes multiple rank-k\nprojection models to enable a larger search space in which the optimal solution\ncan be found. In this way, the discriminant ability is enhanced.",
    "published": "2014-11-23T12:50:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple\n  Tasks",
    "authors": [
      "Xiaojun Chang",
      "Yi Yang"
    ],
    "summary": "In this paper, we propose a novel semi-supervised feature selection framework\nby mining correlations among multiple tasks and apply it to different\nmultimedia applications. Instead of independently computing the importance of\nfeatures for each task, our algorithm leverages shared knowledge from multiple\nrelated tasks, thus, improving the performance of feature selection. Note that\nwe build our algorithm on assumption that different tasks share common\nstructures. The proposed algorithm selects features in a batch mode, by which\nthe correlations between different features are taken into consideration.\nBesides, considering the fact that labeling a large amount of training data in\nreal world is both time-consuming and tedious, we adopt manifold learning which\nexploits both labeled and unlabeled training data for feature space analysis.\nSince the objective function is non-smooth and difficult to solve, we propose\nan iterative algorithm with fast convergence. Extensive experiments on\ndifferent applications demonstrate that our algorithm outperforms other\nstate-of-the-art feature selection algorithms.",
    "published": "2014-11-23T13:00:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Convex Sparse PCA for Feature Analysis",
    "authors": [
      "Xiaojun Chang",
      "Feiping Nie",
      "Yi Yang",
      "Heng Huang"
    ],
    "summary": "Principal component analysis (PCA) has been widely applied to dimensionality\nreduction and data pre-processing for different applications in engineering,\nbiology and social science. Classical PCA and its variants seek for linear\nprojections of the original variables to obtain a low dimensional feature\nrepresentation with maximal variance. One limitation is that it is very\ndifficult to interpret the results of PCA. In addition, the classical PCA is\nvulnerable to certain noisy data. In this paper, we propose a convex sparse\nprincipal component analysis (CSPCA) algorithm and apply it to feature\nanalysis. First we show that PCA can be formulated as a low-rank regression\noptimization problem. Based on the discussion, the l 2 , 1 -norm minimization\nis incorporated into the objective function to make the regression coefficients\nsparse, thereby robust to the outliers. In addition, based on the sparse model\nused in CSPCA, an optimal weight is assigned to each of the original feature,\nwhich in turn provides the output with good interpretability. With the output\nof our CSPCA, we can effectively analyze the importance of each feature under\nthe PCA criteria. The objective function is convex, and we propose an iterative\nalgorithm to optimize it. We apply the CSPCA algorithm to feature selection and\nconduct extensive experiments on six different benchmark datasets. Experimental\nresults demonstrate that the proposed algorithm outperforms state-of-the-art\nunsupervised feature selection algorithms.",
    "published": "2014-11-23T13:06:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Balanced k-Means and Min-Cut Clustering",
    "authors": [
      "Xiaojun Chang",
      "Feiping Nie",
      "Zhigang Ma",
      "Yi Yang"
    ],
    "summary": "Clustering is an effective technique in data mining to generate groups that\nare the matter of interest. Among various clustering approaches, the family of\nk-means algorithms and min-cut algorithms gain most popularity due to their\nsimplicity and efficacy. The classical k-means algorithm partitions a number of\ndata points into several subsets by iteratively updating the clustering centers\nand the associated data points. By contrast, a weighted undirected graph is\nconstructed in min-cut algorithms which partition the vertices of the graph\ninto two sets. However, existing clustering algorithms tend to cluster minority\nof data points into a subset, which shall be avoided when the target dataset is\nbalanced. To achieve more accurate clustering for balanced dataset, we propose\nto leverage exclusive lasso on k-means and min-cut to regulate the balance\ndegree of the clustering results. By optimizing our objective functions that\nbuild atop the exclusive lasso, we can make the clustering result as much\nbalanced as possible. Extensive experiments on several large-scale datasets\nvalidate the advantage of the proposed algorithms compared to the\nstate-of-the-art clustering algorithms.",
    "published": "2014-11-23T13:16:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Improved Spectral Clustering via Embedded Label Propagation",
    "authors": [
      "Xiaojun Chang",
      "Feiping Nie",
      "Yi Yang",
      "Heng Huang"
    ],
    "summary": "Spectral clustering is a key research topic in the field of machine learning\nand data mining. Most of the existing spectral clustering algorithms are built\nupon Gaussian Laplacian matrices, which are sensitive to parameters. We propose\na novel parameter free, distance consistent Locally Linear Embedding. The\nproposed distance consistent LLE promises that edges between closer data points\nhave greater weight.Furthermore, we propose a novel improved spectral\nclustering via embedded label propagation. Our algorithm is built upon two\nadvancements of the state of the art:1) label propagation,which propagates a\nnode\\'s labels to neighboring nodes according to their proximity; and 2)\nmanifold learning, which has been widely used in its capacity to leverage the\nmanifold structure of data points. First we perform standard spectral\nclustering on original data and assign each cluster to k nearest data points.\nNext, we propagate labels through dense, unlabeled data regions. Extensive\nexperiments with various datasets validate the superiority of the proposed\nalgorithm compared to current state of the art spectral algorithms.",
    "published": "2014-11-23T13:35:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Structure Regularization for Structured Prediction: Theories and\n  Experiments",
    "authors": [
      "Xu Sun"
    ],
    "summary": "While there are many studies on weight regularization, the study on structure\nregularization is rare. Many existing systems on structured prediction focus on\nincreasing the level of structural dependencies within the model. However, this\ntrend could have been misdirected, because our study suggests that complex\nstructures are actually harmful to generalization ability in structured\nprediction. To control structure-based overfitting, we propose a structure\nregularization framework via \\emph{structure decomposition}, which decomposes\ntraining samples into mini-samples with simpler structures, deriving a model\nwith better generalization power. We show both theoretically and empirically\nthat structure regularization can effectively control overfitting risk and lead\nto better accuracy. As a by-product, the proposed method can also substantially\naccelerate the training speed. The method and the theoretical results can apply\nto general graphical models with arbitrary structures. Experiments on\nwell-known tasks demonstrate that our method can easily beat the benchmark\nsystems on those highly-competitive tasks, achieving state-of-the-art\naccuracies yet with substantially faster training speed.",
    "published": "2014-11-23T14:11:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Revenue Optimization in Posted-Price Auctions with Strategic Buyers",
    "authors": [
      "Mehryar Mohri",
      "Andres Muñoz Medina"
    ],
    "summary": "We study revenue optimization learning algorithms for posted-price auctions\nwith strategic buyers. We analyze a very broad family of monotone regret\nminimization algorithms for this problem, which includes the previously best\nknown algorithm, and show that no algorithm in that family admits a strategic\nregret more favorable than $\\Omega(\\sqrt{T})$. We then introduce a new\nalgorithm that achieves a strategic regret differing from the lower bound only\nby a factor in $O(\\log T)$, an exponential improvement upon the previous best\nalgorithm. Our new algorithm admits a natural analysis and simpler proofs, and\nthe ideas behind its design are general. We also report the results of\nempirical evaluations comparing our algorithm with the previous state of the\nart and show a consistent exponential improvement in several different\nscenarios.",
    "published": "2014-11-23T21:58:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Convex Formulation for Spectral Shrunk Clustering",
    "authors": [
      "Xiaojun Chang",
      "Feiping Nie",
      "Zhigang Ma",
      "Yi Yang",
      "Xiaofang Zhou"
    ],
    "summary": "Spectral clustering is a fundamental technique in the field of data mining\nand information processing. Most existing spectral clustering algorithms\nintegrate dimensionality reduction into the clustering process assisted by\nmanifold learning in the original space. However, the manifold in\nreduced-dimensional subspace is likely to exhibit altered properties in\ncontrast with the original space. Thus, applying manifold information obtained\nfrom the original space to the clustering process in a low-dimensional subspace\nis prone to inferior performance. Aiming to address this issue, we propose a\nnovel convex algorithm that mines the manifold structure in the low-dimensional\nsubspace. In addition, our unified learning process makes the manifold learning\nparticularly tailored for the clustering. Compared with other related methods,\nthe proposed algorithm results in more structured clustering result. To\nvalidate the efficacy of the proposed algorithm, we perform extensive\nexperiments on several benchmark datasets in comparison with some\nstate-of-the-art clustering approaches. The experimental results demonstrate\nthat the proposed algorithm has quite promising clustering performance.",
    "published": "2014-11-23T22:12:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Accelerated Parallel Optimization Methods for Large Scale Machine\n  Learning",
    "authors": [
      "Haipeng Luo",
      "Patrick Haffner",
      "Jean-Francois Paiement"
    ],
    "summary": "The growing amount of high dimensional data in different machine learning\napplications requires more efficient and scalable optimization algorithms. In\nthis work, we consider combining two techniques, parallelism and Nesterov's\nacceleration, to design faster algorithms for L1-regularized loss. We first\nsimplify BOOM, a variant of gradient descent, and study it in a unified\nframework, which allows us to not only propose a refined measurement of\nsparsity to improve BOOM, but also show that BOOM is provably slower than\nFISTA. Moving on to parallel coordinate descent methods, we then propose an\nefficient accelerated version of Shotgun, improving the convergence rate from\n$O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysis\ncompared to previous work, and also allows one to study several connected work\nin a unified way.",
    "published": "2014-11-25T04:36:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite\n  Feasibility Problems",
    "authors": [
      "Hui Li",
      "Chunhua Shen",
      "Anton van den Hengel",
      "Qinfeng Shi"
    ],
    "summary": "In this paper, we propose an efficient semidefinite programming (SDP)\napproach to worst-case linear discriminant analysis (WLDA). Compared with the\ntraditional LDA, WLDA considers the dimensionality reduction problem from the\nworst-case viewpoint, which is in general more robust for classification.\nHowever, the original problem of WLDA is non-convex and difficult to optimize.\nIn this paper, we reformulate the optimization problem of WLDA into a sequence\nof semidefinite feasibility problems. To efficiently solve the semidefinite\nfeasibility problems, we design a new scalable optimization method with\nquasi-Newton methods and eigen-decomposition being the core components. The\nproposed method is orders of magnitude faster than standard interior-point\nbased SDP solvers.\n  Experiments on a variety of classification problems demonstrate that our\napproach achieves better performance than standard LDA. Our method is also much\nfaster and more scalable than standard interior-point SDP solvers based WLDA.\nThe computational complexity for an SDP with $m$ constraints and matrices of\nsize $d$ by $d$ is roughly reduced from $\\mathcal{O}(m^3+md^3+m^2d^2)$ to\n$\\mathcal{O}(d^3)$ ($m>d$ in our case).",
    "published": "2014-11-27T02:52:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Graph Sensitive Indices for Comparing Clusterings",
    "authors": [
      "Zaeem Hussain",
      "Marina Meila"
    ],
    "summary": "This report discusses two new indices for comparing clusterings of a set of\npoints. The motivation for looking at new ways for comparing clusterings stems\nfrom the fact that the existing clustering indices are based on set cardinality\nalone and do not consider the positions of data points. The new indices,\nnamely, the Random Walk index (RWI) and Variation of Information with Neighbors\n(VIN), are both inspired by the clustering metric Variation of Information\n(VI). VI possesses some interesting theoretical properties which are also\ndesirable in a metric for comparing clusterings. We define our indices and\ndiscuss some of their explored properties which appear relevant for a\nclustering index. We also include the results of these indices on clusterings\nof some example data sets.",
    "published": "2014-11-27T13:19:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Guaranteed Matrix Completion via Non-convex Factorization",
    "authors": [
      "Ruoyu Sun",
      "Zhi-Quan Luo"
    ],
    "summary": "Matrix factorization is a popular approach for large-scale matrix completion.\nThe optimization formulation based on matrix factorization can be solved very\nefficiently by standard algorithms in practice. However, due to the\nnon-convexity caused by the factorization model, there is a limited theoretical\nunderstanding of this formulation. In this paper, we establish a theoretical\nguarantee for the factorization formulation to correctly recover the underlying\nlow-rank matrix. In particular, we show that under similar conditions to those\nin previous works, many standard optimization algorithms converge to the global\noptima of a factorization formulation, and recover the true low-rank matrix. We\nstudy the local geometry of a properly regularized factorization formulation\nand prove that any stationary point in a certain local region is globally\noptimal. A major difference of our work from the existing results is that we do\nnot need resampling in either the algorithm or its analysis. Compared to other\nworks on nonconvex optimization, one extra difficulty lies in analyzing\nnonconvex constrained optimization when the constraint (or the corresponding\nregularizer) is not \"consistent\" with the gradient direction. One technical\ncontribution is the perturbation analysis for non-symmetric matrix\nfactorization.",
    "published": "2014-11-28T20:52:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Loss Surfaces of Multilayer Networks",
    "authors": [
      "Anna Choromanska",
      "Mikael Henaff",
      "Michael Mathieu",
      "Gérard Ben Arous",
      "Yann LeCun"
    ],
    "summary": "We study the connection between the highly non-convex loss function of a\nsimple model of the fully-connected feed-forward neural network and the\nHamiltonian of the spherical spin-glass model under the assumptions of: i)\nvariable independence, ii) redundancy in network parametrization, and iii)\nuniformity. These assumptions enable us to explain the complexity of the fully\ndecoupled neural network through the prism of the results from random matrix\ntheory. We show that for large-size decoupled networks the lowest critical\nvalues of the random loss function form a layered structure and they are\nlocated in a well-defined band lower-bounded by the global minimum. The number\nof local minima outside that band diminishes exponentially with the size of the\nnetwork. We empirically verify that the mathematical model exhibits similar\nbehavior as the computer simulations, despite the presence of high dependencies\nin real networks. We conjecture that both simulated annealing and SGD converge\nto the band of low critical points, and that all critical points found there\nare local minima of high quality measured by the test error. This emphasizes a\nmajor difference between large- and small-size networks where for the latter\npoor quality local minima have non-zero probability of being recovered.\nFinally, we prove that recovering the global minimum becomes harder as the\nnetwork size increases and that it is in practice irrelevant as global minimum\noften leads to overfitting.",
    "published": "2014-11-30T15:48:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Easy Hyperparameter Search Using Optunity",
    "authors": [
      "Marc Claesen",
      "Jaak Simm",
      "Dusan Popovic",
      "Yves Moreau",
      "Bart De Moor"
    ],
    "summary": "Optunity is a free software package dedicated to hyperparameter optimization.\nIt contains various types of solvers, ranging from undirected methods to direct\nsearch, particle swarm and evolutionary optimization. The design focuses on\nease of use, flexibility, code clarity and interoperability with existing\nsoftware in all machine learning environments. Optunity is written in Python\nand contains interfaces to environments such as R and MATLAB. Optunity uses a\nBSD license and is freely available online at http://www.optunity.net.",
    "published": "2014-12-02T21:55:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Rates by Transferring from Auxiliary Hypotheses",
    "authors": [
      "Ilja Kuzborskij",
      "Francesco Orabona"
    ],
    "summary": "In this work we consider the learning setting where, in addition to the\ntraining set, the learner receives a collection of auxiliary hypotheses\noriginating from other tasks. We focus on a broad class of ERM-based linear\nalgorithms that can be instantiated with any non-negative smooth loss function\nand any strongly convex regularizer. We establish generalization and excess\nrisk bounds, showing that, if the algorithm is fed with a good combination of\nsource hypotheses, generalization happens at the fast rate $\\mathcal{O}(1/m)$\ninstead of the usual $\\mathcal{O}(1/\\sqrt{m})$. On the other hand, if the\nsource hypotheses combination is a misfit for the target task, we recover the\nusual learning rate. As a byproduct of our study, we also prove a new bound on\nthe Rademacher complexity of the smooth loss class under weaker assumptions\ncompared to previous works.",
    "published": "2014-12-04T11:01:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A parallel sampling based clustering",
    "authors": [
      "Aditya AV Sastry",
      "Kalyan Netti"
    ],
    "summary": "The problem of automatically clustering data is an age old problem. People\nhave created numerous algorithms to tackle this problem. The execution time of\nany of this algorithm grows with the number of input points and the number of\ncluster centers required. To reduce the number of input points we could average\nthe points locally and use the means or the local centers as the input for\nclustering. However since the required number of local centers is very high,\nrunning the clustering algorithm on the entire dataset to obtain these\nrepresentational points is very time consuming. To remedy this problem, in this\npaper we are proposing two subclustering schemes where by we subdivide the\ndataset into smaller sets and run the clustering algorithm on the smaller\ndatasets to obtain the required number of datapoints to run our clustering\nalgorithm with. As we are subdividing the given dataset, we could run\nclustering algorithm on each smaller piece of the dataset in parallel. We found\nthat both parallel and serial execution of this method to be much faster than\nthe original clustering algorithm and error in running the clustering algorithm\non a reduced set to be very less.",
    "published": "2014-12-05T10:50:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Consistent optimization of AMS by logistic loss minimization",
    "authors": [
      "Wojciech Kotłowski"
    ],
    "summary": "In this paper, we theoretically justify an approach popular among\nparticipants of the Higgs Boson Machine Learning Challenge to optimize\napproximate median significance (AMS). The approach is based on the following\ntwo-stage procedure. First, a real-valued function is learned by minimizing a\nsurrogate loss for binary classification, such as logistic loss, on the\ntraining sample. Then, a threshold is tuned on a separate validation sample, by\ndirect optimization of AMS. We show that the regret of the resulting\n(thresholded) classifier measured with respect to the squared AMS, is\nupperbounded by the regret of the underlying real-valued function measured with\nrespect to the logistic loss. Hence, we prove that minimizing logistic\nsurrogate is a consistent method of optimizing AMS.",
    "published": "2014-12-05T19:28:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Theano-based Large-Scale Visual Recognition with Multiple GPUs",
    "authors": [
      "Weiguang Ding",
      "Ruoyan Wang",
      "Fei Mao",
      "Graham Taylor"
    ],
    "summary": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)\nimplementation and its naive data parallelism on multiple GPUs. Our performance\non 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)\nrun on 1 GPU. To the best of our knowledge, this is the first open-source\nPython-based AlexNet implementation to-date.",
    "published": "2014-12-07T01:12:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Accurate Streaming Support Vector Machines",
    "authors": [
      "Vikram Nathan",
      "Sharath Raghvendra"
    ],
    "summary": "A widely-used tool for binary classification is the Support Vector Machine\n(SVM), a supervised learning technique that finds the \"maximum margin\" linear\nseparator between the two classes. While SVMs have been well studied in the\nbatch (offline) setting, there is considerably less work on the streaming\n(online) setting, which requires only a single pass over the data using\nsub-linear space. Existing streaming algorithms are not yet competitive with\nthe batch implementation. In this paper, we use the formulation of the SVM as a\nminimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based\noff of the blurred ball cover originally proposed by Agarwal and Sharathkumar.\nOur implementation consistently outperforms existing streaming SVM approaches\nand provides higher accuracies than libSVM on several datasets, thus making it\ncompetitive with the standard SVM batch implementation.",
    "published": "2014-12-08T08:46:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Sequential Labeling with online Deep Learning",
    "authors": [
      "Gang Chen",
      "Ran Xu",
      "Sargur Srihari"
    ],
    "summary": "Deep learning has attracted great attention recently and yielded the state of\nthe art performance in dimension reduction and classification problems.\nHowever, it cannot effectively handle the structured output prediction, e.g.\nsequential labeling. In this paper, we propose a deep learning structure, which\ncan learn discriminative features for sequential labeling problems. More\nspecifically, we add the inter-relationship between labels in our deep learning\nstructure, in order to incorporate the context information from the sequential\ndata. Thus, our model is more powerful than linear Conditional Random Fields\n(CRFs) because the objective function learns latent non-linear features so that\ntarget labeling can be better predicted. We pretrain the deep structure with\nstacked restricted Boltzmann machines (RBMs) for feature learning and optimize\nour objective function with online learning algorithm, a mixture of perceptron\ntraining and stochastic gradient descent. We test our model on different\nchallenge tasks, and show that our model outperforms significantly over the\ncompletive baselines.",
    "published": "2014-12-10T18:16:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool",
    "authors": [
      "Eugene Borovikov"
    ],
    "summary": "The purpose of this report is in examining the generalization performance of\nSupport Vector Machines (SVM) as a tool for pattern recognition and object\nclassification. The work is motivated by the growing popularity of the method\nthat is claimed to guarantee a good generalization performance for the task in\nhand. The method is implemented in MATLAB. SVMs based on various kernels are\ntested for classifying data from various domains.",
    "published": "2014-12-13T03:33:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Max-Margin based Discriminative Feature Learning",
    "authors": [
      "Changsheng Li",
      "Qingshan Liu",
      "Weishan Dong",
      "Xin Zhang",
      "Lin Yang"
    ],
    "summary": "In this paper, we propose a new max-margin based discriminative feature\nlearning method. Specifically, we aim at learning a low-dimensional feature\nrepresentation, so as to maximize the global margin of the data and make the\nsamples from the same class as close as possible. In order to enhance the\nrobustness to noise, a $l_{2,1}$ norm constraint is introduced to make the\ntransformation matrix in group sparsity. In addition, for multi-class\nclassification tasks, we further intend to learn and leverage the correlation\nrelationships among multiple class tasks for assisting in learning\ndiscriminative features. The experimental results demonstrate the power of the\nproposed method against the related state-of-the-art methods.",
    "published": "2014-12-16T02:55:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning from Data with Heterogeneous Noise using SGD",
    "authors": [
      "Shuang Song",
      "Kamalika Chaudhuri",
      "Anand D. Sarwate"
    ],
    "summary": "We consider learning from data of variable quality that may be obtained from\ndifferent heterogeneous sources. Addressing learning from heterogeneous data in\nits full generality is a challenging problem. In this paper, we adopt instead a\nmodel in which data is observed through heterogeneous noise, where the noise\nlevel reflects the quality of the data source. We study how to use stochastic\ngradient algorithms to learn in this model. Our study is motivated by two\nconcrete examples where this problem arises naturally: learning with local\ndifferential privacy based on data from multiple sources with different privacy\nrequirements, and learning from data with labels of variable quality.\n  The main contribution of this paper is to identify how heterogeneous noise\nimpacts performance. We show that given two datasets with heterogeneous noise,\nthe order in which to use them in standard SGD depends on the learning rate. We\npropose a method for changing the learning rate as a function of the\nheterogeneity, and prove new regret bounds for our method in two cases of\ninterest. Experiments on real data show that our method performs better than\nusing a single learning rate and using only the less noisy of the two datasets\nwhen the noise level is low to moderate.",
    "published": "2014-12-17T21:15:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dynamic Structure Embedded Online Multiple-Output Regression for Stream\n  Data",
    "authors": [
      "Changsheng Li",
      "Fan Wei",
      "Weishan Dong",
      "Qingshan Liu",
      "Xiangfeng Wang",
      "Xin Zhang"
    ],
    "summary": "Online multiple-output regression is an important machine learning technique\nfor modeling, predicting, and compressing multi-dimensional correlated data\nstreams. In this paper, we propose a novel online multiple-output regression\nmethod, called MORES, for stream data. MORES can \\emph{dynamically} learn the\nstructure of the coefficients change in each update step to facilitate the\nmodel's continuous refinement. We observe that limited expressive ability of\nthe regression model, especially in the preliminary stage of online update,\noften leads to the variables in the residual errors being dependent. In light\nof this point, MORES intends to \\emph{dynamically} learn and leverage the\nstructure of the residual errors to improve the prediction accuracy. Moreover,\nwe define three statistical variables to \\emph{exactly} represent all the seen\nsamples for \\emph{incrementally} calculating prediction loss in each online\nupdate round, which can avoid loading all the training data into memory for\nupdating model, and also effectively prevent drastic fluctuation of the model\nin the presence of noise. Furthermore, we introduce a forgetting factor to set\ndifferent weights on samples so as to track the data streams' evolving\ncharacteristics quickly from the latest samples. Experiments on one synthetic\ndataset and three real-world datasets validate the effectiveness of the\nproposed method. In addition, the update speed of MORES is at least 2000\nsamples processed per second on the three real-world datasets, more than 15\ntimes faster than the state-of-the-art online learning algorithm.",
    "published": "2014-12-18T06:37:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Large Scale Distributed Distance Metric Learning",
    "authors": [
      "Pengtao Xie",
      "Eric Xing"
    ],
    "summary": "In large scale machine learning and data mining problems with high feature\ndimensionality, the Euclidean distance between data points can be\nuninformative, and Distance Metric Learning (DML) is often desired to learn a\nproper similarity measure (using side information such as example data pairs\nbeing similar or dissimilar). However, high dimensionality and large volume of\npairwise constraints in modern big data can lead to prohibitive computational\ncost for both the original DML formulation in Xing et al. (2002) and later\nextensions. In this paper, we present a distributed algorithm for DML, and a\nlarge-scale implementation on a parameter server architecture. Our approach\nbuilds on a parallelizable reformulation of Xing et al. (2002), and an\nasynchronous stochastic gradient descent optimization procedure. To our\nknowledge, this is the first distributed solution to DML, and we show that, on\na system with 256 CPU cores, our program is able to complete a DML task on a\ndataset with 1 million data points, 22-thousand features, and 200 million\nlabeled data pairs, in 15 hours; and the learned metric shows great\neffectiveness in properly measuring distances.",
    "published": "2014-12-18T17:14:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Algorithmic Robustness for Learning via $(ε, γ, τ)$-Good\n  Similarity Functions",
    "authors": [
      "Maria-Irina Nicolae",
      "Marc Sebban",
      "Amaury Habrard",
      "Éric Gaussier",
      "Massih-Reza Amini"
    ],
    "summary": "The notion of metric plays a key role in machine learning problems such as\nclassification, clustering or ranking. However, it is worth noting that there\nis a severe lack of theoretical guarantees that can be expected on the\ngeneralization capacity of the classifier associated to a given metric. The\ntheoretical framework of $(\\epsilon, \\gamma, \\tau)$-good similarity functions\n(Balcan et al., 2008) has been one of the first attempts to draw a link between\nthe properties of a similarity function and those of a linear classifier making\nuse of it. In this paper, we extend and complete this theory by providing a new\ngeneralization bound for the associated classifier based on the algorithmic\nrobustness framework.",
    "published": "2014-12-19T17:43:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fast Label Embeddings via Randomized Linear Algebra",
    "authors": [
      "Paul Mineiro",
      "Nikos Karampatziakis"
    ],
    "summary": "Many modern multiclass and multilabel problems are characterized by\nincreasingly large output spaces. For these problems, label embeddings have\nbeen shown to be a useful primitive that can improve computational and\nstatistical efficiency. In this work we utilize a correspondence between rank\nconstrained estimation and low dimensional label embeddings that uncovers a\nfast label embedding algorithm which works in both the multiclass and\nmultilabel settings. The result is a randomized algorithm whose running time is\nexponentially faster than naive algorithms. We demonstrate our techniques on\ntwo large-scale public datasets, from the Large Scale Hierarchical Text\nChallenge and the Open Directory Project, where we obtain state of the art\nresults.",
    "published": "2014-12-19T22:09:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Hot Swapping for Online Adaptation of Optimization Hyperparameters",
    "authors": [
      "Kevin Bache",
      "Dennis DeCoste",
      "Padhraic Smyth"
    ],
    "summary": "We describe a general framework for online adaptation of optimization\nhyperparameters by `hot swapping' their values during learning. We investigate\nthis approach in the context of adaptive learning rate selection using an\nexplore-exploit strategy from the multi-armed bandit literature. Experiments on\na benchmark neural network show that the hot swapping approach leads to\nconsistently better solutions compared to well-known alternatives such as\nAdaDelta and stochastic gradient with exhaustive hyperparameter search.",
    "published": "2014-12-20T04:36:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Understanding Minimum Probability Flow for RBMs Under Various Kinds of\n  Dynamics",
    "authors": [
      "Daniel Jiwoong Im",
      "Ethan Buchman",
      "Graham W. Taylor"
    ],
    "summary": "Energy-based models are popular in machine learning due to the elegance of\ntheir formulation and their relationship to statistical physics. Among these,\nthe Restricted Boltzmann Machine (RBM), and its staple training algorithm\ncontrastive divergence (CD), have been the prototype for some recent\nadvancements in the unsupervised training of deep neural networks. However, CD\nhas limited theoretical motivation, and can in some cases produce undesirable\nbehavior. Here, we investigate the performance of Minimum Probability Flow\n(MPF) learning for training RBMs. Unlike CD, with its focus on approximating an\nintractable partition function via Gibbs sampling, MPF proposes a tractable,\nconsistent, objective function defined in terms of a Taylor expansion of the KL\ndivergence with respect to sampling dynamics. Here we propose a more general\nform for the sampling dynamics in MPF, and explore the consequences of\ndifferent choices for these dynamics for training RBMs. Experimental results\nshow MPF outperforming CD for various RBM configurations.",
    "published": "2014-12-20T07:08:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adam: A Method for Stochastic Optimization",
    "authors": [
      "Diederik P. Kingma",
      "Jimmy Ba"
    ],
    "summary": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
    "published": "2014-12-22T13:54:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Discriminative Clustering with Relative Constraints",
    "authors": [
      "Yuanli Pei",
      "Xiaoli Z. Fern",
      "Rómer Rosales",
      "Teresa Vania Tjahja"
    ],
    "summary": "We study the problem of clustering with relative constraints, where each\nconstraint specifies relative similarities among instances. In particular, each\nconstraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$\nmore similar to $x_j$ than to $x_k$? We consider the scenario where answers to\nsuch queries are based on an underlying (but unknown) class concept, which we\naim to discover via clustering. Different from most existing methods that only\nconsider constraints derived from yes and no answers, we also incorporate don't\nknow responses. We introduce a Discriminative Clustering method with Relative\nConstraints (DCRC) which assumes a natural probabilistic relationship between\ninstances, their underlying cluster memberships, and the observed constraints.\nThe objective is to maximize the model likelihood given the constraints, and in\nthe meantime enforce cluster separation and cluster balance by also making use\nof the unlabeled instances. We evaluated the proposed method using constraints\ngenerated from ground-truth class labels, and from (noisy) human judgments from\na user study. Experimental results demonstrate: 1) the usefulness of relative\nconstraints, in particular when don't know answers are considered; 2) the\nimproved performance of the proposed method over state-of-the-art methods that\nutilize either relative or pairwise constraints; and 3) the robustness of our\nmethod in the presence of noisy constraints, such as those provided by human\njudgement.",
    "published": "2014-12-30T22:34:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comprehend DeepWalk as Matrix Factorization",
    "authors": [
      "Cheng Yang",
      "Zhiyuan Liu"
    ],
    "summary": "Word2vec, as an efficient tool for learning vector representation of words\nhas shown its effectiveness in many natural language processing tasks. Mikolov\net al. issued Skip-Gram and Negative Sampling model for developing this\ntoolbox. Perozzi et al. introduced the Skip-Gram model into the study of social\nnetwork for the first time, and designed an algorithm named DeepWalk for\nlearning node embedding on a graph. We prove that the DeepWalk algorithm is\nactually factoring a matrix M where each entry M_{ij} is logarithm of the\naverage probability that node i randomly walks to node j in fix steps.",
    "published": "2015-01-02T07:57:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Enhancing The Performance Of Nearest Neighbour Classifiers Using\n  Hassanat Distance Metric",
    "authors": [
      "Mouhammd Alkasassbeh",
      "Ghada A. Altarawneh",
      "Ahmad B. A. Hassanat"
    ],
    "summary": "We showed in this work how the Hassanat distance metric enhances the\nperformance of the nearest neighbour classifiers. The results demonstrate the\nsuperiority of this distance metric over the traditional and most-used\ndistances, such as Manhattan distance and Euclidian distance. Moreover, we\nproved that the Hassanat distance metric is invariant to data scale, noise and\noutliers. Throughout this work, it is clearly notable that both ENN and IINC\nperformed very well with the distance investigated, as their accuracy increased\nsignificantly by 3.3% and 3.1% respectively, with no significant advantage of\nthe ENN over the IINC in terms of accuracy. Correspondingly, it can be noted\nfrom our results that there is no optimal algorithm that can solve all\nreal-life problems perfectly; this is supported by the no-free-lunch theorem",
    "published": "2015-01-04T15:37:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Differential Search Algorithm-based Parametric Optimization of Fuzzy\n  Generalized Eigenvalue Proximal Support Vector Machine",
    "authors": [
      "M. H. Marghny",
      "Rasha M. Abd ElAziz",
      "Ahmed I. Taloba"
    ],
    "summary": "Support Vector Machine (SVM) is an effective model for many classification\nproblems. However, SVM needs the solution of a quadratic program which require\nspecialized code. In addition, SVM has many parameters, which affects the\nperformance of SVM classifier. Recently, the Generalized Eigenvalue Proximal\nSVM (GEPSVM) has been presented to solve the SVM complexity. In real world\napplications data may affected by error or noise, working with this data is a\nchallenging problem. In this paper, an approach has been proposed to overcome\nthis problem. This method is called DSA-GEPSVM. The main improvements are\ncarried out based on the following: 1) a novel fuzzy values in the linear case.\n2) A new Kernel function in the nonlinear case. 3) Differential Search\nAlgorithm (DSA) is reformulated to find near optimal values of the GEPSVM\nparameters and its kernel parameters. The experimental results show that the\nproposed approach is able to find the suitable parameter values, and has higher\nclassification accuracy compared with some other algorithms.",
    "published": "2015-01-04T22:12:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Online Relative Comparison Kernel Learning",
    "authors": [
      "Eric Heim",
      "Matthew Berger",
      "Lee M. Seversky",
      "Milos Hauskrecht"
    ],
    "summary": "Learning a kernel matrix from relative comparison human feedback is an\nimportant problem with applications in collaborative filtering, object\nretrieval, and search. For learning a kernel over a large number of objects,\nexisting methods face significant scalability issues inhibiting the application\nof these methods to settings where a kernel is learned in an online and timely\nfashion. In this paper we propose a novel framework called Efficient online\nRelative comparison Kernel LEarning (ERKLE), for efficiently learning the\nsimilarity of a large set of objects in an online manner. We learn a kernel\nfrom relative comparisons via stochastic gradient descent, one query response\nat a time, by taking advantage of the sparse and low-rank properties of the\ngradient to efficiently restrict the kernel to lie in the space of positive\nsemidefinite matrices. In addition, we derive a passive-aggressive online\nupdate for minimally satisfying new relative comparisons as to not disrupt the\ninfluence of previously obtained comparisons. Experimentally, we demonstrate a\nconsiderable improvement in speed while obtaining improved or comparable\naccuracy compared to current methods in the online learning setting.",
    "published": "2015-01-06T17:19:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening\n  Data",
    "authors": [
      "Lee Zamparo",
      "Zhaolei Zhang"
    ],
    "summary": "High-content screening uses large collections of unlabeled cell image data to\nreason about genetics or cell biology. Two important tasks are to identify\nthose cells which bear interesting phenotypes, and to identify sub-populations\nenriched for these phenotypes. This exploratory data analysis usually involves\ndimensionality reduction followed by clustering, in the hope that clusters\nrepresent a phenotype. We propose the use of stacked de-noising auto-encoders\nto perform dimensionality reduction for high-content screening. We demonstrate\nthe superior performance of our approach over PCA, Local Linear Embedding,\nKernel PCA and Isomap.",
    "published": "2015-01-07T02:13:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Gaussian Particle Filter Approach for Sensors to Track Multiple Moving\n  Targets",
    "authors": [
      "Haojun Li"
    ],
    "summary": "In a variety of problems, the number and state of multiple moving targets are\nunknown and are subject to be inferred from their measurements obtained by a\nsensor with limited sensing ability. This type of problems is raised in a\nvariety of applications, including monitoring of endangered species, cleaning,\nand surveillance. Particle filters are widely used to estimate target state\nfrom its prior information and its measurements that recently become available,\nespecially for the cases when the measurement model and the prior distribution\nof state of interest are non-Gaussian. However, the problem of estimating\nnumber of total targets and their state becomes intractable when the number of\ntotal targets and the measurement-target association are unknown. This paper\npresents a novel Gaussian particle filter technique that combines Kalman filter\nand particle filter for estimating the number and state of total targets based\non the measurement obtained online. The estimation is represented by a set of\nweighted particles, different from classical particle filter, where each\nparticle is a Gaussian distribution instead of a point mass.",
    "published": "2015-01-11T02:24:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC\n  dimension",
    "authors": [
      "Jayadeva",
      "Sanjit Singh Batra",
      "Siddarth Sabharwal"
    ],
    "summary": "The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning\nmachine, and a low VC dimension leads to good generalization. The recently\nproposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by\nminimizing an exact bound on the VC dimension. This paper extends the MCM\nclassifier to the fuzzy domain. The use of a fuzzy membership is known to\nreduce the effect of outliers, and to reduce the effect of noise on learning.\nExperimental results show, that on a number of benchmark datasets, the the\nfuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of\ngeneralization, and that the fuzzy MCM uses fewer support vectors. On several\nbenchmark datasets, the fuzzy MCM classifier yields excellent test set\naccuracies while using one-tenth the number of support vectors used by SVMs.",
    "published": "2015-01-11T09:29:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Max-Cost Discrete Function Evaluation Problem under a Budget",
    "authors": [
      "Feng Nan",
      "Joseph Wang",
      "Venkatesh Saligrama"
    ],
    "summary": "We propose novel methods for max-cost Discrete Function Evaluation Problem\n(DFEP) under budget constraints. We are motivated by applications such as\nclinical diagnosis where a patient is subjected to a sequence of (possibly\nexpensive) tests before a decision is made. Our goal is to develop strategies\nfor minimizing max-costs. The problem is known to be NP hard and greedy methods\nbased on specialized impurity functions have been proposed. We develop a broad\nclass of \\emph{admissible} impurity functions that admit monomials, classes of\npolynomials, and hinge-loss functions that allow for flexible impurity design\nwith provably optimal approximation bounds. This flexibility is important for\ndatasets when max-cost can be overly sensitive to \"outliers.\" Outliers bias\nmax-cost to a few examples that require a large number of tests for\nclassification. We design admissible functions that allow for accuracy-cost\ntrade-off and result in $O(\\log n)$ guarantees of the optimal cost among trees\nwith corresponding classification accuracy levels.",
    "published": "2015-01-12T16:33:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Learning with Nonparametric Clustering",
    "authors": [
      "Gang Chen"
    ],
    "summary": "Clustering is an essential problem in machine learning and data mining. One\nvital factor that impacts clustering performance is how to learn or design the\ndata representation (or features). Fortunately, recent advances in deep\nlearning can learn unsupervised features effectively, and have yielded state of\nthe art performance in many classification problems, such as character\nrecognition, object recognition and document categorization. However, little\nattention has been paid to the potential of deep learning for unsupervised\nclustering problems. In this paper, we propose a deep belief network with\nnonparametric clustering. As an unsupervised method, our model first leverages\nthe advantages of deep learning for feature representation and dimension\nreduction. Then, it performs nonparametric clustering under a maximum margin\nframework -- a discriminative clustering model and can be trained online\nefficiently in the code space. Lastly model parameters are refined in the deep\nbelief network. Thus, this model can learn features for clustering and infer\nmodel complexity in an unified framework. The experimental results show the\nadvantage of our approach over competitive baselines.",
    "published": "2015-01-13T17:26:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Classification with Low Rank and Missing Data",
    "authors": [
      "Elad Hazan",
      "Roi Livni",
      "Yishay Mansour"
    ],
    "summary": "We consider classification and regression tasks where we have missing data\nand assume that the (clean) data resides in a low rank subspace. Finding a\nhidden subspace is known to be computationally hard. Nevertheless, using a\nnon-proper formulation we give an efficient agnostic algorithm that classifies\nas good as the best linear classifier coupled with the best low-dimensional\nsubspace in which the data resides. A direct implication is that our algorithm\ncan linearly (and non-linearly through kernels) classify provably as well as\nthe best classifier that has access to the full data.",
    "published": "2015-01-14T08:16:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Proximal Approach for Sparse Multiclass SVM",
    "authors": [
      "G. Chierchia",
      "Nelly Pustelnik",
      "Jean-Christophe Pesquet",
      "B. Pesquet-Popescu"
    ],
    "summary": "Sparsity-inducing penalties are useful tools to design multiclass support\nvector machines (SVMs). In this paper, we propose a convex optimization\napproach for efficiently and exactly solving the multiclass SVM learning\nproblem involving a sparse regularization and the multiclass hinge loss\nformulated by Crammer and Singer. We provide two algorithms: the first one\ndealing with the hinge loss as a penalty term, and the other one addressing the\ncase when the hinge loss is enforced through a constraint. The related convex\noptimization problems can be efficiently solved thanks to the flexibility\noffered by recent primal-dual proximal algorithms and epigraphical splitting\ntechniques. Experiments carried out on several datasets demonstrate the\ninterest of considering the exact expression of the hinge loss rather than a\nsmooth approximation. The efficiency of the proposed algorithms w.r.t. several\nstate-of-the-art methods is also assessed through comparisons of execution\ntimes.",
    "published": "2015-01-15T13:23:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-view learning for multivariate performance measures optimization",
    "authors": [
      "Jim Jing-Yan Wang"
    ],
    "summary": "In this paper, we propose the problem of optimizing multivariate performance\nmeasures from multi-view data, and an effective method to solve it. This\nproblem has two features: the data points are presented by multiple views, and\nthe target of learning is to optimize complex multivariate performance\nmeasures. We propose to learn a linear discriminant functions for each view,\nand combine them to construct a overall multivariate mapping function for\nmult-view data. To learn the parameters of the linear dis- criminant functions\nof different views to optimize multivariate performance measures, we formulate\na optimization problem. In this problem, we propose to minimize the complexity\nof the linear discriminant functions of each view, encourage the consistences\nof the responses of different views over the same data points, and minimize the\nupper boundary of a given multivariate performance measure. To optimize this\nproblem, we employ the cutting-plane method in an iterative algorithm. In each\niteration, we update a set of constrains, and optimize the mapping function\nparameter of each view one by one.",
    "published": "2015-01-15T19:33:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generalised Random Forest Space Overview",
    "authors": [
      "Miron B. Kursa"
    ],
    "summary": "Assuming a view of the Random Forest as a special case of a nested ensemble\nof interchangeable modules, we construct a generalisation space allowing one to\neasily develop novel methods based on this algorithm. We discuss the role and\nrequired properties of modules at each level, especially in context of some\nalready proposed RF generalisations.",
    "published": "2015-01-17T23:42:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Comment on \"Clustering by fast search and find of density peaks\"",
    "authors": [
      "Shuliang Wang",
      "Dakui Wang",
      "Caoyuan Li",
      "Yan Li"
    ],
    "summary": "In [1], a clustering algorithm was given to find the centers of clusters\nquickly. However, the accuracy of this algorithm heavily depend on the\nthreshold value of d-c. Furthermore, [1] has not provided any efficient way to\nselect the threshold value of d-c, that is, one can have to estimate the value\nof d_c depend on one's subjective experience. In this paper, based on the data\nfield [2], we propose a new way to automatically extract the threshold value of\nd_c from the original data set by using the potential entropy of data field.\nFor any data set to be clustered, the most reasonable value of d_c can be\nobjectively calculated from the data set by using our proposed method. The same\nexperiments in [1] are redone with our proposed method on the same experimental\ndata set used in [1], the results of which shows that the problem to calculate\nthe threshold value of d_c in [1] has been solved by using our method.",
    "published": "2015-01-18T05:15:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Regularized maximum correntropy machine",
    "authors": [
      "Jim Jing-Yan Wang",
      "Yunji Wang",
      "Bing-Yi Jing",
      "Xin Gao"
    ],
    "summary": "In this paper we investigate the usage of regularized correntropy framework\nfor learning of classifiers from noisy labels. The class label predictors\nlearned by minimizing transitional loss functions are sensitive to the noisy\nand outlying labels of training samples, because the transitional loss\nfunctions are equally applied to all the samples. To solve this problem, we\npropose to learn the class label predictors by maximizing the correntropy\nbetween the predicted labels and the true labels of the training samples, under\nthe regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we\nregularize the predictor parameter to control the complexity of the predictor.\nThe learning problem is formulated by an objective function considering the\nparameter regularization and MCC simultaneously. By optimizing the objective\nfunction alternately, we develop a novel predictor learning algorithm. The\nexperiments on two chal- lenging pattern classification tasks show that it\nsignificantly outperforms the machines with transitional loss functions.",
    "published": "2015-01-18T11:46:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extreme Entropy Machines: Robust information theoretic classification",
    "authors": [
      "Wojciech Marian Czarnecki",
      "Jacek Tabor"
    ],
    "summary": "Most of the existing classification methods are aimed at minimization of\nempirical risk (through some simple point-based error measured with loss\nfunction) with added regularization. We propose to approach this problem in a\nmore information theoretic way by investigating applicability of entropy\nmeasures as a classification model objective function. We focus on quadratic\nRenyi's entropy and connected Cauchy-Schwarz Divergence which leads to the\nconstruction of Extreme Entropy Machines (EEM).\n  The main contribution of this paper is proposing a model based on the\ninformation theoretic concepts which on the one hand shows new, entropic\nperspective on known linear classifiers and on the other leads to a\nconstruction of very robust method competetitive with the state of the art\nnon-information theoretic ones (including Support Vector Machines and Extreme\nLearning Machines).\n  Evaluation on numerous problems spanning from small, simple ones from UCI\nrepository to the large (hundreads of thousands of samples) extremely\nunbalanced (up to 100:1 classes' ratios) datasets shows wide applicability of\nthe EEM in real life problems and that it scales well.",
    "published": "2015-01-21T19:54:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Transductive Semi-supervised Maximum Margin Clustering",
    "authors": [
      "Gang Chen"
    ],
    "summary": "Semi-supervised clustering is an very important topic in machine learning and\ncomputer vision. The key challenge of this problem is how to learn a metric,\nsuch that the instances sharing the same label are more likely close to each\nother on the embedded space. However, little attention has been paid to learn\nbetter representations when the data lie on non-linear manifold. Fortunately,\ndeep learning has led to great success on feature learning recently. Inspired\nby the advances of deep learning, we propose a deep transductive\nsemi-supervised maximum margin clustering approach. More specifically, given\npairwise constraints, we exploit both labeled and unlabeled data to learn a\nnon-linear mapping under maximum margin framework for clustering analysis.\nThus, our model unifies transductive learning, feature learning and maximum\nmargin techniques in the semi-supervised clustering framework. We pretrain the\ndeep network structure with restricted Boltzmann machines (RBMs) layer by layer\ngreedily, and optimize our objective function with gradient descent. By\nchecking the most violated constraints, our approach updates the model\nparameters through error backpropagation, in which deep features are learned\nautomatically. The experimental results shows that our model is significantly\nbetter than the state of the art on semi-supervised clustering.",
    "published": "2015-01-26T02:28:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On a Family of Decomposable Kernels on Sequences",
    "authors": [
      "Andrea Baisero",
      "Florian T. Pokorny",
      "Carl Henrik Ek"
    ],
    "summary": "In many applications data is naturally presented in terms of orderings of\nsome basic elements or symbols. Reasoning about such data requires a notion of\nsimilarity capable of handling sequences of different lengths. In this paper we\ndescribe a family of Mercer kernel functions for such sequentially structured\ndata. The family is characterized by a decomposable structure in terms of\nsymbol-level and structure-level similarities, representing a specific\ncombination of kernels which allows for efficient computation. We provide an\nexperimental evaluation on sequential classification tasks comparing kernels\nfrom our family of kernels to a state of the art sequence kernel called the\nGlobal Alignment kernel which has been shown to outperform Dynamic Time Warping",
    "published": "2015-01-26T08:30:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Compressed Support Vector Machines",
    "authors": [
      "Zhixiang Xu",
      "Jacob R. Gardner",
      "Stephen Tyree",
      "Kilian Q. Weinberger"
    ],
    "summary": "Support vector machines (SVM) can classify data sets along highly non-linear\ndecision boundaries because of the kernel-trick. This expressiveness comes at a\nprice: During test-time, the SVM classifier needs to compute the kernel\ninner-product between a test sample and all support vectors. With large\ntraining data sets, the time required for this computation can be substantial.\nIn this paper, we introduce a post-processing algorithm, which compresses the\nlearned SVM model by reducing and optimizing support vectors. We evaluate our\nalgorithm on several medium-scaled real-world data sets, demonstrating that it\nmaintains high test accuracy while reducing the test-time evaluation cost by\nseveral orders of magnitude---in some cases from hours to seconds. It is fair\nto say that most of the work in this paper was previously been invented by\nBurges and Sch\\\"olkopf almost 20 years ago. For most of the time during which\nwe conducted this research, we were unaware of this prior work. However, in the\npast two decades, computing power has increased drastically, and we can\ntherefore provide empirical insights that were not possible in their original\npaper.",
    "published": "2015-01-26T16:51:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Novel Approaches for Predicting Risk Factors of Atherosclerosis",
    "authors": [
      "V. Sree Hari Rao",
      "M. Naresh Kumar"
    ],
    "summary": "Coronary heart disease (CHD) caused by hardening of artery walls due to\ncholesterol known as atherosclerosis is responsible for large number of deaths\nworld-wide. The disease progression is slow, asymptomatic and may lead to\nsudden cardiac arrest, stroke or myocardial infraction. Presently, imaging\ntechniques are being employed to understand the molecular and metabolic\nactivity of atherosclerotic plaques to estimate the risk. Though imaging\nmethods are able to provide some information on plaque metabolism they lack the\nrequired resolution and sensitivity for detection. In this paper we consider\nthe clinical observations and habits of individuals for predicting the risk\nfactors of CHD. The identification of risk factors helps in stratifying\npatients for further intensive tests such as nuclear imaging or coronary\nangiography. We present a novel approach for predicting the risk factors of\natherosclerosis with an in-built imputation algorithm and particle swarm\noptimization (PSO). We compare the performance of our methodology with other\nmachine learning techniques on STULONG dataset which is based on longitudinal\nstudy of middle aged individuals lasting for twenty years. Our methodology\npowered by PSO search has identified physical inactivity as one of the risk\nfactor for the onset of atherosclerosis in addition to other already known\nfactors. The decision rules extracted by our methodology are able to predict\nthe risk factors with an accuracy of $99.73%$ which is higher than the\naccuracies obtained by application of the state-of-the-art machine learning\ntechniques presently being employed in the identification of atherosclerosis\nrisk studies.",
    "published": "2015-01-28T13:26:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation",
    "authors": [
      "Konstantinos Slavakis",
      "Georgios B. Giannakis"
    ],
    "summary": "Applications involving dictionary learning, non-negative matrix\nfactorization, subspace clustering, and parallel factor tensor decomposition\ntasks motivate well algorithms for per-block-convex and non-smooth optimization\nproblems. By leveraging the stochastic approximation paradigm and first-order\nacceleration schemes, this paper develops an online and modular learning\nalgorithm for a large class of non-convex data models, where convexity is\nmanifested only per-block of variables whenever the rest of them are held\nfixed. The advocated algorithm incurs computational complexity that scales\nlinearly with the number of unknowns. Under minimal assumptions on the cost\nfunctions of the composite optimization task, without bounding constraints on\nthe optimization variables, or any explicit information on bounds of Lipschitz\ncoefficients, the expected cost evaluated online at the resultant iterates is\nprovably convergent with quadratic rate to an accumulation point of the\n(per-block) minima, while subgradients of the expected cost asymptotically\nvanish in the mean-squared sense. The merits of the general approach are\ndemonstrated in two online learning setups: (i) Robust linear regression using\na sparsity-cognizant total least-squares criterion; and (ii) semi-supervised\ndictionary learning for network-wide link load tracking and imputation with\nmissing entries. Numerical tests on synthetic and real data highlight the\npotential of the proposed framework for streaming data analytics by\ndemonstrating superior performance over block coordinate descent, and reduced\ncomplexity relative to the popular alternating-direction method of multipliers.",
    "published": "2015-01-29T00:15:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Divide-And-Conquer Classification Based on Feature-Space\n  Decomposition",
    "authors": [
      "Qi Guo",
      "Bo-Wei Chen",
      "Feng Jiang",
      "Xiangyang Ji",
      "Sun-Yuan Kung"
    ],
    "summary": "This study presents a divide-and-conquer (DC) approach based on feature space\ndecomposition for classification. When large-scale datasets are present,\ntypical approaches usually employed truncated kernel methods on the feature\nspace or DC approaches on the sample space. However, this did not guarantee\nseparability between classes, owing to overfitting. To overcome such problems,\nthis work proposes a novel DC approach on feature spaces consisting of three\nsteps. Firstly, we divide the feature space into several subspaces using the\ndecomposition method proposed in this paper. Subsequently, these feature\nsubspaces are sent into individual local classifiers for training. Finally, the\noutcomes of local classifiers are fused together to generate the final\nclassification results. Experiments on large-scale datasets are carried out for\nperformance evaluation. The results show that the error rates of the proposed\nDC method decreased comparing with the state-of-the-art fast SVM solvers, e.g.,\nreducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets\nrespectively.",
    "published": "2015-01-29T20:41:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Representing Objects, Relations, and Sequences",
    "authors": [
      "Stephen I. Gallant",
      "T. Wendy Okaywe"
    ],
    "summary": "Vector Symbolic Architectures (VSAs) are high-dimensional vector\nrepresentations of objects (eg., words, image parts), relations (eg., sentence\nstructures), and sequences for use with machine learning algorithms. They\nconsist of a vector addition operator for representing a collection of\nunordered objects, a Binding operator for associating groups of objects, and a\nmethodology for encoding complex structures.\n  We first develop Constraints that machine learning imposes upon VSAs: for\nexample, similar structures must be represented by similar vectors. The\nconstraints suggest that current VSAs should represent phrases (\"The smart\nBrazilian girl\") by binding sums of terms, in addition to simply binding the\nterms directly.\n  We show that matrix multiplication can be used as the binding operator for a\nVSA, and that matrix elements can be chosen at random. A consequence for living\nsystems is that binding is mathematically possible without the need to specify,\nin advance, precise neuron-to-neuron connection properties for large numbers of\nsynapses.\n  A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms),\nis described that satisfies all Constraints.\n  With respect to machine learning, for some types of problems appropriate VSA\nrepresentations permit us to prove learnability, rather than relying on\nsimulations. We also propose dividing machine (and neural) learning and\nrepresentation into three Stages, with differing roles for learning in each\nstage.\n  For neural modeling, we give \"representational reasons\" for nervous systems\nto have many recurrent connections, as well as for the importance of phrases in\nlanguage processing.\n  Sizing simulations and analyses suggest that VSAs in general, and MBAT in\nparticular, are ready for real-world applications.",
    "published": "2015-01-29T22:13:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Batchwise Monotone Algorithm for Dictionary Learning",
    "authors": [
      "Huan Wang",
      "John Wright",
      "Daniel Spielman"
    ],
    "summary": "We propose a batchwise monotone algorithm for dictionary learning. Unlike the\nstate-of-the-art dictionary learning algorithms which impose sparsity\nconstraints on a sample-by-sample basis, we instead treat the samples as a\nbatch, and impose the sparsity constraint on the whole. The benefit of\nbatchwise optimization is that the non-zeros can be better allocated across the\nsamples, leading to a better approximation of the whole. To accomplish this, we\npropose procedures to switch non-zeros in both rows and columns in the support\nof the coefficient matrix to reduce the reconstruction error. We prove in the\nproposed support switching procedure the objective of the algorithm, i.e., the\nreconstruction error, decreases monotonically and converges. Furthermore, we\nintroduce a block orthogonal matching pursuit algorithm that also operates on\nsample batches to provide a warm start. Experiments on both natural image\npatches and UCI data sets show that the proposed algorithm produces a better\napproximation with the same sparsity levels compared to the state-of-the-art\nalgorithms.",
    "published": "2015-01-31T03:40:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Lock in Feedback in Sequential Experiments",
    "authors": [
      "Maurits Kaptein",
      "Davide Iannuzzi"
    ],
    "summary": "We often encounter situations in which an experimenter wants to find, by\nsequential experimentation, $x_{max} = \\arg\\max_{x} f(x)$, where $f(x)$ is a\n(possibly unknown) function of a well controllable variable $x$. Taking\ninspiration from physics and engineering, we have designed a new method to\naddress this problem. In this paper, we first introduce the method in\ncontinuous time, and then present two algorithms for use in sequential\nexperiments. Through a series of simulation studies, we show that the method is\neffective for finding maxima of unknown functions by experimentation, even when\nthe maximum of the functions drifts or when the signal to noise ratio is low.",
    "published": "2015-02-02T20:00:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Parametric-Output HMMs with Two Aliased States",
    "authors": [
      "Roi Weiss",
      "Boaz Nadler"
    ],
    "summary": "In various applications involving hidden Markov models (HMMs), some of the\nhidden states are aliased, having identical output distributions. The\nminimality, identifiability and learnability of such aliased HMMs have been\nlong standing problems, with only partial solutions provided thus far. In this\npaper we focus on parametric-output HMMs, whose output distributions come from\na parametric family, and that have exactly two aliased states. For this class,\nwe present a complete characterization of their minimality and identifiability.\nFurthermore, for a large family of parametric output distributions, we derive\ncomputationally efficient and statistically consistent algorithms to detect the\npresence of aliasing and learn the aliased HMM transition and emission\nparameters. We illustrate our theoretical analysis by several simulations.",
    "published": "2015-02-07T16:21:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization",
    "authors": [
      "Zheng Qu",
      "Peter Richtárik",
      "Martin Takáč",
      "Olivier Fercoq"
    ],
    "summary": "We propose a new algorithm for minimizing regularized empirical loss:\nStochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each\niteration we update a random subset of the dual variables. However, unlike\nexisting methods such as stochastic dual coordinate ascent, SDNA is capable of\nutilizing all curvature information contained in the examples, which leads to\nstriking improvements in both theory and practice - sometimes by orders of\nmagnitude. In the special case when an L2-regularizer is used in the primal,\nthe dual problem is a concave quadratic maximization problem plus a separable\nterm. In this regime, SDNA in each step solves a proximal subproblem involving\na random principal submatrix of the Hessian of the quadratic function; whence\nthe name of the method. If, in addition, the loss functions are quadratic, our\nmethod can be interpreted as a novel variant of the recently introduced\nIterative Hessian Sketch.",
    "published": "2015-02-08T16:34:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Rademacher Observations, Private Data, and Boosting",
    "authors": [
      "Richard Nock",
      "Giorgio Patrini",
      "Arik Friedman"
    ],
    "summary": "The minimization of the logistic loss is a popular approach to batch\nsupervised learning. Our paper starts from the surprising observation that,\nwhen fitting linear (or kernelized) classifiers, the minimization of the\nlogistic loss is \\textit{equivalent} to the minimization of an exponential\n\\textit{rado}-loss computed (i) over transformed data that we call Rademacher\nobservations (rados), and (ii) over the \\textit{same} classifier as the one of\nthe logistic loss. Thus, a classifier learnt from rados can be\n\\textit{directly} used to classify \\textit{observations}. We provide a learning\nalgorithm over rados with boosting-compliant convergence rates on the\n\\textit{logistic loss} (computed over examples). Experiments on domains with up\nto millions of examples, backed up by theoretical arguments, display that\nlearning over a small set of random rados can challenge the state of the art\nthat learns over the \\textit{complete} set of examples. We show that rados\ncomply with various privacy requirements that make them good candidates for\nmachine learning in a privacy framework. We give several algebraic, geometric\nand computational hardness results on reconstructing examples from rados. We\nalso show how it is possible to craft, and efficiently learn from, rados in a\ndifferential privacy framework. Tests reveal that learning from differentially\nprivate rados can compete with learning from random rados, and hence with batch\nlearning from examples, achieving non-trivial privacy vs accuracy tradeoffs.",
    "published": "2015-02-09T01:12:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Infinite Restricted Boltzmann Machine",
    "authors": [
      "Marc-Alexandre Côté",
      "Hugo Larochelle"
    ],
    "summary": "We present a mathematical construction for the restricted Boltzmann machine\n(RBM) that doesn't require specifying the number of hidden units. In fact, the\nhidden layer size is adaptive and can grow during training. This is obtained by\nfirst extending the RBM to be sensitive to the ordering of its hidden units.\nThen, thanks to a carefully chosen definition of the energy function, we show\nthat the limit of infinitely many hidden units is well defined. As with RBM,\napproximate maximum likelihood training can be performed, resulting in an\nalgorithm that naturally and adaptively adds trained hidden units during\nlearning. We empirically study the behaviour of this infinite RBM, showing that\nits performance is competitive to that of the RBM, while not requiring the\ntuning of a hidden layer size.",
    "published": "2015-02-09T13:18:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction",
    "authors": [
      "Mohamed Elshrif",
      "Ernest Fokoue"
    ],
    "summary": "We present a novel adaptive random subspace learning algorithm (RSSL) for\nprediction purpose. This new framework is flexible where it can be adapted with\nany learning technique. In this paper, we tested the algorithm for regression\nand classification problems. In addition, we provide a variety of weighting\nschemes to increase the robustness of the developed algorithm. These different\nwighting flavors were evaluated on simulated as well as on real-world data sets\nconsidering the cases where the ratio between features (attributes) and\ninstances (samples) is large and vice versa. The framework of the new algorithm\nconsists of many stages: first, calculate the weights of all features on the\ndata set using the correlation coefficient and F-statistic statistical\nmeasurements. Second, randomly draw n samples with replacement from the data\nset. Third, perform regular bootstrap sampling (bagging). Fourth, draw without\nreplacement the indices of the chosen variables. The decision was taken based\non the heuristic subspacing scheme. Fifth, call base learners and build the\nmodel. Sixth, use the model for prediction purpose on test set of the data. The\nresults show the advancement of the adaptive RSSL algorithm in most of the\ncases compared with the synonym (conventional) machine learning algorithms.",
    "published": "2015-02-09T18:49:29Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal and Adaptive Algorithms for Online Boosting",
    "authors": [
      "Alina Beygelzimer",
      "Satyen Kale",
      "Haipeng Luo"
    ],
    "summary": "We study online boosting, the task of converting any weak online learner into\na strong online learner. Based on a novel and natural definition of weak online\nlearnability, we develop two online boosting algorithms. The first algorithm is\nan online version of boost-by-majority. By proving a matching lower bound, we\nshow that this algorithm is essentially optimal in terms of the number of weak\nlearners and the sample complexity needed to achieve a specified accuracy. This\noptimal algorithm is not adaptive however. Using tools from online loss\nminimization, we derive an adaptive online boosting algorithm that is also\nparameter-free, but not optimal. Both algorithms work with base learners that\ncan handle example importance weights directly, as well as by rejection\nsampling examples with probability defined by the booster. Results are\ncomplemented with an extensive experimental study.",
    "published": "2015-02-09T20:58:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Reductions that Really Work",
    "authors": [
      "Alina Beygelzimer",
      "Hal Daumé III",
      "John Langford",
      "Paul Mineiro"
    ],
    "summary": "We provide a summary of the mathematical and computational techniques that\nhave enabled learning reductions to effectively address a wide class of\nproblems, and show that this approach to solving machine learning problems can\nbe broadly useful.",
    "published": "2015-02-09T22:05:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scalable Multilabel Prediction via Randomized Methods",
    "authors": [
      "Nikos Karampatziakis",
      "Paul Mineiro"
    ],
    "summary": "Modeling the dependence between outputs is a fundamental challenge in\nmultilabel classification. In this work we show that a generic regularized\nnonlinearity mapping independent predictions to joint predictions is sufficient\nto achieve state-of-the-art performance on a variety of benchmark problems.\nCrucially, we compute the joint predictions without ever obtaining any\nindependent predictions, while incorporating low-rank and smoothness\nregularization. We achieve this by leveraging randomized algorithms for matrix\ndecomposition and kernel approximation. Furthermore, our techniques are\napplicable to the multiclass setting. We apply our method to a variety of\nmulticlass and multilabel data sets, obtaining state-of-the-art results.",
    "published": "2015-02-09T22:18:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Transferable Features with Deep Adaptation Networks",
    "authors": [
      "Mingsheng Long",
      "Yue Cao",
      "Jianmin Wang",
      "Michael I. Jordan"
    ],
    "summary": "Recent studies reveal that a deep neural network can learn transferable\nfeatures which generalize well to novel tasks for domain adaptation. However,\nas deep features eventually transition from general to specific along the\nnetwork, the feature transferability drops significantly in higher layers with\nincreasing domain discrepancy. Hence, it is important to formally reduce the\ndataset bias and enhance the transferability in task-specific layers. In this\npaper, we propose a new Deep Adaptation Network (DAN) architecture, which\ngeneralizes deep convolutional neural network to the domain adaptation\nscenario. In DAN, hidden representations of all task-specific layers are\nembedded in a reproducing kernel Hilbert space where the mean embeddings of\ndifferent domain distributions can be explicitly matched. The domain\ndiscrepancy is further reduced using an optimal multi-kernel selection method\nfor mean embedding matching. DAN can learn transferable features with\nstatistical guarantees, and can scale linearly by unbiased estimate of kernel\nembedding. Extensive empirical evidence shows that the proposed architecture\nyields state-of-the-art image classification error rates on standard domain\nadaptation benchmarks.",
    "published": "2015-02-10T06:01:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift",
    "authors": [
      "Sergey Ioffe",
      "Christian Szegedy"
    ],
    "summary": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
    "published": "2015-02-11T01:44:18Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite\n  Matrices",
    "authors": [
      "Florian Yger",
      "Masashi Sugiyama"
    ],
    "summary": "Metric learning has been shown to be highly effective to improve the\nperformance of nearest neighbor classification. In this paper, we address the\nproblem of metric learning for Symmetric Positive Definite (SPD) matrices such\nas covariance matrices, which arise in many real-world applications. Naively\nusing standard Mahalanobis metric learning methods under the Euclidean geometry\nfor SPD matrices is not appropriate, because the difference of SPD matrices can\nbe a non-SPD matrix and thus the obtained solution can be uninterpretable. To\ncope with this problem, we propose to use a properly parameterized LogEuclidean\ndistance and optimize the metric with respect to kernel-target alignment, which\nis a supervised criterion for kernel learning. Then the resulting non-trivial\noptimization problem is solved by utilizing the Riemannian geometry. Finally,\nwe experimentally demonstrate the usefulness of our LogEuclidean metric\nlearning algorithm on real-world classification tasks for EEG signals and\ntexture patches.",
    "published": "2015-02-12T01:38:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Adding vs. Averaging in Distributed Primal-Dual Optimization",
    "authors": [
      "Chenxin Ma",
      "Virginia Smith",
      "Martin Jaggi",
      "Michael I. Jordan",
      "Peter Richtárik",
      "Martin Takáč"
    ],
    "summary": "Distributed optimization methods for large-scale machine learning suffer from\na communication bottleneck. It is difficult to reduce this bottleneck while\nstill efficiently and accurately aggregating partial work from different\nmachines. In this paper, we present a novel generalization of the recent\ncommunication-efficient primal-dual framework (CoCoA) for distributed\noptimization. Our framework, CoCoA+, allows for additive combination of local\nupdates to the global parameters at each iteration, whereas previous schemes\nwith convergence guarantees only allow conservative averaging. We give stronger\n(primal-dual) convergence rate guarantees for both CoCoA as well as our new\nvariants, and generalize the theory for both methods to cover non-smooth convex\nloss functions. We provide an extensive experimental comparison that shows the\nmarkedly improved performance of CoCoA+ on several real-world distributed\ndatasets, especially when scaling up the number of machines.",
    "published": "2015-02-12T01:51:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scalable Stochastic Alternating Direction Method of Multipliers",
    "authors": [
      "Shen-Yi Zhao",
      "Wu-Jun Li",
      "Zhi-Hua Zhou"
    ],
    "summary": "Stochastic alternating direction method of multipliers (ADMM), which visits\nonly one sample or a mini-batch of samples each time, has recently been proved\nto achieve better performance than batch ADMM. However, most stochastic methods\ncan only achieve a convergence rate $O(1/\\sqrt T)$ on general convex\nproblems,where T is the number of iterations. Hence, these methods are not\nscalable with respect to convergence rate (computation cost). There exists only\none stochastic method, called SA-ADMM, which can achieve convergence rate\n$O(1/T)$ on general convex problems. However, an extra memory is needed for\nSA-ADMM to store the historic gradients on all samples, and thus it is not\nscalable with respect to storage cost. In this paper, we propose a novel\nmethod, called scalable stochastic ADMM(SCAS-ADMM), for large-scale\noptimization and learning problems. Without the need to store the historic\ngradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the best\nstochastic method SA-ADMM and batch ADMM on general convex problems.\nExperiments on graph-guided fused lasso show that SCAS-ADMM can achieve\nstate-of-the-art performance in real applications",
    "published": "2015-02-12T04:01:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Predictive System for detection of Bankruptcy using Machine Learning\n  techniques",
    "authors": [
      "Kalyan Nagaraj",
      "Amulyashree Sridhar"
    ],
    "summary": "Bankruptcy is a legal procedure that claims a person or organization as a\ndebtor. It is essential to ascertain the risk of bankruptcy at initial stages\nto prevent financial losses. In this perspective, different soft computing\ntechniques can be employed to ascertain bankruptcy. This study proposes a\nbankruptcy prediction system to categorize the companies based on extent of\nrisk. The prediction system acts as a decision support tool for detection of\nbankruptcy\n  Keywords: Bankruptcy, soft computing, decision support tool",
    "published": "2015-02-12T11:07:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Non-Adaptive Learning a Hidden Hipergraph",
    "authors": [
      "Hasan Abasi",
      "Nader H. Bshouty",
      "Hanna Mazzawi"
    ],
    "summary": "We give a new deterministic algorithm that non-adaptively learns a hidden\nhypergraph from edge-detecting queries. All previous non-adaptive algorithms\neither run in exponential time or have non-optimal query complexity. We give\nthe first polynomial time non-adaptive learning algorithm for learning\nhypergraph that asks almost optimal number of queries.",
    "published": "2015-02-13T21:32:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Biologically Plausible Deep Learning",
    "authors": [
      "Yoshua Bengio",
      "Dong-Hyun Lee",
      "Jorg Bornschein",
      "Thomas Mesnard",
      "Zhouhan Lin"
    ],
    "summary": "Neuroscientists have long criticised deep learning algorithms as incompatible\nwith current knowledge of neurobiology. We explore more biologically plausible\nversions of deep representation learning, focusing here mostly on unsupervised\nlearning but developing a learning mechanism that could account for supervised,\nunsupervised and reinforcement learning. The starting point is that the basic\nlearning rule believed to govern synaptic weight updates\n(Spike-Timing-Dependent Plasticity) arises out of a simple update rule that\nmakes a lot of sense from a machine learning point of view and can be\ninterpreted as gradient descent on some objective function so long as the\nneuronal dynamics push firing rates towards better values of the objective\nfunction (be it supervised, unsupervised, or reward-driven). The second main\nidea is that this corresponds to a form of the variational EM algorithm, i.e.,\nwith approximate rather than exact posteriors, implemented by neural dynamics.\nAnother contribution of this paper is that the gradients required for updating\nthe hidden states in the above variational interpretation can be estimated\nusing an approximation that only requires propagating activations forward and\nbackward, with pairs of layers learning to form a denoising auto-encoder.\nFinally, we extend the theory about the probabilistic interpretation of\nauto-encoders to justify improved sampling schemes based on the generative\ninterpretation of denoising auto-encoders, and we validate all these ideas on\ngenerative learning tasks.",
    "published": "2015-02-14T01:11:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Application of Deep Neural Network in Estimation of the Weld Bead\n  Parameters",
    "authors": [
      "Soheil Keshmiri",
      "Xin Zheng",
      "Chee Meng Chew",
      "Chee Khiang Pang"
    ],
    "summary": "We present a deep learning approach to estimation of the bead parameters in\nwelding tasks. Our model is based on a four-hidden-layer neural network\narchitecture. More specifically, the first three hidden layers of this\narchitecture utilize Sigmoid function to produce their respective intermediate\noutputs. On the other hand, the last hidden layer uses a linear transformation\nto generate the final output of this architecture. This transforms our deep\nnetwork architecture from a classifier to a non-linear regression model. We\ncompare the performance of our deep network with a selected number of results\nin the literature to show a considerable improvement in reducing the errors in\nestimation of these values. Furthermore, we show its scalability on estimating\nthe weld bead parameters with same level of accuracy on combination of datasets\nthat pertain to different welding techniques. This is a nontrivial result that\nis counter-intuitive to the general belief in this field of research.",
    "published": "2015-02-14T10:58:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions",
    "authors": [
      "Avrim Blum",
      "Moritz Hardt"
    ],
    "summary": "The organizer of a machine learning competition faces the problem of\nmaintaining an accurate leaderboard that faithfully represents the quality of\nthe best submission of each competing team. What makes this estimation problem\nparticularly challenging is its sequential and adaptive nature. As participants\nare allowed to repeatedly evaluate their submissions on the leaderboard, they\nmay begin to overfit to the holdout data that supports the leaderboard. Few\ntheoretical results give actionable advice on how to design a reliable\nleaderboard. Existing approaches therefore often resort to poorly understood\nheuristics such as limiting the bit precision of answers and the rate of\nre-submission.\n  In this work, we introduce a notion of \"leaderboard accuracy\" tailored to the\nformat of a competition. We introduce a natural algorithm called \"the Ladder\"\nand demonstrate that it simultaneously supports strong theoretical guarantees\nin a fully adaptive model of estimation, withstands practical adversarial\nattacks, and achieves high utility on real submission files from an actual\ncompetition hosted by Kaggle.\n  Notably, we are able to sidestep a powerful recent hardness result for\nadaptive risk estimation that rules out algorithms such as ours under a\nseemingly very similar notion of accuracy. On a practical note, we provide a\ncompletely parameter-free variant of our algorithm that can be deployed in a\nreal competition with no tuning required whatsoever.",
    "published": "2015-02-16T15:53:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Transform: Error Correction via Probabilistic Re-Synthesis",
    "authors": [
      "Andrew J. R. Simpson"
    ],
    "summary": "Errors in data are usually unwelcome and so some means to correct them is\nuseful. However, it is difficult to define, detect or correct errors in an\nunsupervised way. Here, we train a deep neural network to re-synthesize its\ninputs at its output layer for a given class of data. We then exploit the fact\nthat this abstract transformation, which we call a deep transform (DT),\ninherently rejects information (errors) existing outside of the abstract\nfeature space. Using the DT to perform probabilistic re-synthesis, we\ndemonstrate the recovery of data that has been subject to extreme degradation.",
    "published": "2015-02-16T16:41:26Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Generalized Gradient Learning on Time Series under Elastic\n  Transformations",
    "authors": [
      "Brijnesh Jain"
    ],
    "summary": "The majority of machine learning algorithms assumes that objects are\nrepresented as vectors. But often the objects we want to learn on are more\nnaturally represented by other data structures such as sequences and time\nseries. For these representations many standard learning algorithms are\nunavailable. We generalize gradient-based learning algorithms to time series\nunder dynamic time warping. To this end, we introduce elastic functions, which\nextend functions on time series to matrix spaces. Necessary conditions are\npresented under which generalized gradient learning on time series is\nconsistent. We indicate how results carry over to arbitrary elastic distance\nfunctions and to sequences consisting of symbolic elements. Specifically, four\nlinear classifiers are extended to time series under dynamic time warping and\napplied to benchmark datasets. Results indicate that generalized gradient\nlearning via elastic functions have the potential to complement the\nstate-of-the-art in statistical pattern recognition on time series.",
    "published": "2015-02-17T10:08:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Real time clustering of time series using triangular potentials",
    "authors": [
      "Aldo Pacchiano",
      "Oliver Williams"
    ],
    "summary": "Motivated by the problem of computing investment portfolio weightings we\ninvestigate various methods of clustering as alternatives to traditional\nmean-variance approaches. Such methods can have significant benefits from a\npractical point of view since they remove the need to invert a sample\ncovariance matrix, which can suffer from estimation error and will almost\ncertainly be non-stationary. The general idea is to find groups of assets which\nshare similar return characteristics over time and treat each group as a single\ncomposite asset. We then apply inverse volatility weightings to these new\ncomposite assets. In the course of our investigation we devise a method of\nclustering based on triangular potentials and we present associated theoretical\nresults as well as various examples based on synthetic data.",
    "published": "2015-02-18T00:27:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "CSAL: Self-adaptive Labeling based Clustering Integrating Supervised\n  Learning on Unlabeled Data",
    "authors": [
      "Fangfang Li",
      "Guandong Xu",
      "Longbing Cao"
    ],
    "summary": "Supervised classification approaches can predict labels for unknown data\nbecause of the supervised training process. The success of classification is\nheavily dependent on the labeled training data. Differently, clustering is\neffective in revealing the aggregation property of unlabeled data, but the\nperformance of most clustering methods is limited by the absence of labeled\ndata. In real applications, however, it is time-consuming and sometimes\nimpossible to obtain labeled data. The combination of clustering and\nclassification is a promising and active approach which can largely improve the\nperformance. In this paper, we propose an innovative and effective clustering\nframework based on self-adaptive labeling (CSAL) which integrates clustering\nand classification on unlabeled data. Clustering is first employed to partition\ndata and a certain proportion of clustered data are selected by our proposed\nlabeling approach for training classifiers. In order to refine the trained\nclassifiers, an iterative process of Expectation-Maximization algorithm is\ndevised into the proposed clustering framework CSAL. Experiments are conducted\non publicly data sets to test different combinations of clustering algorithms\nand classification models as well as various training data labeling methods.\nThe experimental results show that our approach along with the self-adaptive\nmethod outperforms other methods.",
    "published": "2015-02-18T04:04:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Supervised cross-modal factor analysis for multiple modal data\n  classification",
    "authors": [
      "Jingbin Wang",
      "Yihua Zhou",
      "Kanghong Duan",
      "Jim Jing-Yan Wang",
      "Halima Bensmail"
    ],
    "summary": "In this paper we study the problem of learning from multiple modal data for\npurpose of document classification. In this problem, each document is composed\ntwo different modals of data, i.e., an image and a text. Cross-modal factor\nanalysis (CFA) has been proposed to project the two different modals of data to\na shared data space, so that the classification of a image or a text can be\nperformed directly in this space. A disadvantage of CFA is that it has ignored\nthe supervision information. In this paper, we improve CFA by incorporating the\nsupervision information to represent and classify both image and text modals of\ndocuments. We project both image and text data to a shared data space by factor\nanalysis, and then train a class label predictor in the shared space to use the\nclass label information. The factor analysis parameter and the predictor\nparameter are learned jointly by solving one single objective function. With\nthis objective function, we minimize the distance between the projections of\nimage and text of the same document, and the classification error of the\nprojection measured by hinge loss function. The objective function is optimized\nby an alternate optimization strategy in an iterative algorithm. Experiments in\ntwo different multiple modal document data sets show the advantage of the\nproposed algorithm over other CFA methods.",
    "published": "2015-02-18T06:55:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Trust Region Policy Optimization",
    "authors": [
      "John Schulman",
      "Sergey Levine",
      "Philipp Moritz",
      "Michael I. Jordan",
      "Pieter Abbeel"
    ],
    "summary": "We describe an iterative procedure for optimizing policies, with guaranteed\nmonotonic improvement. By making several approximations to the\ntheoretically-justified procedure, we develop a practical algorithm, called\nTrust Region Policy Optimization (TRPO). This algorithm is similar to natural\npolicy gradient methods and is effective for optimizing large nonlinear\npolicies such as neural networks. Our experiments demonstrate its robust\nperformance on a wide variety of tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari games using images of the screen\nas input. Despite its approximations that deviate from the theory, TRPO tends\nto give monotonic improvement, with little tuning of hyperparameters.",
    "published": "2015-02-19T06:44:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Achieving All with No Parameters: Adaptive NormalHedge",
    "authors": [
      "Haipeng Luo",
      "Robert E. Schapire"
    ],
    "summary": "We study the classic online learning problem of predicting with expert\nadvice, and propose a truly parameter-free and adaptive algorithm that achieves\nseveral objectives simultaneously without using any prior information. The main\ncomponent of this work is an improved version of the NormalHedge.DT algorithm\n(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this new\nalgorithm ensures small regret when the competitor has small loss and almost\nconstant regret when the losses are stochastic. On the other hand, the\nalgorithm is able to compete with any convex combination of the experts\nsimultaneously, with a regret in terms of the relative entropy of the prior and\nthe competitor. This resolves an open problem proposed by Chaudhuri et al.\n(2009) and Chernov and Vovk (2010). Moreover, we extend the results to the\nsleeping expert setting and provide two applications to illustrate the power of\nAdaNormalHedge: 1) competing with time-varying unknown competitors and 2)\npredicting almost as well as the best pruning tree. Our results on these\napplications significantly improve previous work from different aspects, and a\nspecial case of the first application resolves another open problem proposed by\nWarmuth and Koolen (2014) on whether one can simultaneously achieve optimal\nshifting regret for both adversarial and stochastic losses.",
    "published": "2015-02-20T16:58:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "SDCA without Duality",
    "authors": [
      "Shai Shalev-Shwartz"
    ],
    "summary": "Stochastic Dual Coordinate Ascent is a popular method for solving regularized\nloss minimization for the case of convex losses. In this paper we show how a\nvariant of SDCA can be applied for non-convex losses. We prove linear\nconvergence rate even if individual loss functions are non-convex as long as\nthe expected loss is convex.",
    "published": "2015-02-22T04:42:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Teaching and compressing for low VC-dimension",
    "authors": [
      "Shay Moran",
      "Amir Shpilka",
      "Avi Wigderson",
      "Amir Yehudayoff"
    ],
    "summary": "In this work we study the quantitative relation between VC-dimension and two\nother basic parameters related to learning and teaching. Namely, the quality of\nsample compression schemes and of teaching sets for classes of low\nVC-dimension. Let $C$ be a binary concept class of size $m$ and VC-dimension\n$d$. Prior to this work, the best known upper bounds for both parameters were\n$\\log(m)$, while the best lower bounds are linear in $d$. We present\nsignificantly better upper bounds on both as follows. Set $k = O(d 2^d \\log\n\\log |C|)$.\n  We show that there always exists a concept $c$ in $C$ with a teaching set\n(i.e. a list of $c$-labeled examples uniquely identifying $c$ in $C$) of size\n$k$. This problem was studied by Kuhlmann (1999). Our construction implies that\nthe recursive teaching (RT) dimension of $C$ is at most $k$ as well. The\nRT-dimension was suggested by Zilles et al. and Doliwa et al. (2010). The same\nnotion (under the name partial-ID width) was independently studied by Wigderson\nand Yehudayoff (2013). An upper bound on this parameter that depends only on\n$d$ is known just for the very simple case $d=1$, and is open even for $d=2$.\nWe also make small progress towards this seemingly modest goal.\n  We further construct sample compression schemes of size $k$ for $C$, with\nadditional information of $k \\log(k)$ bits. Roughly speaking, given any list of\n$C$-labelled examples of arbitrary length, we can retain only $k$ labeled\nexamples in a way that allows to recover the labels of all others examples in\nthe list, using additional $k\\log (k)$ information bits. This problem was first\nsuggested by Littlestone and Warmuth (1986).",
    "published": "2015-02-22T06:21:28Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Contextual Dueling Bandits",
    "authors": [
      "Miroslav Dudík",
      "Katja Hofmann",
      "Robert E. Schapire",
      "Aleksandrs Slivkins",
      "Masrour Zoghi"
    ],
    "summary": "We consider the problem of learning to choose actions using contextual\ninformation when provided with limited feedback in the form of relative\npairwise comparisons. We study this problem in the dueling-bandits framework of\nYue et al. (2009), which we extend to incorporate context. Roughly, the\nlearner's goal is to find the best policy, or way of behaving, in some space of\npolicies, although \"best\" is not always so clearly defined. Here, we propose a\nnew and natural solution concept, rooted in game theory, called a von Neumann\nwinner, a randomized policy that beats or ties every other policy. We show that\nthis notion overcomes important limitations of existing solutions, particularly\nthe Condorcet winner which has typically been used in the past, but which\nrequires strong and often unrealistic assumptions. We then present three\nefficient algorithms for online learning in our setting, and for approximating\na von Neumann winner from batch-like data. The first of these algorithms\nachieves particularly low regret, even when data is adversarial, although its\ntime and space requirements are linear in the size of the policy space. The\nother two algorithms require time and space only logarithmic in the size of the\npolicy space when provided access to an oracle for solving classification\nproblems on the space.",
    "published": "2015-02-23T09:47:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reified Context Models",
    "authors": [
      "Jacob Steinhardt",
      "Percy Liang"
    ],
    "summary": "A classic tension exists between exact inference in a simple model and\napproximate inference in a complex model. The latter offers expressivity and\nthus accuracy, but the former provides coverage of the space, an important\nproperty for confidence estimation and learning with indirect supervision. In\nthis work, we introduce a new approach, reified context models, to reconcile\nthis tension. Specifically, we let the amount of context (the arity of the\nfactors in a graphical model) be chosen \"at run-time\" by reifying it---that is,\nletting this choice itself be a random variable inside the model. Empirically,\nwe show that our approach obtains expressivity and coverage on three natural\nlanguage tasks.",
    "published": "2015-02-24T01:26:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Fast-Mixing Models for Structured Prediction",
    "authors": [
      "Jacob Steinhardt",
      "Percy Liang"
    ],
    "summary": "Markov Chain Monte Carlo (MCMC) algorithms are often used for approximate\ninference inside learning, but their slow mixing can be difficult to diagnose\nand the approximations can seriously degrade learning. To alleviate these\nissues, we define a new model family using strong Doeblin Markov chains, whose\nmixing times can be precisely controlled by a parameter. We also develop an\nalgorithm to learn such models, which involves maximizing the data likelihood\nunder the induced stationary distribution of these chains. We show empirical\nimprovements on two challenging inference tasks.",
    "published": "2015-02-24T01:42:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Strongly Adaptive Online Learning",
    "authors": [
      "Amit Daniely",
      "Alon Gonen",
      "Shai Shalev-Shwartz"
    ],
    "summary": "Strongly adaptive algorithms are algorithms whose performance on every time\ninterval is close to optimal. We present a reduction that can transform\nstandard low-regret algorithms to strongly adaptive. As a consequence, we\nderive simple, yet efficient, strongly adaptive algorithms for a handful of\nproblems.",
    "published": "2015-02-25T07:24:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The VC-Dimension of Similarity Hypotheses Spaces",
    "authors": [
      "Mark Herbster",
      "Paul Rubenstein",
      "James Townsend"
    ],
    "summary": "Given a set $X$ and a function $h:X\\longrightarrow\\{0,1\\}$ which labels each\nelement of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ to\nmeasure the similarity of pairs of points in $X$ according to $h$.\nSpecifically, for $h\\in \\{0,1\\}^X$ we define $h^{(s)}\\in \\{0,1\\}^{X\\times X}$\nby $h^{(s)}(w,x):= \\mathbb{1}[h(w) = h(x)]$. This idea can be extended to a set\nof functions, or hypothesis space $\\mathcal{H} \\subseteq \\{0,1\\}^X$ by defining\na similarity hypothesis space $\\mathcal{H}^{(s)}:=\\{h^{(s)}:h\\in\\mathcal{H}\\}$.\nWe show that ${{vc-dimension}}(\\mathcal{H}^{(s)}) \\in\n\\Theta({{vc-dimension}}(\\mathcal{H}))$.",
    "published": "2015-02-25T12:14:04Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Learning with Feedback Graphs: Beyond Bandits",
    "authors": [
      "Noga Alon",
      "Nicolò Cesa-Bianchi",
      "Ofer Dekel",
      "Tomer Koren"
    ],
    "summary": "We study a general class of online learning problems where the feedback is\nspecified by a graph. This class includes online prediction with expert advice\nand the multi-armed bandit problem, but also several learning problems where\nthe online player does not necessarily observe his own loss. We analyze how the\nstructure of the feedback graph controls the inherent difficulty of the induced\n$T$-round learning problem. Specifically, we show that any feedback graph\nbelongs to one of three classes: strongly observable graphs, weakly observable\ngraphs, and unobservable graphs. We prove that the first class induces learning\nproblems with $\\widetilde\\Theta(\\alpha^{1/2} T^{1/2})$ minimax regret, where\n$\\alpha$ is the independence number of the underlying graph; the second class\ninduces problems with $\\widetilde\\Theta(\\delta^{1/3}T^{2/3})$ minimax regret,\nwhere $\\delta$ is the domination number of a certain portion of the graph; and\nthe third class induces problems with linear minimax regret. Our results\nsubsume much of the previous work on learning with feedback graphs and reveal\nnew connections to partial monitoring games. We also show how the regret is\naffected if the graphs are allowed to vary with time.",
    "published": "2015-02-26T16:18:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unsupervised Feature Selection with Adaptive Structure Learning",
    "authors": [
      "Liang Du",
      "Yi-Dong Shen"
    ],
    "summary": "The problem of feature selection has raised considerable interests in the\npast decade. Traditional unsupervised methods select the features which can\nfaithfully preserve the intrinsic structures of data, where the intrinsic\nstructures are estimated using all the input features of data. However, the\nestimated intrinsic structures are unreliable/inaccurate when the redundant and\nnoisy features are not removed. Therefore, we face a dilemma here: one need the\ntrue structures of data to identify the informative features, and one need the\ninformative features to accurately estimate the true structures of data. To\naddress this, we propose a unified learning framework which performs structure\nlearning and feature selection simultaneously. The structures are adaptively\nlearned from the results of feature selection, and the informative features are\nreselected to preserve the refined structures of data. By leveraging the\ninteractions between these two essential tasks, we are able to capture accurate\nstructures and select more informative features. Experimental results on many\nbenchmark data sets demonstrate that the proposed method outperforms many state\nof the art unsupervised feature selection methods.",
    "published": "2015-04-03T03:21:15Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Child is Father of the Man: Foresee the Success at the Early Stage",
    "authors": [
      "Liangyue Li",
      "Hanghang Tong"
    ],
    "summary": "Understanding the dynamic mechanisms that drive the high-impact scientific\nwork (e.g., research papers, patents) is a long-debated research topic and has\nmany important implications, ranging from personal career development and\nrecruitment search, to the jurisdiction of research resources. Recent advances\nin characterizing and modeling scientific success have made it possible to\nforecast the long-term impact of scientific work, where data mining techniques,\nsupervised learning in particular, play an essential role. Despite much\nprogress, several key algorithmic challenges in relation to predicting\nlong-term scientific impact have largely remained open. In this paper, we\npropose a joint predictive model to forecast the long-term scientific impact at\nthe early stage, which simultaneously addresses a number of these open\nchallenges, including the scholarly feature design, the non-linearity, the\ndomain-heterogeneity and dynamics. In particular, we formulate it as a\nregularized optimization problem and propose effective and scalable algorithms\nto solve it. We perform extensive empirical evaluations on large, real\nscholarly data sets to validate the effectiveness and the efficiency of our\nmethod.",
    "published": "2015-04-03T22:04:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by\n  Noise and Interference",
    "authors": [
      "Silvija Kokalj-Filipovic",
      "Larry Greenstein"
    ],
    "summary": "We propose a method for estimating channel parameters from RSSI measurements\nand the lost packet count, which can work in the presence of losses due to both\ninterference and signal attenuation below the noise floor. This is especially\nimportant in the wireless networks, such as vehicular, where propagation model\nchanges with the density of nodes. The method is based on Stochastic\nExpectation Maximization, where the received data is modeled as a mixture of\ndistributions (no/low interference and strong interference), incomplete\n(censored) due to packet losses. The PDFs in the mixture are Gamma, according\nto the commonly accepted model for wireless signal and interference power. This\napproach leverages the loss count as additional information, hence\noutperforming maximum likelihood estimation, which does not use this\ninformation (ML-), for a small number of received RSSI samples. Hence, it\nallows inexpensive on-line channel estimation from ad-hoc collected data. The\nmethod also outperforms ML- on uncensored data mixtures, as ML- assumes that\nsamples are from a single-mode PDF.",
    "published": "2015-04-05T02:20:55Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent",
    "authors": [
      "Cho-Jui Hsieh",
      "Hsiang-Fu Yu",
      "Inderjit S. Dhillon"
    ],
    "summary": "Stochastic Dual Coordinate Descent (SDCD) has become one of the most\nefficient ways to solve the family of $\\ell_2$-regularized empirical risk\nminimization problems, including linear SVM, logistic regression, and many\nothers. The vanilla implementation of DCD is quite slow; however, by\nmaintaining primal variables while updating dual variables, the time complexity\nof SDCD can be significantly reduced. Such a strategy forms the core algorithm\nin the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD\nalgorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD\nalgorithms have been proposed, however, they fail to achieve good speedup in\nthe shared memory multi-core setting. In this paper, we propose a family of\nasynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread\nrepeatedly selects a random dual variable and conducts coordinate updates using\nthe primal variables that are stored in the shared memory. We analyze the\nconvergence properties when different locking/atomic mechanisms are applied.\nFor implementation with atomic operations, we show linear convergence under\nmild conditions. For implementation without any atomic operations or locking,\nwe present the first {\\it backward error analysis} for ASDCD under the\nmulti-core environment, showing that the converged solution is the exact\nsolution for a primal problem with perturbed regularizer. Experimental results\nshow that our methods are much faster than previous parallel coordinate descent\nsolvers.",
    "published": "2015-04-06T19:25:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Autonomous CRM Control via CLV Approximation with Deep Reinforcement\n  Learning in Discrete and Continuous Action Space",
    "authors": [
      "Yegor Tkachenko"
    ],
    "summary": "The paper outlines a framework for autonomous control of a CRM (customer\nrelationship management) system. First, it explores how a modified version of\nthe widely accepted Recency-Frequency-Monetary Value system of metrics can be\nused to define the state space of clients or donors. Second, it describes a\nprocedure to determine the optimal direct marketing action in discrete and\ncontinuous action space for the given individual, based on his position in the\nstate space. The procedure involves the use of model-free Q-learning to train a\ndeep neural network that relates a client's position in the state space to\nrewards associated with possible marketing actions. The estimated value\nfunction over the client state space can be interpreted as customer lifetime\nvalue, and thus allows for a quick plug-in estimation of CLV for a given\nclient. Experimental results are presented, based on KDD Cup 1998 mailing\ndataset of donation solicitations.",
    "published": "2015-04-08T06:22:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Data Mining for Prediction of Human Performance Capability in the\n  Software-Industry",
    "authors": [
      "Gaurav Singh Thakur",
      "Anubhav Gupta",
      "Sangita Gupta"
    ],
    "summary": "The recruitment of new personnel is one of the most essential business\nprocesses which affect the quality of human capital within any company. It is\nhighly essential for the companies to ensure the recruitment of right talent to\nmaintain a competitive edge over the others in the market. However IT companies\noften face a problem while recruiting new people for their ongoing projects due\nto lack of a proper framework that defines a criteria for the selection\nprocess. In this paper we aim to develop a framework that would allow any\nproject manager to take the right decision for selecting new talent by\ncorrelating performance parameters with the other domain-specific attributes of\nthe candidates. Also, another important motivation behind this project is to\ncheck the validity of the selection procedure often followed by various big\ncompanies in both public and private sectors which focus only on academic\nscores, GPA/grades of students from colleges and other academic backgrounds. We\ntest if such a decision will produce optimal results in the industry or is\nthere a need for change that offers a more holistic approach to recruitment of\nnew talent in the software companies. The scope of this work extends beyond the\nIT domain and a similar procedure can be adopted to develop a recruitment\nframework in other fields as well. Data-mining techniques provide useful\ninformation from the historical projects depending on which the hiring-manager\ncan make decisions for recruiting high-quality workforce. This study aims to\nbridge this hiatus by developing a data-mining framework based on an\nensemble-learning technique to refocus on the criteria for personnel selection.\nThe results from this research clearly demonstrated that there is a need to\nrefocus on the selection-criteria for quality objectives.",
    "published": "2015-04-08T12:26:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Maximum Entropy Linear Manifold for Learning Discriminative\n  Low-dimensional Representation",
    "authors": [
      "Wojciech Marian Czarnecki",
      "Rafał Józefowicz",
      "Jacek Tabor"
    ],
    "summary": "Representation learning is currently a very hot topic in modern machine\nlearning, mostly due to the great success of the deep learning methods. In\nparticular low-dimensional representation which discriminates classes can not\nonly enhance the classification procedure, but also make it faster, while\ncontrary to the high-dimensional embeddings can be efficiently used for visual\nbased exploratory data analysis.\n  In this paper we propose Maximum Entropy Linear Manifold (MELM), a\nmultidimensional generalization of Multithreshold Entropy Linear Classifier\nmodel which is able to find a low-dimensional linear data projection maximizing\ndiscriminativeness of projected classes. As a result we obtain a linear\nembedding which can be used for classification, class aware dimensionality\nreduction and data visualization. MELM provides highly discriminative 2D\nprojections of the data which can be used as a method for constructing robust\nclassifiers.\n  We provide both empirical evaluation as well as some interesting theoretical\nproperties of our objective function such us scale and affine transformation\ninvariance, connections with PCA and bounding of the expected balanced accuracy\nerror.",
    "published": "2015-04-10T09:56:51Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Deep Embedding Model for Co-occurrence Learning",
    "authors": [
      "Yelong Shen",
      "Ruoming Jin",
      "Jianshu Chen",
      "Xiaodong He",
      "Jianfeng Gao",
      "Li Deng"
    ],
    "summary": "Co-occurrence Data is a common and important information source in many\nareas, such as the word co-occurrence in the sentences, friends co-occurrence\nin social networks and products co-occurrence in commercial transaction data,\netc, which contains rich correlation and clustering information about the\nitems. In this paper, we study co-occurrence data using a general energy-based\nprobabilistic model, and we analyze three different categories of energy-based\nmodel, namely, the $L_1$, $L_2$ and $L_k$ models, which are able to capture\ndifferent levels of dependency in the co-occurrence data. We also discuss how\nseveral typical existing models are related to these three types of energy\nmodels, including the Fully Visible Boltzmann Machine (FVBM) ($L_2$), Matrix\nFactorization ($L_2$), Log-BiLinear (LBL) models ($L_2$), and the Restricted\nBoltzmann Machine (RBM) model ($L_k$). Then, we propose a Deep Embedding Model\n(DEM) (an $L_k$ model) from the energy model in a \\emph{principled} manner.\nFurthermore, motivated by the observation that the partition function in the\nenergy model is intractable and the fact that the major objective of modeling\nthe co-occurrence data is to predict using the conditional probability, we\napply the \\emph{maximum pseudo-likelihood} method to learn DEM. In consequence,\nthe developed model and its learning method naturally avoid the above\ndifficulties and can be easily used to compute the conditional probability in\nprediction. Interestingly, our method is equivalent to learning a special\nstructured deep neural network using back-propagation and a special sampling\nstrategy, which makes it scalable on large-scale datasets. Finally, in the\nexperiments, we show that the DEM can achieve comparable or better results than\nstate-of-the-art methods on datasets across several application domains.",
    "published": "2015-04-11T02:56:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Classification with Extreme Learning Machine and Ensemble Algorithms\n  Over Randomly Partitioned Data",
    "authors": [
      "Ferhat Özgür Çatak"
    ],
    "summary": "In this age of Big Data, machine learning based data mining methods are\nextensively used to inspect large scale data sets. Deriving applicable\npredictive modeling from these type of data sets is a challenging obstacle\nbecause of their high complexity. Opportunity with high data availability\nlevels, automated classification of data sets has become a critical and\ncomplicated function. In this paper, the power of applying MapReduce based\nDistributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build\nreliable predictive bag of classification models. Thus, (i) dataset ensembles\nare build; (ii) ELM algorithm is used to build weak classification models; and\n(iii) build a strong classification model from a set of weak classification\nmodels. This training model is applied to the publicly available knowledge\ndiscovery and data mining datasets.",
    "published": "2015-04-12T14:03:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Convex Learning of Multiple Tasks and their Structure",
    "authors": [
      "Carlo Ciliberto",
      "Youssef Mroueh",
      "Tomaso Poggio",
      "Lorenzo Rosasco"
    ],
    "summary": "Reducing the amount of human supervision is a key problem in machine learning\nand a natural approach is that of exploiting the relations (structure) among\ndifferent tasks. This is the idea at the core of multi-task learning. In this\ncontext a fundamental question is how to incorporate the tasks structure in the\nlearning problem.We tackle this question by studying a general computational\nframework that allows to encode a-priori knowledge of the tasks structure in\nthe form of a convex penalty; in this setting a variety of previously proposed\nmethods can be recovered as special cases, including linear and non-linear\napproaches. Within this framework, we show that tasks and their structure can\nbe efficiently learned considering a convex optimization problem that can be\napproached by means of block coordinate methods such as alternating\nminimization and for which we prove convergence to the global minimum.",
    "published": "2015-04-13T09:13:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients",
    "authors": [
      "Bo Xie",
      "Yingyu Liang",
      "Le Song"
    ],
    "summary": "Nonlinear component analysis such as kernel Principle Component Analysis\n(KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in\nmachine learning, statistics and data analysis, but they can not scale up to\nbig datasets. Recent attempts have employed random feature approximations to\nconvert the problem to the primal form for linear computational complexity.\nHowever, to obtain high quality solutions, the number of random features should\nbe the same order of magnitude as the number of data points, making such\napproach not directly applicable to the regime with millions of data points.\n  We propose a simple, computationally efficient, and memory friendly algorithm\nbased on the \"doubly stochastic gradients\" to scale up a range of kernel\nnonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the\n\\emph{non-convex} nature of these problems, our method enjoys theoretical\nguarantees that it converges at the rate $\\tilde{O}(1/t)$ to the global\noptimum, even for the top $k$ eigen subspace. Unlike many alternatives, our\nalgorithm does not require explicit orthogonalization, which is infeasible on\nbig datasets. We demonstrate the effectiveness and scalability of our algorithm\non large scale synthetic and real world datasets.",
    "published": "2015-04-14T18:34:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Linear Maximum Margin Classifier for Learning from Uncertain Data",
    "authors": [
      "Christos Tzelepis",
      "Vasileios Mezaris",
      "Ioannis Patras"
    ],
    "summary": "In this paper, we propose a maximum margin classifier that deals with\nuncertainty in data input. More specifically, we reformulate the SVM framework\nsuch that each training example can be modeled by a multi-dimensional Gaussian\ndistribution described by its mean vector and its covariance matrix -- the\nlatter modeling the uncertainty. We address the classification problem and\ndefine a cost function that is the expected value of the classical SVM cost\nwhen data samples are drawn from the multi-dimensional Gaussian distributions\nthat form the set of the training examples. Our formulation approximates the\nclassical SVM formulation when the training examples are isotropic Gaussians\nwith variance tending to zero. We arrive at a convex optimization problem,\nwhich we solve efficiently in the primal form using a stochastic gradient\ndescent approach. The resulting classifier, which we name SVM with Gaussian\nSample Uncertainty (SVM-GSU), is tested on synthetic data and five publicly\navailable and popular datasets; namely, the MNIST, WDBC, DEAP, TV News Channel\nCommercial Detection, and TRECVID MED datasets. Experimental results verify the\neffectiveness of the proposed method.",
    "published": "2015-04-15T12:47:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Nataf-Beta Random Field Classifier: An Extension of the Beta\n  Conjugate Prior to Classification Problems",
    "authors": [
      "James-A. Goulet"
    ],
    "summary": "This paper presents the Nataf-Beta Random Field Classifier, a discriminative\napproach that extends the applicability of the Beta conjugate prior to\nclassification problems. The approach's key feature is to model the probability\nof a class conditional on attribute values as a random field whose marginals\nare Beta distributed, and where the parameters of marginals are themselves\ndescribed by random fields. Although the classification accuracy of the\napproach proposed does not statistically outperform the best accuracies\nreported in the literature, it ranks among the top tier for the six benchmark\ndatasets tested. The Nataf-Beta Random Field Classifier is suited as a general\npurpose classification approach for real-continuous and real-integer attribute\nvalue problems.",
    "published": "2015-04-17T17:32:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Performance Evaluation of Machine Learning Algorithms in Post-operative\n  Life Expectancy in the Lung Cancer Patients",
    "authors": [
      "Kwetishe Joro Danjuma"
    ],
    "summary": "The nature of clinical data makes it difficult to quickly select, tune and\napply machine learning algorithms to clinical prognosis. As a result, a lot of\ntime is spent searching for the most appropriate machine learning algorithms\napplicable in clinical prognosis that contains either binary-valued or\nmulti-valued attributes. The study set out to identify and evaluate the\nperformance of machine learning classification schemes applied in clinical\nprognosis of post-operative life expectancy in the lung cancer patients.\nMultilayer Perceptron, J48, and the Naive Bayes algorithms were used to train\nand test models on Thoracic Surgery datasets obtained from the University of\nCalifornia Irvine machine learning repository. Stratified 10-fold\ncross-validation was used to evaluate baseline performance accuracy of the\nclassifiers. The comparative analysis shows that multilayer perceptron\nperformed best with classification accuracy of 82.3%, J48 came out second with\nclassification accuracy of 81.8%, and Naive Bayes came out the worst with\nclassification accuracy of 74.4%. The quality and outcome of the chosen machine\nlearning algorithms depends on the ingenuity of the clinical miner.",
    "published": "2015-04-17T22:05:34Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Instance Optimal Learning",
    "authors": [
      "Gregory Valiant",
      "Paul Valiant"
    ],
    "summary": "We consider the following basic learning task: given independent draws from\nan unknown distribution over a discrete support, output an approximation of the\ndistribution that is as accurate as possible in $\\ell_1$ distance (i.e. total\nvariation or statistical distance). Perhaps surprisingly, it is often possible\nto \"de-noise\" the empirical distribution of the samples to return an\napproximation of the true distribution that is significantly more accurate than\nthe empirical distribution, without relying on any prior assumptions on the\ndistribution. We present an instance optimal learning algorithm which optimally\nperforms this de-noising for every distribution for which such a de-noising is\npossible. More formally, given $n$ independent draws from a distribution $p$,\nour algorithm returns a labelled vector whose expected distance from $p$ is\nequal to the minimum possible expected error that could be obtained by any\nalgorithm that knows the true unlabeled vector of probabilities of distribution\n$p$ and simply needs to assign labels, up to an additive subconstant term that\nis independent of $p$ and goes to zero as $n$ gets large. One conceptual\nimplication of this result is that for large samples, Bayesian assumptions on\nthe \"shape\" or bounds on the tail probabilities of a distribution over discrete\nsupport are not helpful for the task of learning the distribution.\n  As a consequence of our techniques, we also show that given a set of $n$\nsamples from an arbitrary distribution, one can accurately estimate the\nexpected number of distinct elements that will be observed in a sample of any\nsize up to $n \\log n$. This sort of extrapolation is practically relevant,\nparticularly to domains such as genomics where it is important to understand\nhow much more might be discovered given larger sample sizes, and we are\noptimistic that our approach is practically viable.",
    "published": "2015-04-21T07:05:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Effective Discriminative Feature Selection with Non-trivial Solutions",
    "authors": [
      "Hong Tao",
      "Chenping Hou",
      "Feiping Nie",
      "Yuanyuan Jiao",
      "Dongyun Yi"
    ],
    "summary": "Feature selection and feature transformation, the two main ways to reduce\ndimensionality, are often presented separately. In this paper, a feature\nselection method is proposed by combining the popular transformation based\ndimensionality reduction method Linear Discriminant Analysis (LDA) and sparsity\nregularization. We impose row sparsity on the transformation matrix of LDA\nthrough ${\\ell}_{2,1}$-norm regularization to achieve feature selection, and\nthe resultant formulation optimizes for selecting the most discriminative\nfeatures and removing the redundant ones simultaneously. The formulation is\nextended to the ${\\ell}_{2,p}$-norm regularized case: which is more likely to\noffer better sparsity when $0<p<1$. Thus the formulation is a better\napproximation to the feature selection problem. An efficient algorithm is\ndeveloped to solve the ${\\ell}_{2,p}$-norm based optimization problem and it is\nproved that the algorithm converges when $0<p\\le 2$. Systematical experiments\nare conducted to understand the work of the proposed method. Promising\nexperimental results on various types of real-world data sets demonstrate the\neffectiveness of our algorithm.",
    "published": "2015-04-21T12:40:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Temporal-Difference Networks",
    "authors": [
      "Richard S. Sutton",
      "Brian Tanner"
    ],
    "summary": "We introduce a generalization of temporal-difference (TD) learning to\nnetworks of interrelated predictions. Rather than relating a single prediction\nto itself at a later time, as in conventional TD methods, a TD network relates\neach prediction in a set of predictions to other predictions in the set at a\nlater time. TD networks can represent and apply TD learning to a much wider\nclass of predictions than has previously been possible. Using a random-walk\nexample, we show that these networks can be used to learn to predict by a fixed\ninterval, which is not possible with conventional TD methods. Secondly, we show\nthat if the inter-predictive relationships are made conditional on action, then\nthe usual learning-efficiency advantage of TD methods over Monte Carlo\n(supervised learning) methods becomes particularly pronounced. Thirdly, we\ndemonstrate that TD networks can learn predictive state representations that\nenable exact solution of a non-Markov problem. A very broad range of\ninter-predictive temporal relationships can be expressed in these networks.\nOverall we argue that TD networks represent a substantial extension of the\nabilities of TD methods and bring us closer to the goal of representing world\nknowledge in entirely predictive, grounded terms.",
    "published": "2015-04-21T18:33:39Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Discriminative Switching Linear Dynamical Systems applied to\n  Physiological Condition Monitoring",
    "authors": [
      "Konstantinos Georgatzis",
      "Christopher K. I. Williams"
    ],
    "summary": "We present a Discriminative Switching Linear Dynamical System (DSLDS) applied\nto patient monitoring in Intensive Care Units (ICUs). Our approach is based on\nidentifying the state-of-health of a patient given their observed vital signs\nusing a discriminative classifier, and then inferring their underlying\nphysiological values conditioned on this status. The work builds on the\nFactorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which\nhas been previously used in a similar setting. The FSLDS is a generative model,\nwhereas the DSLDS is a discriminative model. We demonstrate on two real-world\ndatasets that the DSLDS is able to outperform the FSLDS in most cases of\ninterest, and that an $\\alpha$-mixture of the two models achieves higher\nperformance than either of the two models separately.",
    "published": "2015-04-24T13:23:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Convex Optimization Using Predictions",
    "authors": [
      "Niangjun Chen",
      "Anish Agarwal",
      "Adam Wierman",
      "Siddharth Barman",
      "Lachlan L. H. Andrew"
    ],
    "summary": "Making use of predictions is a crucial, but under-explored, area of online\nalgorithms. This paper studies a class of online optimization problems where we\nhave external noisy predictions available. We propose a stochastic prediction\nerror model that generalizes prior models in the learning and stochastic\ncontrol communities, incorporates correlation among prediction errors, and\ncaptures the fact that predictions improve as time passes. We prove that\nachieving sublinear regret and constant competitive ratio for online algorithms\nrequires the use of an unbounded prediction window in adversarial settings, but\nthat under more realistic stochastic prediction error models it is possible to\nuse Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear\nregret and constant competitive ratio in expectation using only a\nconstant-sized prediction window. Furthermore, we show that the performance of\nAFHC is tightly concentrated around its mean.",
    "published": "2015-04-25T04:41:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Random Forest for the Contextual Bandit Problem - extended version",
    "authors": [
      "Raphaël Féraud",
      "Robin Allesiardo",
      "Tanguy Urvoy",
      "Fabrice Clérot"
    ],
    "summary": "To address the contextual bandit problem, we propose an online random forest\nalgorithm. The analysis of the proposed algorithm is based on the sample\ncomplexity needed to find the optimal decision stump. Then, the decision stumps\nare assembled in a random collection of decision trees, Bandit Forest. We show\nthat the proposed algorithm is optimal up to logarithmic factors. The\ndependence of the sample complexity upon the number of contextual variables is\nlogarithmic. The computational cost of the proposed algorithm with respect to\nthe time horizon is linear. These analytical results allow the proposed\nalgorithm to be efficient in real applications, where the number of events to\nprocess is huge, and where we expect that some contextual variables, chosen\nfrom a large set, have potentially non- linear dependencies with the rewards.\nIn the experiments done to illustrate the theoretical analysis, Bandit Forest\nobtain promising results in comparison with state-of-the-art algorithms.",
    "published": "2015-04-27T07:27:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Accelerated kernel discriminant analysis",
    "authors": [
      "Nikolaos Gkalelis",
      "Vasileios Mezaris"
    ],
    "summary": "In this paper, using a novel matrix factorization and simultaneous reduction\nto diagonal form approach (or in short simultaneous reduction approach),\nAccelerated Kernel Discriminant Analysis (AKDA) and Accelerated Kernel Subclass\nDiscriminant Analysis (AKSDA) are proposed. Specifically, instead of performing\nthe simultaneous reduction of the between- and within-class or subclass scatter\nmatrices, the nonzero eigenpairs (NZEP) of the so-called core matrix, which is\nof relatively small dimensionality, and the Cholesky factorization of the\nkernel matrix are computed, achieving more than one order of magnitude speed up\nover kernel discriminant analysis (KDA). Moreover, consisting of a few\nelementary matrix operations and very stable numerical algorithms, AKDA and\nAKSDA offer improved classification accuracy. The experimental evaluation on\nvarious datasets confirms that the proposed approaches provide state-of-the-art\nperformance in terms of both training time and classification accuracy.",
    "published": "2015-04-27T09:41:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Surrogate regret bounds for generalized classification performance\n  metrics",
    "authors": [
      "Wojciech Kotłowski",
      "Krzysztof Dembczyński"
    ],
    "summary": "We consider optimization of generalized performance metrics for binary\nclassification by means of surrogate losses. We focus on a class of metrics,\nwhich are linear-fractional functions of the false positive and false negative\nrates (examples of which include $F_{\\beta}$-measure, Jaccard similarity\ncoefficient, AM measure, and many others). Our analysis concerns the following\ntwo-step procedure. First, a real-valued function $f$ is learned by minimizing\na surrogate loss for binary classification on the training sample. It is\nassumed that the surrogate loss is a strongly proper composite loss function\n(examples of which include logistic loss, squared-error loss, exponential loss,\netc.). Then, given $f$, a threshold $\\widehat{\\theta}$ is tuned on a separate\nvalidation sample, by direct optimization of the target performance metric. We\nshow that the regret of the resulting classifier (obtained from thresholding\n$f$ on $\\widehat{\\theta}$) measured with respect to the target metric is\nupperbounded by the regret of $f$ measured with respect to the surrogate loss.\nWe also extend our results to cover multilabel classification and provide\nregret bounds for micro- and macro-averaging measures. Our findings are further\nanalyzed in a computational study on both synthetic and real data sets.",
    "published": "2015-04-27T20:45:47Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Or's of And's for Interpretable Classification, with Application to\n  Context-Aware Recommender Systems",
    "authors": [
      "Tong Wang",
      "Cynthia Rudin",
      "Finale Doshi-Velez",
      "Yimin Liu",
      "Erica Klampfl",
      "Perry MacNeille"
    ],
    "summary": "We present a machine learning algorithm for building classifiers that are\ncomprised of a small number of disjunctions of conjunctions (or's of and's). An\nexample of a classifier of this form is as follows: If X satisfies (x1 = 'blue'\nAND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we\npredict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal\nand a conjunction of literals is called a pattern. Models of this form have the\nadvantage of being interpretable to human experts, since they produce a set of\nconditions that concisely describe a specific class. We present two\nprobabilistic models for forming a pattern set, one with a Beta-Binomial prior,\nand the other with Poisson priors. In both cases, there are prior parameters\nthat the user can set to encourage the model to have a desired size and shape,\nto conform with a domain-specific definition of interpretability. We provide\ntwo scalable MAP inference approaches: a pattern level search, which involves\nassociation rule mining, and a literal level search. We show stronger priors\nreduce computation. We apply the Bayesian Or's of And's (BOA) model to predict\nuser behavior with respect to in-vehicle context-aware personalized recommender\nsystems.",
    "published": "2015-04-28T19:53:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Evaluation of Explore-Exploit Policies in Multi-result Ranking Systems",
    "authors": [
      "Dragomir Yankov",
      "Pavel Berkhin",
      "Lihong Li"
    ],
    "summary": "We analyze the problem of using Explore-Exploit techniques to improve\nprecision in multi-result ranking systems such as web search, query\nautocompletion and news recommendation. Adopting an exploration policy directly\nonline, without understanding its impact on the production system, may have\nunwanted consequences - the system may sustain large losses, create user\ndissatisfaction, or collect exploration data which does not help improve\nranking quality. An offline framework is thus necessary to let us decide what\npolicy and how we should apply in a production environment to ensure positive\noutcome. Here, we describe such an offline framework.\n  Using the framework, we study a popular exploration policy - Thompson\nsampling. We show that there are different ways of implementing it in\nmulti-result ranking systems, each having different semantic interpretation and\nleading to different results in terms of sustained click-through-rate (CTR)\nloss and expected model improvement. In particular, we demonstrate that\nThompson sampling can act as an online learner optimizing CTR, which in some\ncases can lead to an interesting outcome: lift in CTR during exploration. The\nobservation is important for production systems as it suggests that one can get\nboth valuable exploration data to improve ranking performance on the long run,\nand at the same time increase CTR while exploration lasts.",
    "published": "2015-04-28T21:16:07Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Contextualized Music Semantics from Tags via a Siamese Network",
    "authors": [
      "Ubai Sandouk",
      "Ke Chen"
    ],
    "summary": "Music information retrieval faces a challenge in modeling contextualized\nmusical concepts formulated by a set of co-occurring tags. In this paper, we\ninvestigate the suitability of our recently proposed approach based on a\nSiamese neural network in fighting off this challenge. By means of tag features\nand probabilistic topic models, the network captures contextualized semantics\nfrom tags via unsupervised learning. This leads to a distributed semantics\nspace and a potential solution to the out of vocabulary problem which has yet\nto be sufficiently addressed. We explore the nature of the resultant\nmusic-based semantics and address computational needs. We conduct experiments\non three public music tag collections -namely, CAL500, MagTag5K and Million\nSong Dataset- and compare our approach to a number of state-of-the-art\nsemantics learning approaches. Comparative results suggest that this approach\noutperforms previous approaches in terms of semantic priming and music tag\ncompletion.",
    "published": "2015-04-29T19:05:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Note on Equivalence Between Recurrent Neural Network Time Series Models\n  and Variational Bayesian Models",
    "authors": [
      "Jascha Sohl-Dickstein",
      "Diederik P. Kingma"
    ],
    "summary": "We observe that the standard log likelihood training objective for a\nRecurrent Neural Network (RNN) model of time series data is equivalent to a\nvariational Bayesian training objective, given the proper choice of generative\nand inference models. This perspective may motivate extensions to both RNNs and\nvariational Bayesian models. We propose one such extension, where multiple\nparticles are used for the hidden state of an RNN, allowing a natural\nrepresentation of uncertainty or multimodality.",
    "published": "2015-04-29T21:08:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Thompson Sampling for Budgeted Multi-armed Bandits",
    "authors": [
      "Yingce Xia",
      "Haifang Li",
      "Tao Qin",
      "Nenghai Yu",
      "Tie-Yan Liu"
    ],
    "summary": "Thompson sampling is one of the earliest randomized algorithms for\nmulti-armed bandits (MAB). In this paper, we extend the Thompson sampling to\nBudgeted MAB, where there is random cost for pulling an arm and the total cost\nis constrained by a budget. We start with the case of Bernoulli bandits, in\nwhich the random rewards (costs) of an arm are independently sampled from a\nBernoulli distribution. To implement the Thompson sampling algorithm in this\ncase, at each round, we sample two numbers from the posterior distributions of\nthe reward and cost for each arm, obtain their ratio, select the arm with the\nmaximum ratio, and then update the posterior distributions. We prove that the\ndistribution-dependent regret bound of this algorithm is $O(\\ln B)$, where $B$\ndenotes the budget. By introducing a Bernoulli trial, we further extend this\nalgorithm to the setting that the rewards (costs) are drawn from general\ndistributions, and prove that its regret bound remains almost the same. Our\nsimulation results demonstrate the effectiveness of the proposed algorithm.",
    "published": "2015-05-01T10:35:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Theory of Optimizing Pseudolinear Performance Measures: Application to\n  F-measure",
    "authors": [
      "Shameem A Puthiya Parambath",
      "Nicolas Usunier",
      "Yves Grandvalet"
    ],
    "summary": "Non-linear performance measures are widely used for the evaluation of\nlearning algorithms. For example, $F$-measure is a commonly used performance\nmeasure for classification problems in machine learning and information\nretrieval community. We study the theoretical properties of a subset of\nnon-linear performance measures called pseudo-linear performance measures which\nincludes $F$-measure, \\emph{Jaccard Index}, among many others. We establish\nthat many notions of $F$-measures and \\emph{Jaccard Index} are pseudo-linear\nfunctions of the per-class false negatives and false positives for binary,\nmulticlass and multilabel classification. Based on this observation, we present\na general reduction of such performance measure optimization problem to\ncost-sensitive classification problem with unknown costs. We then propose an\nalgorithm with provable guarantees to obtain an approximately optimal\nclassifier for the $F$-measure by solving a series of cost-sensitive\nclassification problems. The strength of our analysis is to be valid on any\ndataset and any class of classifiers, extending the existing theoretical\nresults on pseudo-linear measures, which are asymptotic in nature. We also\nestablish the multi-objective nature of the $F$-score maximization problem by\nlinking the algorithm with the weighted-sum approach used in multi-objective\noptimization. We present numerical experiments to illustrate the relative\nimportance of cost asymmetry and thresholding when learning linear classifiers\non various $F$-measure optimization tasks.",
    "published": "2015-05-01T15:25:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Can deep learning help you find the perfect match?",
    "authors": [
      "Harm de Vries",
      "Jason Yosinski"
    ],
    "summary": "Is he/she my type or not? The answer to this question depends on the personal\npreferences of the one asking it. The individual process of obtaining a full\nanswer may generally be difficult and time consuming, but often an approximate\nanswer can be obtained simply by looking at a photo of the potential match.\nSuch approximate answers based on visual cues can be produced in a fraction of\na second, a phenomenon that has led to a series of recently successful dating\napps in which users rate others positively or negatively using primarily a\nsingle photo. In this paper we explore using convolutional networks to create a\nmodel of an individual's personal preferences based on rated photos. This\nintroduced task is difficult due to the large number of variations in profile\npictures and the noise in attractiveness labels. Toward this task we collect a\ndataset comprised of $9364$ pictures and binary labels for each. We compare\nperformance of convolutional models trained in three ways: first directly on\nthe collected dataset, second with features transferred from a network trained\nto predict gender, and third with features transferred from a network trained\non ImageNet. Our findings show that ImageNet features transfer best, producing\na model that attains $68.1\\%$ accuracy on the test set and is moderately\nsuccessful at predicting matches.",
    "published": "2015-05-02T17:20:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reinforcement Learning Neural Turing Machines - Revised",
    "authors": [
      "Wojciech Zaremba",
      "Ilya Sutskever"
    ],
    "summary": "The Neural Turing Machine (NTM) is more expressive than all previously\nconsidered models because of its external memory. It can be viewed as a broader\neffort to use abstract external Interfaces and to learn a parametric model that\ninteracts with them.\n  The capabilities of a model can be extended by providing it with proper\nInterfaces that interact with the world. These external Interfaces include\nmemory, a database, a search engine, or a piece of software such as a theorem\nverifier. Some of these Interfaces are provided by the developers of the model.\nHowever, many important existing Interfaces, such as databases and search\nengines, are discrete.\n  We examine feasibility of learning models to interact with discrete\nInterfaces. We investigate the following discrete Interfaces: a memory Tape, an\ninput Tape, and an output Tape. We use a Reinforcement Learning algorithm to\ntrain a neural network that interacts with such Interfaces to solve simple\nalgorithmic tasks. Our Interfaces are expressive enough to make our model\nTuring complete.",
    "published": "2015-05-04T04:14:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Reinforced Decision Trees",
    "authors": [
      "Aurélia Léon",
      "Ludovic Denoyer"
    ],
    "summary": "In order to speed-up classification models when facing a large number of\ncategories, one usual approach consists in organizing the categories in a\nparticular structure, this structure being then used as a way to speed-up the\nprediction computation. This is for example the case when using\nerror-correcting codes or even hierarchies of categories. But in the majority\nof approaches, this structure is chosen \\textit{by hand}, or during a\npreliminary step, and not integrated in the learning process. We propose a new\nmodel called Reinforced Decision Tree which simultaneously learns how to\norganize categories in a tree structure and how to classify any input based on\nthis structure. This approach keeps the advantages of existing techniques (low\ninference complexity) but allows one to build efficient classifiers in one\nlearning step. The learning algorithm is inspired by reinforcement learning and\npolicy-gradient techniques which allows us to integrate the two steps (building\nthe tree, and learning the classifier) in one single algorithm.",
    "published": "2015-05-05T07:58:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Comprehensive Study On The Applications Of Machine Learning For\n  Diagnosis Of Cancer",
    "authors": [
      "Mohnish Chakravarti",
      "Tanay Kothari"
    ],
    "summary": "Collectively, lung cancer, breast cancer and melanoma was diagnosed in over\n535,340 people out of which, 209,400 deaths were reported [13]. It is estimated\nthat over 600,000 people will be diagnosed with these forms of cancer in 2015.\nMost of the deaths from lung cancer, breast cancer and melanoma result due to\nlate detection. All of these cancers, if detected early, are 100% curable. In\nthis study, we develop and evaluate algorithms to diagnose Breast cancer,\nMelanoma, and Lung cancer. In the first part of the study, we employed a\nnormalised Gradient Descent and an Artificial Neural Network to diagnose breast\ncancer with an overall accuracy of 91% and 95% respectively. In the second part\nof the study, an artificial neural network coupled with image processing and\nanalysis algorithms was employed to achieve an overall accuracy of 93% A naive\nmobile based application that allowed people to take diagnostic tests on their\nphones was developed. Finally, a Support Vector Machine algorithm incorporating\nimage processing and image analysis algorithms was developed to diagnose lung\ncancer with an accuracy of 94%. All of the aforementioned systems had very low\nfalse positive and false negative rates. We are developing an online network\nthat incorporates all of these systems and allows people to collaborate\nglobally.",
    "published": "2015-05-06T12:52:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning and Optimization with Submodular Functions",
    "authors": [
      "Bharath Sankaran",
      "Marjan Ghazvininejad",
      "Xinran He",
      "David Kale",
      "Liron Cohen"
    ],
    "summary": "In many naturally occurring optimization problems one needs to ensure that\nthe definition of the optimization problem lends itself to solutions that are\ntractable to compute. In cases where exact solutions cannot be computed\ntractably, it is beneficial to have strong guarantees on the tractable\napproximate solutions. In order operate under these criterion most optimization\nproblems are cast under the umbrella of convexity or submodularity. In this\nreport we will study design and optimization over a common class of functions\ncalled submodular functions. Set functions, and specifically submodular set\nfunctions, characterize a wide variety of naturally occurring optimization\nproblems, and the property of submodularity of set functions has deep\ntheoretical consequences with wide ranging applications. Informally, the\nproperty of submodularity of set functions concerns the intuitive \"principle of\ndiminishing returns. This property states that adding an element to a smaller\nset has more value than adding it to a larger set. Common examples of\nsubmodular monotone functions are entropies, concave functions of cardinality,\nand matroid rank functions; non-monotone examples include graph cuts, network\nflows, and mutual information.\n  In this paper we will review the formal definition of submodularity; the\noptimization of submodular functions, both maximization and minimization; and\nfinally discuss some applications in relation to learning and reasoning using\nsubmodular functions.",
    "published": "2015-05-07T04:04:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Survey of Predictive Modelling under Imbalanced Distributions",
    "authors": [
      "Paula Branco",
      "Luis Torgo",
      "Rita Ribeiro"
    ],
    "summary": "Many real world data mining applications involve obtaining predictive models\nusing data sets with strongly imbalanced distributions of the target variable.\nFrequently, the least common values of this target variable are associated with\nevents that are highly relevant for end users (e.g. fraud detection, unusual\nreturns on stock markets, anticipation of catastrophes, etc.). Moreover, the\nevents may have different costs and benefits, which when associated with the\nrarity of some of them on the available training data creates serious problems\nto predictive modelling techniques. This paper presents a survey of existing\ntechniques for handling these important applications of predictive analytics.\nAlthough most of the existing work addresses classification tasks (nominal\ntarget variables), we also describe methods designed to handle similar problems\nwithin regression tasks (numeric target variables). In this survey we discuss\nthe main challenges raised by imbalanced distributions, describe the main\napproaches to these problems, propose a taxonomy of these methods and refer to\nsome related problems within predictive modelling.",
    "published": "2015-05-07T10:44:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounded-Distortion Metric Learning",
    "authors": [
      "Renjie Liao",
      "Jianping Shi",
      "Ziyang Ma",
      "Jun Zhu",
      "Jiaya Jia"
    ],
    "summary": "Metric learning aims to embed one metric space into another to benefit tasks\nlike classification and clustering. Although a greatly distorted metric space\nhas a high degree of freedom to fit training data, it is prone to overfitting\nand numerical inaccuracy. This paper presents {\\it bounded-distortion metric\nlearning} (BDML), a new metric learning framework which amounts to finding an\noptimal Mahalanobis metric space with a bounded-distortion constraint. An\nefficient solver based on the multiplicative weights update method is proposed.\nMoreover, we generalize BDML to pseudo-metric learning and devise the\nsemidefinite relaxation and a randomized algorithm to approximately solve it.\nWe further provide theoretical analysis to show that distortion is a key\ningredient for stability and generalization ability of our BDML algorithm.\nExtensive experiments on several benchmark datasets yield promising results.",
    "published": "2015-05-10T13:27:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data\n  Matrices",
    "authors": [
      "Jie Wang",
      "Jieping Ye"
    ],
    "summary": "Multi-task feature learning (MTFL) is a powerful technique in boosting the\npredictive performance by learning multiple related\nclassification/regression/clustering tasks simultaneously. However, solving the\nMTFL problem remains challenging when the feature dimension is extremely large.\nIn this paper, we propose a novel screening rule---that is based on the dual\nprojection onto convex sets (DPC)---to quickly identify the inactive\nfeatures---that have zero coefficients in the solution vectors across all\ntasks. One of the appealing features of DPC is that: it is safe in the sense\nthat the detected inactive features are guaranteed to have zero coefficients in\nthe solution vectors across all tasks. Thus, by removing the inactive features\nfrom the training phase, we may have substantial savings in the computational\ncost and memory usage without sacrificing accuracy. To the best of our\nknowledge, it is the first screening rule that is applicable to sparse models\nwith multiple data matrices. A key challenge in deriving DPC is to solve a\nnonconvex problem. We show that we can solve for the global optimum efficiently\nvia a properly chosen parametrization of the constraint set. Moreover, DPC has\nvery low computational cost and can be integrated with any existing solvers. We\nhave evaluated the proposed DPC rule on both synthetic and real data sets. The\nexperiments indicate that DPC is very effective in identifying the inactive\nfeatures---especially for high dimensional data---which leads to a speedup up\nto several orders of magnitude.",
    "published": "2015-05-15T14:31:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Shrinkage degree in $L_2$-re-scale boosting for regression",
    "authors": [
      "Lin Xu",
      "Shaobo Lin",
      "Yao Wang",
      "Zongben Xu"
    ],
    "summary": "Re-scale boosting (RBoosting) is a variant of boosting which can essentially\nimprove the generalization performance of boosting learning. The key feature of\nRBoosting lies in introducing a shrinkage degree to re-scale the ensemble\nestimate in each gradient-descent step. Thus, the shrinkage degree determines\nthe performance of RBoosting.\n  The aim of this paper is to develop a concrete analysis concerning how to\ndetermine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways\nto select the shrinkage degree. The first one is to parameterize the shrinkage\ndegree and the other one is to develope a data-driven approach of it. After\nrigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting\nlearning, we compare the pros and cons of the proposed methods. We find that\nalthough these approaches can reach the same learning rates, the structure of\nthe final estimate of the parameterized approach is better, which sometimes\nyields a better generalization capability when the number of sample is finite.\nWith this, we recommend to parameterize the shrinkage degree of\n$L_2$-RBoosting. To this end, we present an adaptive parameter-selection\nstrategy for shrinkage degree and verify its feasibility through both\ntheoretical analysis and numerical verification.\n  The obtained results enhance the understanding of RBoosting and further give\nguidance on how to use $L_2$-RBoosting for regression tasks.",
    "published": "2015-05-17T08:09:43Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Ensemble of Example-Dependent Cost-Sensitive Decision Trees",
    "authors": [
      "Alejandro Correa Bahnsen",
      "Djamila Aouada",
      "Bjorn Ottersten"
    ],
    "summary": "Several real-world classification problems are example-dependent\ncost-sensitive in nature, where the costs due to misclassification vary between\nexamples and not only within classes. However, standard classification methods\ndo not take these costs into account, and assume a constant cost of\nmisclassification errors. In previous works, some methods that take into\naccount the financial costs into the training of different algorithms have been\nproposed, with the example-dependent cost-sensitive decision tree algorithm\nbeing the one that gives the highest savings. In this paper we propose a new\nframework of ensembles of example-dependent cost-sensitive decision-trees. The\nframework consists in creating different example-dependent cost-sensitive\ndecision trees on random subsamples of the training set, and then combining\nthem using three different combination approaches. Moreover, we propose two new\ncost-sensitive combination approaches; cost-sensitive weighted voting and\ncost-sensitive stacking, the latter being based on the cost-sensitive logistic\nregression method. Finally, using five different databases, from four\nreal-world applications: credit card fraud detection, churn modeling, credit\nscoring and direct marketing, we evaluate the proposed method against\nstate-of-the-art example-dependent cost-sensitive techniques, namely,\ncost-proportionate sampling, Bayes minimum risk and cost-sensitive decision\ntrees. The results show that the proposed algorithms have better results for\nall databases, in the sense of higher savings.",
    "published": "2015-05-18T13:43:53Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning with a Drifting Target Concept",
    "authors": [
      "Steve Hanneke",
      "Varun Kanade",
      "Liu Yang"
    ],
    "summary": "We study the problem of learning in the presence of a drifting target\nconcept. Specifically, we provide bounds on the error rate at a given time,\ngiven a learner with access to a history of independent samples labeled\naccording to a target concept that can change on each round. One of our main\ncontributions is a refinement of the best previous results for polynomial-time\nalgorithms for the space of linear separators under a uniform distribution. We\nalso provide general results for an algorithm capable of adapting to a variable\nrate of drift of the target concept. Some of the results also describe an\nactive learning variant of this setting, and provide bounds on the number of\nqueries for the labels of points in the sequence sufficient to obtain the\nstated bounds on the error rates.",
    "published": "2015-05-20T00:41:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bounds on the Minimax Rate for Estimating a Prior over a VC Class from\n  Independent Learning Tasks",
    "authors": [
      "Liu Yang",
      "Steve Hanneke",
      "Jaime Carbonell"
    ],
    "summary": "We study the optimal rates of convergence for estimating a prior distribution\nover a VC class from a sequence of independent data sets respectively labeled\nby independent target functions sampled from the prior. We specifically derive\nupper and lower bounds on the optimal rates under a smoothness condition on the\ncorrect prior, with the number of samples per data set equal the VC dimension.\nThese results have implications for the improvements achievable via transfer\nlearning. We additionally extend this setting to real-valued function, where we\nestablish consistency of an estimator for the prior, and discuss an additional\napplication to a preference elicitation problem in algorithmic economics.",
    "published": "2015-05-20T02:43:24Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear\n  Regret",
    "authors": [
      "Haitham Bou Ammar",
      "Rasul Tutunov",
      "Eric Eaton"
    ],
    "summary": "Lifelong reinforcement learning provides a promising framework for developing\nversatile agents that can accumulate knowledge over a lifetime of experience\nand rapidly learn new tasks by building upon prior knowledge. However, current\nlifelong learning methods exhibit non-vanishing regret as the amount of\nexperience increases and include limitations that can lead to suboptimal or\nunsafe control policies. To address these issues, we develop a lifelong policy\ngradient learner that operates in an adversarial set- ting to learn multiple\ntasks online while enforcing safety constraints on the learned policies. We\ndemonstrate, for the first time, sublinear regret for lifelong policy search,\nand validate our algorithm on several benchmark dynamical systems and an\napplication to quadrotor control.",
    "published": "2015-05-21T17:24:57Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Instant Learning: Parallel Deep Neural Networks and Convolutional\n  Bootstrapping",
    "authors": [
      "Andrew J. R. Simpson"
    ],
    "summary": "Although deep neural networks (DNN) are able to scale with direct advances in\ncomputational power (e.g., memory and processing speed), they are not well\nsuited to exploit the recent trends for parallel architectures. In particular,\ngradient descent is a sequential process and the resulting serial dependencies\nmean that DNN training cannot be parallelized effectively. Here, we show that a\nDNN may be replicated over a massive parallel architecture and used to provide\na cumulative sampling of local solution space which results in rapid and robust\nlearning. We introduce a complimentary convolutional bootstrapping approach\nthat enhances performance of the parallel architecture further. Our\nparallelized convolutional bootstrapping DNN out-performs an identical\nfully-trained traditional DNN after only a single iteration of training.",
    "published": "2015-05-22T07:24:14Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Monotonic Calibrated Interpolated Look-Up Tables",
    "authors": [
      "Maya Gupta",
      "Andrew Cotter",
      "Jan Pfeifer",
      "Konstantin Voevodski",
      "Kevin Canini",
      "Alexander Mangylov",
      "Wojtek Moczydlowski",
      "Alex van Esbroeck"
    ],
    "summary": "Real-world machine learning applications may require functions that are\nfast-to-evaluate and interpretable. In particular, guaranteed monotonicity of\nthe learned function can be critical to user trust. We propose meeting these\ngoals for low-dimensional machine learning problems by learning flexible,\nmonotonic functions using calibrated interpolated look-up tables. We extend the\nstructural risk minimization framework of lattice regression to train monotonic\nlook-up tables by solving a convex problem with appropriate linear inequality\nconstraints. In addition, we propose jointly learning interpretable\ncalibrations of each feature to normalize continuous features and handle\ncategorical or missing data, at the cost of making the objective non-convex. We\naddress large-scale learning through parallelization, mini-batching, and\npropose random sampling of additive regularizer terms. Case studies with\nreal-world problems with five to sixteen features and thousands to millions of\ntraining samples demonstrate the proposed monotonic functions can achieve\nstate-of-the-art accuracy on practical problems while providing greater\ntransparency to users.",
    "published": "2015-05-23T20:57:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Domain Adaptation Extreme Learning Machines for Drift Compensation in\n  E-nose Systems",
    "authors": [
      "Lei Zhang",
      "David Zhang"
    ],
    "summary": "This paper addresses an important issue, known as sensor drift that behaves a\nnonlinear dynamic property in electronic nose (E-nose), from the viewpoint of\nmachine learning. Traditional methods for drift compensation are laborious and\ncostly due to the frequent acquisition and labeling process for gases samples\nrecalibration. Extreme learning machines (ELMs) have been confirmed to be\nefficient and effective learning techniques for pattern recognition and\nregression. However, ELMs primarily focus on the supervised, semi-supervised\nand unsupervised learning problems in single domain (i.e. source domain). To\nour best knowledge, ELM with cross-domain learning capability has never been\nstudied. This paper proposes a unified framework, referred to as Domain\nAdaptation Extreme Learning Machine (DAELM), which learns a robust classifier\nby leveraging a limited number of labeled data from target domain for drift\ncompensation as well as gases recognition in E-nose systems, without loss of\nthe computational efficiency and learning ability of traditional ELM. In the\nunified framework, two algorithms called DAELM-S and DAELM-T are proposed for\nthe purpose of this paper, respectively. In order to percept the differences\namong ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on the\npopular sensor drift data with multiple batches collected by E-nose system\nclearly demonstrate that the proposed DAELM significantly outperforms existing\ndrift compensation methods without cumbersome measures, and also bring new\nperspectives for ELM.",
    "published": "2015-05-24T04:34:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Elastic Net Regularization for Sparse Linear Models",
    "authors": [
      "Zachary C. Lipton",
      "Charles Elkan"
    ],
    "summary": "This paper presents an algorithm for efficient training of sparse linear\nmodels with elastic net regularization. Extending previous work on delayed\nupdates, the new algorithm applies stochastic gradient updates to non-zero\nfeatures only, bringing weights current as needed with closed-form updates.\nClosed-form delayed updates for the $\\ell_1$, $\\ell_{\\infty}$, and rarely used\n$\\ell_2$ regularizers have been described previously. This paper provides\nclosed-form updates for the popular squared norm $\\ell^2_2$ and elastic net\nregularizers.\n  We provide dynamic programming algorithms that perform each delayed update in\nconstant time. The new $\\ell^2_2$ and elastic net methods handle both fixed and\nvarying learning rates, and both standard {stochastic gradient descent} (SGD)\nand {forward backward splitting (FoBoS)}. Experimental results show that on a\nbag-of-words dataset with $260,941$ features, but only $88$ nonzero features on\naverage per training example, the dynamic programming method trains a logistic\nregression classifier with elastic net regularization over $2000$ times faster\nthan otherwise.",
    "published": "2015-05-24T15:42:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Differentially Private Distributed Online Learning",
    "authors": [
      "Chencheng Li",
      "Pan Zhou"
    ],
    "summary": "Online learning has been in the spotlight from the machine learning society\nfor a long time. To handle massive data in Big Data era, one single learner\ncould never efficiently finish this heavy task. Hence, in this paper, we\npropose a novel distributed online learning algorithm to solve the problem.\nComparing to typical centralized online learner, the distributed learners\noptimize their own learning parameters based on local data sources and timely\ncommunicate with neighbors. However, communication may lead to a privacy\nbreach. Thus, we use differential privacy to preserve the privacy of learners,\nand study the influence of guaranteeing differential privacy on the utility of\nthe distributed online learning algorithm. Furthermore, by using the results\nfrom Kakade and Tewari (2009), we use the regret bounds of online learning to\nachieve fast convergence rates for offline learning algorithms in distributed\nscenarios, which provides tighter utility performance than the existing\nstate-of-the-art results. In simulation, we demonstrate that the differentially\nprivate offline learning algorithm has high variance, but we can use mini-batch\nto improve the performance. Finally, the simulations show that the analytical\nresults of our proposed theorems are right and our private distributed online\nlearning algorithm is a general framework.",
    "published": "2015-05-25T07:59:36Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Fantasy Football Prediction",
    "authors": [
      "Roman Lutz"
    ],
    "summary": "The ubiquity of professional sports and specifically the NFL have lead to an\nincrease in popularity for Fantasy Football. Users have many tools at their\ndisposal: statistics, predictions, rankings of experts and even recommendations\nof peers. There are issues with all of these, though. Especially since many\npeople pay money to play, the prediction tools should be enhanced as they\nprovide unbiased and easy-to-use assistance for users. This paper provides and\ndiscusses approaches to predict Fantasy Football scores of Quarterbacks with\nrelatively limited data. In addition to that, it includes several suggestions\non how the data could be enhanced to achieve better results. The dataset\nconsists only of game data from the last six NFL seasons. I used two different\nmethods to predict the Fantasy Football scores of NFL players: Support Vector\nRegression (SVR) and Neural Networks. The results of both are promising given\nthe limited data that was used.",
    "published": "2015-05-26T12:14:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning with Symmetric Label Noise: The Importance of Being Unhinged",
    "authors": [
      "Brendan van Rooyen",
      "Aditya Krishna Menon",
      "Robert C. Williamson"
    ],
    "summary": "Convex potential minimisation is the de facto approach to binary\nclassification. However, Long and Servedio [2010] proved that under symmetric\nlabel noise (SLN), minimisation of any convex potential over a linear function\nclass can result in classification performance equivalent to random guessing.\nThis ostensibly shows that convex losses are not SLN-robust. In this paper, we\npropose a convex, classification-calibrated loss and prove that it is\nSLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of\nbeing negatively unbounded. The loss is a modification of the hinge loss, where\none does not clamp at zero; hence, we call it the unhinged loss. We show that\nthe optimal unhinged solution is equivalent to that of a strongly regularised\nSVM, and is the limiting solution for any convex potential; this implies that\nstrong l2 regularisation makes most standard learners SLN-robust. Experiments\nconfirm the SLN-robustness of the unhinged loss.",
    "published": "2015-05-28T10:38:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Copeland Dueling Bandits",
    "authors": [
      "Masrour Zoghi",
      "Zohar Karnin",
      "Shimon Whiteson",
      "Maarten de Rijke"
    ],
    "summary": "A version of the dueling bandit problem is addressed in which a Condorcet\nwinner may not exist. Two algorithms are proposed that instead seek to minimize\nregret with respect to the Copeland winner, which, unlike the Condorcet winner,\nis guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed\nfor small numbers of arms, while the second, Scalable Copeland Bandits (SCB),\nworks better for large-scale problems. We provide theoretical results bounding\nthe regret accumulated by CCB and SCB, both substantially improving existing\nresults. Such existing results either offer bounds of the form $O(K \\log T)$\nbut require restrictive assumptions, or offer bounds of the form $O(K^2 \\log\nT)$ without requiring such assumptions. Our results offer the best of both\nworlds: $O(K \\log T)$ bounds without restrictive assumptions.",
    "published": "2015-06-01T00:44:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unsupervised Learning on Neural Network Outputs: with Application in\n  Zero-shot Learning",
    "authors": [
      "Yao Lu"
    ],
    "summary": "The outputs of a trained neural network contain much richer information than\njust an one-hot classifier. For example, a neural network might give an image\nof a dog the probability of one in a million of being a cat but it is still\nmuch larger than the probability of being a car. To reveal the hidden structure\nin them, we apply two unsupervised learning algorithms, PCA and ICA, to the\noutputs of a deep Convolutional Neural Network trained on the ImageNet of 1000\nclasses. The PCA/ICA embedding of the object classes reveals their visual\nsimilarity and the PCA/ICA components can be interpreted as common visual\nfeatures shared by similar object classes. For an application, we proposed a\nnew zero-shot learning method, in which the visual features learned by PCA/ICA\nare employed. Our zero-shot learning method achieves the state-of-the-art\nresults on the ImageNet of over 20000 classes.",
    "published": "2015-06-02T19:12:00Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Global and Local Structure Preserving Sparse Subspace Learning: An\n  Iterative Approach to Unsupervised Feature Selection",
    "authors": [
      "Nan Zhou",
      "Yangyang Xu",
      "Hong Cheng",
      "Jun Fang",
      "Witold Pedrycz"
    ],
    "summary": "As we aim at alleviating the curse of high-dimensionality, subspace learning\nis becoming more popular. Existing approaches use either information about\nglobal or local structure of the data, and few studies simultaneously focus on\nglobal and local structures as the both of them contain important information.\nIn this paper, we propose a global and local structure preserving sparse\nsubspace learning (GLoSS) model for unsupervised feature selection. The model\ncan simultaneously realize feature selection and subspace learning. In\naddition, we develop a greedy algorithm to establish a generic combinatorial\nmodel, and an iterative strategy based on an accelerated block coordinate\ndescent is used to solve the GLoSS problem. We also provide whole iterate\nsequence convergence analysis of the proposed iterative algorithm. Extensive\nexperiments are conducted on real-world datasets to show the superiority of the\nproposed approach over several state-of-the-art unsupervised feature selection\napproaches.",
    "published": "2015-06-02T21:02:16Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On bicluster aggregation and its benefits for enumerative solutions",
    "authors": [
      "Saullo Haniell Galvão de Oliveira",
      "Rosana Veroneze",
      "Fernando José Von Zuben"
    ],
    "summary": "Biclustering involves the simultaneous clustering of objects and their\nattributes, thus defining local two-way clustering models. Recently, efficient\nalgorithms were conceived to enumerate all biclusters in real-valued datasets.\nIn this case, the solution composes a complete set of maximal and non-redundant\nbiclusters. However, the ability to enumerate biclusters revealed a challenging\nscenario: in noisy datasets, each true bicluster may become highly fragmented\nand with a high degree of overlapping. It prevents a direct analysis of the\nobtained results. To revert the fragmentation, we propose here two approaches\nfor properly aggregating the whole set of enumerated biclusters: one based on\nsingle linkage and the other directly exploring the rate of overlapping. Both\nproposals were compared with each other and with the actual state-of-the-art in\nseveral experiments, and they not only significantly reduced the number of\nbiclusters but also consistently increased the quality of the solution.",
    "published": "2015-06-02T22:26:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition",
    "authors": [
      "Yi-Hsiu Liao",
      "Hung-Yi Lee",
      "Lin-shan Lee"
    ],
    "summary": "In this paper we propose the Structured Deep Neural Network (Structured DNN)\nas a structured and deep learning algorithm, learning to find the best\nstructured object (such as a label sequence) given a structured input (such as\na vector sequence) by globally considering the mapping relationships between\nthe structure rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learned utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning algorithm. It was shown to beat\nstructured SVM in preliminary experiments on TIMIT.",
    "published": "2015-06-03T08:41:05Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unsupervised Feature Analysis with Class Margin Optimization",
    "authors": [
      "Sen Wang",
      "Feiping Nie",
      "Xiaojun Chang",
      "Lina Yao",
      "Xue Li",
      "Quan Z. Sheng"
    ],
    "summary": "Unsupervised feature selection has been always attracting research attention\nin the communities of machine learning and data mining for decades. In this\npaper, we propose an unsupervised feature selection method seeking a feature\ncoefficient matrix to select the most distinctive features. Specifically, our\nproposed algorithm integrates the Maximum Margin Criterion with a\nsparsity-based model into a joint framework, where the class margin and feature\ncorrelation are taken into account at the same time. To maximize the total data\nseparability while preserving minimized within-class scatter simultaneously, we\npropose to embed Kmeans into the framework generating pseudo class label\ninformation in a scenario of unsupervised feature selection. Meanwhile, a\nsparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to\neffectively discover the sparse structures of the feature coefficient matrix.\nIn this way, noisy and irrelevant features are removed by ruling out those\nfeatures whose corresponding coefficients are zeros. To alleviate the local\noptimum problem that is caused by random initializations of K-means, a\nconvergence guaranteed algorithm with an updating strategy for the clustering\nindicator matrix, is proposed to iteractively chase the optimal solution.\nPerformance evaluation is extensively conducted over six benchmark data sets.\nFrom plenty of experimental results, it is demonstrated that our method has\nsuperior performance against all other compared approaches.",
    "published": "2015-06-03T17:49:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Exploiting an Oracle that Reports AUC Scores in Machine Learning\n  Contests",
    "authors": [
      "Jacob Whitehill"
    ],
    "summary": "In machine learning contests such as the ImageNet Large Scale Visual\nRecognition Challenge and the KDD Cup, contestants can submit candidate\nsolutions and receive from an oracle (typically the organizers of the\ncompetition) the accuracy of their guesses compared to the ground-truth labels.\nOne of the most commonly used accuracy metrics for binary classification tasks\nis the Area Under the Receiver Operating Characteristics Curve (AUC). In this\npaper we provide proofs-of-concept of how knowledge of the AUC of a set of\nguesses can be used, in two different kinds of attacks, to improve the accuracy\nof those guesses. On the other hand, we also demonstrate the intractability of\none kind of AUC exploit by proving that the number of possible binary labelings\nof $n$ examples for which a candidate solution obtains a AUC score of $c$ grows\nexponentially in $n$, for every $c\\in (0,1)$.",
    "published": "2015-06-03T18:06:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Semidefinite and Spectral Relaxations for Multi-Label Classification",
    "authors": [
      "Rémi Lajugie",
      "Piotr Bojanowski",
      "Sylvain Arlot",
      "Francis Bach"
    ],
    "summary": "In this paper, we address the problem of multi-label classification. We\nconsider linear classifiers and propose to learn a prior over the space of\nlabels to directly leverage the performance of such methods. This prior takes\nthe form of a quadratic function of the labels and permits to encode both\nattractive and repulsive relations between labels. We cast this problem as a\nstructured prediction one aiming at optimizing either the accuracies of the\npredictors or the F 1-score. This leads to an optimization problem closely\nrelated to the max-cut problem, which naturally leads to semidefinite and\nspectral relaxations. We show on standard datasets how such a general prior can\nimprove the performances of multi-label techniques.",
    "published": "2015-06-05T09:19:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Multiple Tasks with Multilinear Relationship Networks",
    "authors": [
      "Mingsheng Long",
      "Zhangjie Cao",
      "Jianmin Wang",
      "Philip S. Yu"
    ],
    "summary": "Deep networks trained on large-scale data can learn transferable features to\npromote learning multiple tasks. Since deep features eventually transition from\ngeneral to specific along deep networks, a fundamental problem of multi-task\nlearning is how to exploit the task relatedness underlying parameter tensors\nand improve feature transferability in the multiple task-specific layers. This\npaper presents Multilinear Relationship Networks (MRN) that discover the task\nrelationships based on novel tensor normal priors over parameter tensors of\nmultiple task-specific layers in deep convolutional networks. By jointly\nlearning transferable features and multilinear relationships of tasks and\nfeatures, MRN is able to alleviate the dilemma of negative-transfer in the\nfeature layers and under-transfer in the classifier layer. Experiments show\nthat MRN yields state-of-the-art results on three multi-task learning datasets.",
    "published": "2015-06-06T04:38:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Recurrent Latent Variable Model for Sequential Data",
    "authors": [
      "Junyoung Chung",
      "Kyle Kastner",
      "Laurent Dinh",
      "Kratarth Goel",
      "Aaron Courville",
      "Yoshua Bengio"
    ],
    "summary": "In this paper, we explore the inclusion of latent random variables into the\ndynamic hidden state of a recurrent neural network (RNN) by combining elements\nof the variational autoencoder. We argue that through the use of high-level\nlatent random variables, the variational RNN (VRNN)1 can model the kind of\nvariability observed in highly structured sequential data such as natural\nspeech. We empirically evaluate the proposed model against related sequential\nmodels on four speech datasets and one handwriting dataset. Our results show\nthe important roles that latent random variables can play in the RNN dynamic\nhidden state.",
    "published": "2015-06-07T04:23:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Efficient Learning of Ensembles with QuadBoost",
    "authors": [
      "Louis Fortier-Dubois",
      "François Laviolette",
      "Mario Marchand",
      "Louis-Emile Robitaille",
      "Jean-Francis Roy"
    ],
    "summary": "We first present a general risk bound for ensembles that depends on the Lp\nnorm of the weighted combination of voters which can be selected from a\ncontinuous set. We then propose a boosting method, called QuadBoost, which is\nstrongly supported by the general risk bound and has very simple rules for\nassigning the voters' weights. Moreover, QuadBoost exhibits a rate of decrease\nof its empirical error which is slightly faster than the one achieved by\nAdaBoost. The experimental results confirm the expectation of the theory that\nQuadBoost is a very efficient method for learning ensembles.",
    "published": "2015-06-08T15:10:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On Convergence of Emphatic Temporal-Difference Learning",
    "authors": [
      "Huizhen Yu"
    ],
    "summary": "We consider emphatic temporal-difference learning algorithms for policy\nevaluation in discounted Markov decision processes with finite spaces. Such\nalgorithms were recently proposed by Sutton, Mahmood, and White (2015) as an\nimproved solution to the problem of divergence of off-policy\ntemporal-difference learning with linear function approximation. We present in\nthis paper the first convergence proofs for two emphatic algorithms,\nETD($\\lambda$) and ELSTD($\\lambda$). We prove, under general off-policy\nconditions, the convergence in $L^1$ for ELSTD($\\lambda$) iterates, and the\nalmost sure convergence of the approximate value functions calculated by both\nalgorithms using a single infinitely long trajectory. Our analysis involves new\ntechniques with applications beyond emphatic algorithms leading, for example,\nto the first proof that standard TD($\\lambda$) also converges under off-policy\ntraining for $\\lambda$ sufficiently large.",
    "published": "2015-06-08T16:42:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection",
    "authors": [
      "Zhimin Peng",
      "Prudhvi Gurram",
      "Heesung Kwon",
      "Wotao Yin"
    ],
    "summary": "In this paper, a novel framework of sparse kernel learning for Support Vector\nData Description (SVDD) based anomaly detection is presented. In this work,\noptimal sparse feature selection for anomaly detection is first modeled as a\nMixed Integer Programming (MIP) problem. Due to the prohibitively high\ncomputational complexity of the MIP, it is relaxed into a Quadratically\nConstrained Linear Programming (QCLP) problem. The QCLP problem can then be\npractically solved by using an iterative optimization method, in which multiple\nsubsets of features are iteratively found as opposed to a single subset. The\nQCLP-based iterative optimization problem is solved in a finite space called\nthe \\emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space\nor \\emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of\nthe fact that the geometrical properties of the EKFS and the corresponding RKHS\nremain the same. Now, an explicit nonlinear exploitation of the data in a\nfinite EKFS is achievable, which results in optimal feature ranking.\nExperimental results based on a hyperspectral image show that the proposed\nmethod can provide improved performance over the current state-of-the-art\ntechniques.",
    "published": "2015-06-08T16:51:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Interpretability of Conditional Probability Estimates in the\n  Agnostic Setting",
    "authors": [
      "Yihan Gao",
      "Aditya Parameswaran",
      "Jian Peng"
    ],
    "summary": "We study the interpretability of conditional probability estimates for binary\nclassification under the agnostic setting or scenario. Under the agnostic\nsetting, conditional probability estimates do not necessarily reflect the true\nconditional probabilities. Instead, they have a certain calibration property:\namong all data points that the classifier has predicted P(Y = 1|X) = p, p\nportion of them actually have label Y = 1. For cost-sensitive decision\nproblems, this calibration property provides adequate support for us to use\nBayes Decision Theory. In this paper, we define a novel measure for the\ncalibration property together with its empirical counterpart, and prove an\nuniform convergence result between them. This new measure enables us to\nformally justify the calibration property of conditional probability\nestimations, and provides new insights on the problem of estimating and\ncalibrating conditional probabilities.",
    "published": "2015-06-09T17:41:48Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Max-Entropy Feed-Forward Clustering Neural Network",
    "authors": [
      "Han Xiao",
      "Xiaoyan Zhu"
    ],
    "summary": "The outputs of non-linear feed-forward neural network are positive, which\ncould be treated as probability when they are normalized to one. If we take\nEntropy-Based Principle into consideration, the outputs for each sample could\nbe represented as the distribution of this sample for different clusters.\nEntropy-Based Principle is the principle with which we could estimate the\nunknown distribution under some limited conditions. As this paper defines two\nprocesses in Feed-Forward Neural Network, our limited condition is the\nabstracted features of samples which are worked out in the abstraction process.\nAnd the final outputs are the probability distribution for different clusters\nin the clustering process. As Entropy-Based Principle is considered into the\nfeed-forward neural network, a clustering method is born. We have conducted\nsome experiments on six open UCI datasets, comparing with a few baselines and\napplied purity as the measurement . The results illustrate that our method\noutperforms all the other baselines that are most popular clustering methods.",
    "published": "2015-06-11T11:01:40Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Margin-Based Feed-Forward Neural Network Classifiers",
    "authors": [
      "Han Xiao",
      "Xiaoyan Zhu"
    ],
    "summary": "Margin-Based Principle has been proposed for a long time, it has been proved\nthat this principle could reduce the structural risk and improve the\nperformance in both theoretical and practical aspects. Meanwhile, feed-forward\nneural network is a traditional classifier, which is very hot at present with a\ndeeper architecture. However, the training algorithm of feed-forward neural\nnetwork is developed and generated from Widrow-Hoff Principle that means to\nminimize the squared error. In this paper, we propose a new training algorithm\nfor feed-forward neural networks based on Margin-Based Principle, which could\neffectively promote the accuracy and generalization ability of neural network\nclassifiers with less labelled samples and flexible network. We have conducted\nexperiments on four UCI open datasets and achieved good results as expected. In\nconclusion, our model could handle more sparse labelled and more high-dimension\ndataset in a high accuracy while modification from old ANN method to our method\nis easy and almost free of work.",
    "published": "2015-06-11T11:10:25Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Equivalence of CoCoA+ and DisDCA",
    "authors": [
      "Ching-pei Lee"
    ],
    "summary": "In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)\nunder the setting used in their experiments, which is also the best setting\nsuggested by the authors that proposed this algorithm, is equivalent to the\npractical variant of DisDCA (Yang, NIPS, 2013).",
    "published": "2015-06-13T03:52:44Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to\n  Novel Algorithms",
    "authors": [
      "Yunwen Lei",
      "Ürün Dogan",
      "Alexander Binder",
      "Marius Kloft"
    ],
    "summary": "This paper studies the generalization performance of multi-class\nclassification algorithms, for which we obtain, for the first time, a\ndata-dependent generalization error bound with a logarithmic dependence on the\nclass size, substantially improving the state-of-the-art linear dependence in\nthe existing data-dependent generalization analysis. The theoretical analysis\nmotivates us to introduce a new multi-class classification machine based on\n$\\ell_p$-norm regularization, where the parameter $p$ controls the complexity\nof the corresponding bounds. We derive an efficient optimization algorithm\nbased on Fenchel duality theory. Benchmarks on several real-world datasets show\nthat the proposed algorithm can achieve significant accuracy gains over the\nstate of the art.",
    "published": "2015-06-14T08:07:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Localized Multiple Kernel Learning---A Convex Approach",
    "authors": [
      "Yunwen Lei",
      "Alexander Binder",
      "Ürün Dogan",
      "Marius Kloft"
    ],
    "summary": "We propose a localized approach to multiple kernel learning that can be\nformulated as a convex optimization problem over a given cluster structure. For\nwhich we obtain generalization error guarantees and derive an optimization\nalgorithm based on the Fenchel dual representation. Experiments on real-world\ndatasets from the application domains of computational biology and computer\nvision show that convex localized multiple kernel learning can achieve higher\nprediction accuracies than its global and non-convex local counterparts.",
    "published": "2015-06-14T09:11:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Fast Incremental Gaussian Mixture Model",
    "authors": [
      "Rafael Pinto",
      "Paulo Engel"
    ],
    "summary": "This work builds upon previous efforts in online incremental learning, namely\nthe Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of\nlearning from data streams in a single-pass by improving its model after\nanalyzing each data point and discarding it thereafter. Nevertheless, it\nsuffers from the scalability point-of-view, due to its asymptotic time\ncomplexity of $\\operatorname{O}\\bigl(NKD^3\\bigr)$ for $N$ data points, $K$\nGaussian components and $D$ dimensions, rendering it inadequate for\nhigh-dimensional data. In this paper, we manage to reduce this complexity to\n$\\operatorname{O}\\bigl(NKD^2\\bigr)$ by deriving formulas for working directly\nwith precision matrices instead of covariance matrices. The final result is a\nmuch faster and scalable algorithm which can be applied to high dimensional\ntasks. This is confirmed by applying the modified algorithm to high-dimensional\nclassification datasets.",
    "published": "2015-06-14T17:02:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an\n  Online-Incremental-Transfer Strategy",
    "authors": [
      "Sang-Woo Lee",
      "Min-Oh Heo",
      "Jiwon Kim",
      "Jeonghee Kim",
      "Byoung-Tak Zhang"
    ],
    "summary": "The online learning of deep neural networks is an interesting problem of\nmachine learning because, for example, major IT companies want to manage the\ninformation of the massive data uploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong learning. We aim to train\ndeep models from new data that consists of new classes, distributions, and\ntasks at minimal computational cost, which we call online deep learning.\nUnfortunately, deep neural network learning through classical online and\nincremental methods does not work well in both theory and practice. In this\npaper, we introduce dual memory architectures for online incremental deep\nlearning. The proposed architecture consists of deep representation learners\nand fast learnable shallow kernel networks, both of which synergize to track\nthe information of new data. During the training phase, we use various online,\nincremental ensemble, and transfer learning techniques in order to achieve\nlower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image\nrecognition tasks, the proposed dual memory architectures performs much better\nthan the classical online and incremental ensemble algorithm, and their\naccuracies are similar to that of the batch learner.",
    "published": "2015-06-15T04:44:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning Deep Generative Models with Doubly Stochastic MCMC",
    "authors": [
      "Chao Du",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "summary": "We present doubly stochastic gradient MCMC, a simple and generic method for\n(approximate) Bayesian inference of deep generative models (DGMs) in a\ncollapsed continuous parameter space. At each MCMC sampling step, the algorithm\nrandomly draws a mini-batch of data samples to estimate the gradient of\nlog-posterior and further estimates the intractable expectation over hidden\nvariables via a neural adaptive importance sampler, where the proposal\ndistribution is parameterized by a deep neural network and learnt jointly. We\ndemonstrate the effectiveness on learning various DGMs in a wide range of\ntasks, including density estimation, data generation and missing data\nimputation. Our method outperforms many state-of-the-art competitors.",
    "published": "2015-06-15T11:37:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Latent Regression Bayesian Network for Data Representation",
    "authors": [
      "Siqi Nie",
      "Qiang Ji"
    ],
    "summary": "Deep directed generative models have attracted much attention recently due to\ntheir expressive representation power and the ability of ancestral sampling.\nOne major difficulty of learning directed models with many latent variables is\nthe intractable inference. To address this problem, most existing algorithms\nmake assumptions to render the latent variables independent of each other,\neither by designing specific priors, or by approximating the true posterior\nusing a factorized distribution. We believe the correlations among latent\nvariables are crucial for faithful data representation. Driven by this idea, we\npropose an inference method based on the conditional pseudo-likelihood that\npreserves the dependencies among the latent variables. For learning, we propose\nto employ the hard Expectation Maximization (EM) algorithm, which avoids the\nintractability of the traditional EM by max-out instead of sum-out to compute\nthe data likelihood. Qualitative and quantitative evaluations of our model\nagainst state of the art deep models on benchmark datasets demonstrate the\neffectiveness of the proposed algorithm in data representation and\nreconstruction.",
    "published": "2015-06-15T19:34:59Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cheap Bandits",
    "authors": [
      "Manjesh Kumar Hanawal",
      "Venkatesh Saligrama",
      "Michal Valko",
      "R\\' emi Munos"
    ],
    "summary": "We consider stochastic sequential learning problems where the learner can\nobserve the \\textit{average reward of several actions}. Such a setting is\ninteresting in many applications involving monitoring and surveillance, where\nthe set of the actions to observe represent some (geographical) area. The\nimportance of this setting is that in these applications, it is actually\n\\textit{cheaper} to observe average reward of a group of actions rather than\nthe reward of a single action. We show that when the reward is \\textit{smooth}\nover a given graph representing the neighboring actions, we can maximize the\ncumulative reward of learning while \\textit{minimizing the sensing cost}. In\nthis paper we propose CheapUCB, an algorithm that matches the regret guarantees\nof the known algorithms for this setting and at the same time guarantees a\nlinear cost again over them. As a by-product of our analysis, we establish a\n$\\Omega(\\sqrt{dT})$ lower bound on the cumulative regret of spectral bandits\nfor a class of graphs with effective dimension $d$.",
    "published": "2015-06-15T21:42:45Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Online Gradient Boosting",
    "authors": [
      "Alina Beygelzimer",
      "Elad Hazan",
      "Satyen Kale",
      "Haipeng Luo"
    ],
    "summary": "We extend the theory of boosting for regression problems to the online\nlearning setting. Generalizing from the batch setting for boosting, the notion\nof a weak learning algorithm is modeled as an online learning algorithm with\nlinear loss functions that competes with a base class of regression functions,\nwhile a strong learning algorithm is an online learning algorithm with convex\nloss functions that competes with a larger class of regression functions. Our\nmain result is an online gradient boosting algorithm which converts a weak\nonline learning algorithm into a strong one where the larger class of functions\nis the linear span of the base class. We also give a simpler boosting algorithm\nthat converts a weak online learning algorithm into a strong one where the\nlarger class of functions is the convex hull of the base class, and prove its\noptimality.",
    "published": "2015-06-16T02:20:32Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Learning with Clustering Structure",
    "authors": [
      "Vincent Roulet",
      "Fajwel Fogel",
      "Alexandre d'Aspremont",
      "Francis Bach"
    ],
    "summary": "We study supervised learning problems using clustering constraints to impose\nstructure on either features or samples, seeking to help both prediction and\ninterpretation. The problem of clustering features arises naturally in text\nclassification for instance, to reduce dimensionality by grouping words\ntogether and identify synonyms. The sample clustering problem on the other\nhand, applies to multiclass problems where we are allowed to make multiple\npredictions and the performance of the best answer is recorded. We derive a\nunified optimization formulation highlighting the common structure of these\nproblems and produce algorithms whose core iteration complexity amounts to a\nk-means clustering step, which can be approximated efficiently. We extend these\nresults to combine sparsity and clustering constraints, and develop a new\nprojection algorithm on the set of clustered sparse vectors. We prove\nconvergence of our algorithms on random instances, based on a union of\nsubspaces interpretation of the clustering structure. Finally, we test the\nrobustness of our methods on artificial data sets as well as real data\nextracted from movie reviews.",
    "published": "2015-06-16T10:44:30Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Numeric Input Relations for Relational Learning with Applications to\n  Community Structure Analysis",
    "authors": [
      "Jiuchuan Jiang",
      "Manfred Jaeger"
    ],
    "summary": "Most work in the area of statistical relational learning (SRL) is focussed on\ndiscrete data, even though a few approaches for hybrid SRL models have been\nproposed that combine numerical and discrete variables. In this paper we\ndistinguish numerical random variables for which a probability distribution is\ndefined by the model from numerical input variables that are only used for\nconditioning the distribution of discrete response variables. We show how\nnumerical input relations can very easily be used in the Relational Bayesian\nNetwork framework, and that existing inference and learning methods need only\nminor adjustments to be applied in this generalized setting. The resulting\nframework provides natural relational extensions of classical probabilistic\nmodels for categorical data. We demonstrate the usefulness of RBN models with\nnumeric input relations by several examples.\n  In particular, we use the augmented RBN framework to define probabilistic\nmodels for multi-relational (social) networks in which the probability of a\nlink between two nodes depends on numeric latent feature vectors associated\nwith the nodes. A generic learning procedure can be used to obtain a\nmaximum-likelihood fit of model parameters and latent feature values for a\nvariety of models that can be expressed in the high-level RBN representation.\nSpecifically, we propose a model that allows us to interpret learned latent\nfeature values as community centrality degrees by which we can identify nodes\nthat are central for one community, that are hubs between communities, or that\nare isolated nodes. In a multi-relational setting, the model also provides a\ncharacterization of how different relations are associated with each community.",
    "published": "2015-06-16T18:18:06Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "On the Depth of Deep Neural Networks: A Theoretical View",
    "authors": [
      "Shizhao Sun",
      "Wei Chen",
      "Liwei Wang",
      "Xiaoguang Liu",
      "Tie-Yan Liu"
    ],
    "summary": "People believe that depth plays an important role in success of deep neural\nnetworks (DNN). However, this belief lacks solid theoretical justifications as\nfar as we know. We investigate role of depth from perspective of margin bound.\nIn margin bound, expected error is upper bounded by empirical margin error plus\nRademacher Average (RA) based capacity term. First, we derive an upper bound\nfor RA of DNN, and show that it increases with increasing depth. This indicates\nnegative impact of depth on test performance. Second, we show that deeper\nnetworks tend to have larger representation power (measured by Betti numbers\nbased complexity) than shallower networks in multi-class setting, and thus can\nlead to smaller empirical margin error. This implies positive impact of depth.\nThe combination of these two results shows that for DNN with restricted number\nof hidden units, increasing depth is not always good since there is a tradeoff\nbetween positive and negative impacts. These results inspire us to seek\nalternative ways to achieve positive impact of depth, e.g., imposing\nmargin-based penalty terms to cross entropy loss so as to reduce empirical\nmargin error without increasing depth. Our experiments show that in this way,\nwe achieve significantly better test performance.",
    "published": "2015-06-17T07:51:42Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Gradient Estimation Using Stochastic Computation Graphs",
    "authors": [
      "John Schulman",
      "Nicolas Heess",
      "Theophane Weber",
      "Pieter Abbeel"
    ],
    "summary": "In a variety of problems originating in supervised, unsupervised, and\nreinforcement learning, the loss function is defined by an expectation over a\ncollection of random variables, which might be part of a probabilistic model or\nthe external world. Estimating the gradient of this loss function, using\nsamples, lies at the core of gradient-based learning algorithms for these\nproblems. We introduce the formalism of stochastic computation\ngraphs---directed acyclic graphs that include both deterministic functions and\nconditional probability distributions---and describe how to easily and\nautomatically derive an unbiased estimator of the loss function's gradient. The\nresulting algorithm for computing the gradient estimator is a simple\nmodification of the standard backpropagation algorithm. The generic scheme we\npropose unifies estimators derived in variety of prior work, along with\nvariance-reduction techniques therein. It could assist researchers in\ndeveloping intricate models involving a combination of stochastic and\ndeterministic operations, enabling, for example, attention, memory, and control\nactions.",
    "published": "2015-06-17T09:32:31Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Scalable Semi-Supervised Aggregation of Classifiers",
    "authors": [
      "Akshay Balsubramani",
      "Yoav Freund"
    ],
    "summary": "We present and empirically evaluate an efficient algorithm that learns to\naggregate the predictions of an ensemble of binary classifiers. The algorithm\nuses the structure of the ensemble predictions on unlabeled data to yield\nsignificant performance improvements. It does this without making assumptions\non the structure or origin of the ensemble, without parameters, and as scalably\nas linear learning. We empirically demonstrate these performance gains with\nrandom forests.",
    "published": "2015-06-18T19:53:12Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "The Extreme Value Machine",
    "authors": [
      "Ethan M. Rudd",
      "Lalit P. Jain",
      "Walter J. Scheirer",
      "Terrance E. Boult"
    ],
    "summary": "It is often desirable to be able to recognize when inputs to a recognition\nfunction learned in a supervised manner correspond to classes unseen at\ntraining time. With this ability, new class labels could be assigned to these\ninputs by a human operator, allowing them to be incorporated into the\nrecognition function --- ideally under an efficient incremental update\nmechanism. While good algorithms that assume inputs from a fixed set of classes\nexist, e.g., artificial neural networks and kernel machines, it is not\nimmediately obvious how to extend them to perform incremental learning in the\npresence of unknown query classes. Existing algorithms take little to no\ndistributional information into account when learning recognition functions and\nlack a strong theoretical foundation. We address this gap by formulating a\nnovel, theoretically sound classifier --- the Extreme Value Machine (EVM). The\nEVM has a well-grounded interpretation derived from statistical Extreme Value\nTheory (EVT), and is the first classifier to be able to perform nonlinear\nkernel-free variable bandwidth incremental learning. Compared to other\nclassifiers in the same deep network derived feature space, the EVM is accurate\nand efficient on an established benchmark partition of the ImageNet dataset.",
    "published": "2015-06-19T19:04:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Strategic Classification",
    "authors": [
      "Moritz Hardt",
      "Nimrod Megiddo",
      "Christos Papadimitriou",
      "Mary Wootters"
    ],
    "summary": "Machine learning relies on the assumption that unseen test instances of a\nclassification problem follow the same distribution as observed training data.\nHowever, this principle can break down when machine learning is used to make\nimportant decisions about the welfare (employment, education, health) of\nstrategic individuals. Knowing information about the classifier, such\nindividuals may manipulate their attributes in order to obtain a better\nclassification outcome. As a result of this behavior---often referred to as\ngaming---the performance of the classifier may deteriorate sharply. Indeed,\ngaming is a well-known obstacle for using machine learning methods in practice;\nin financial policy-making, the problem is widely known as Goodhart's law. In\nthis paper, we formalize the problem, and pursue algorithms for learning\nclassifiers that are robust to gaming.\n  We model classification as a sequential game between a player named \"Jury\"\nand a player named \"Contestant.\" Jury designs a classifier, and Contestant\nreceives an input to the classifier, which he may change at some cost. Jury's\ngoal is to achieve high classification accuracy with respect to Contestant's\noriginal input and some underlying target classification function. Contestant's\ngoal is to achieve a favorable classification outcome while taking into account\nthe cost of achieving it.\n  For a natural class of cost functions, we obtain computationally efficient\nlearning algorithms which are near-optimal. Surprisingly, our algorithms are\nefficient even on concept classes that are computationally hard to learn. For\ngeneral cost functions, designing an approximately optimal strategy-proof\nclassifier, for inverse-polynomial approximation, is NP-hard.",
    "published": "2015-06-23T13:22:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Unconfused ultraconservative multiclass algorithms",
    "authors": [
      "Ugo Louche",
      "Liva Ralaivola"
    ],
    "summary": "We tackle the problem of learning linear classifiers from noisy datasets in a\nmulticlass setting. The two-class version of this problem was studied a few\nyears ago where the proposed approaches to combat the noise revolve around a\nPer-ceptron learning scheme fed with peculiar examples computed through a\nweighted average of points from the noisy training set. We propose to build\nupon these approaches and we introduce a new algorithm called UMA (for\nUnconfused Multiclass additive Algorithm) which may be seen as a generalization\nto the multiclass setting of the previous approaches. In order to characterize\nthe noise we use the confusion matrix as a multiclass extension of the\nclassification noise studied in the aforemen-tioned literature. Theoretically\nwell-founded, UMA furthermore displays very good empirical noise robustness, as\nevidenced by numerical simulations conducted on both synthetic and real data.",
    "published": "2015-06-24T06:31:21Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications",
    "authors": [
      "Luc Le Magoarou",
      "Rémi Gribonval"
    ],
    "summary": "The computational cost of many signal processing and machine learning\ntechniques is often dominated by the cost of applying certain linear operators\nto high-dimensional vectors. This paper introduces an algorithm aimed at\nreducing the complexity of applying linear operators in high dimension by\napproximately factorizing the corresponding matrix into few sparse factors. The\napproach relies on recent advances in non-convex optimization. It is first\nexplained and analyzed in details and then demonstrated experimentally on\nvarious problems including dictionary learning for image denoising, and the\napproximation of large matrices arising in inverse problems.",
    "published": "2015-06-24T10:02:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic\n  Algorithms",
    "authors": [
      "Yuchen Zhang",
      "Michael I. Jordan"
    ],
    "summary": "Stochastic algorithms are efficient approaches to solving machine learning\nand optimization problems. In this paper, we propose a general framework called\nSplash for parallelizing stochastic algorithms on multi-node distributed\nsystems. Splash consists of a programming interface and an execution engine.\nUsing the programming interface, the user develops sequential stochastic\nalgorithms without concerning any detail about distributed computing. The\nalgorithm is then automatically parallelized by a communication-efficient\nexecution engine. We provide theoretical justifications on the optimal rate of\nconvergence for parallelizing stochastic gradient descent. Splash is built on\ntop of Apache Spark. The real-data experiments on logistic regression,\ncollaborative filtering and topic modeling verify that Splash yields\norder-of-magnitude speedup over single-thread stochastic algorithms and over\nstate-of-the-art implementations on Spark.",
    "published": "2015-06-24T20:39:54Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Conservativeness of untied auto-encoders",
    "authors": [
      "Daniel Jiwoong Im",
      "Mohamed Ishmael Diwan Belghazi",
      "Roland Memisevic"
    ],
    "summary": "We discuss necessary and sufficient conditions for an auto-encoder to define\na conservative vector field, in which case it is associated with an energy\nfunction akin to the unnormalized log-probability of the data. We show that the\nconditions for conservativeness are more general than for encoder and decoder\nweights to be the same (\"tied weights\"), and that they also depend on the form\nof the hidden unit activation function, but that contractive training criteria,\nsuch as denoising, will enforce these conditions locally. Based on these\nobservations, we show how we can use auto-encoders to extract the conservative\ncomponent of a vector field.",
    "published": "2015-06-25T07:23:56Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Occam's Gates",
    "authors": [
      "Jonathan Raiman",
      "Szymon Sidor"
    ],
    "summary": "We present a complimentary objective for training recurrent neural networks\n(RNN) with gating units that helps with regularization and interpretability of\nthe trained model. Attention-based RNN models have shown success in many\ndifficult sequence to sequence classification problems with long and short term\ndependencies, however these models are prone to overfitting. In this paper, we\ndescribe how to regularize these models through an L1 penalty on the activation\nof the gating units, and show that this technique reduces overfitting on a\nvariety of tasks while also providing to us a human-interpretable visualization\nof the inputs used by the network. These tasks include sentiment analysis,\nparaphrase recognition, and question answering.",
    "published": "2015-06-27T03:03:10Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Non-convex Regularizations for Feature Selection in Ranking With Sparse\n  SVM",
    "authors": [
      "Léa Laporte",
      "Rémi Flamary",
      "Stephane Canu",
      "Sébastien Déjean",
      "Josiane Mothe"
    ],
    "summary": "Feature selection in learning to rank has recently emerged as a crucial\nissue. Whereas several preprocessing approaches have been proposed, only a few\nworks have been focused on integrating the feature selection into the learning\nprocess. In this work, we propose a general framework for feature selection in\nlearning to rank using SVM with a sparse regularization term. We investigate\nboth classical convex regularizations such as $\\ell\\_1$ or weighted $\\ell\\_1$\nand non-convex regularization terms such as log penalty, Minimax Concave\nPenalty (MCP) or $\\ell\\_p$ pseudo norm with $p\\textless{}1$. Two algorithms are\nproposed, first an accelerated proximal approach for solving the convex\nproblems, second a reweighted $\\ell\\_1$ scheme to address the non-convex\nregularizations. We conduct intensive experiments on nine datasets from Letor\n3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convex\nregularizations we propose leads to more sparsity in the resulting models while\nprediction performance is preserved. The number of features is decreased by up\nto a factor of six compared to the $\\ell\\_1$ regularization. In addition, the\nsoftware is publicly available on the web.",
    "published": "2015-07-02T10:06:02Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Optimal Transport for Domain Adaptation",
    "authors": [
      "Nicolas Courty",
      "Rémi Flamary",
      "Devis Tuia",
      "Alain Rakotomamonjy"
    ],
    "summary": "Domain adaptation from one data space (or domain) to another is one of the\nmost challenging tasks of modern data analytics. If the adaptation is done\ncorrectly, models built on a specific data space become more robust when\nconfronted to data depicting the same semantic concepts (the classes), but\nobserved by another observation system with its own specificities. Among the\nmany strategies proposed to adapt a domain to another, finding a common\nrepresentation has shown excellent properties: by finding a common\nrepresentation for both domains, a single classifier can be effective in both\nand use labelled samples from the source domain to predict the unlabelled\nsamples of the target domain. In this paper, we propose a regularized\nunsupervised optimal transportation model to perform the alignment of the\nrepresentations in the source and target domains. We learn a transportation\nplan matching both PDFs, which constrains labelled samples in the source domain\nto remain close during transport. This way, we exploit at the same time the few\nlabeled information in the source and the unlabelled distributions observed in\nboth domains. Experiments in toy and challenging real visual adaptation\nexamples show the interest of the method, that consistently outperforms state\nof the art approaches.",
    "published": "2015-07-02T10:15:11Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Combining Models of Approximation with Partial Learning",
    "authors": [
      "Ziyuan Gao",
      "Frank Stephan",
      "Sandra Zilles"
    ],
    "summary": "In Gold's framework of inductive inference, the model of partial learning\nrequires the learner to output exactly one correct index for the target object\nand only the target object infinitely often. Since infinitely many of the\nlearner's hypotheses may be incorrect, it is not obvious whether a partial\nlearner can be modifed to \"approximate\" the target object.\n  Fulk and Jain (Approximate inference and scientific method. Information and\nComputation 114(2):179--191, 1994) introduced a model of approximate learning\nof recursive functions. The present work extends their research and solves an\nopen problem of Fulk and Jain by showing that there is a learner which\napproximates and partially identifies every recursive function by outputting a\nsequence of hypotheses which, in addition, are also almost all finite variants\nof the target function.\n  The subsequent study is dedicated to the question how these findings\ngeneralise to the learning of r.e. languages from positive data. Here three\nvariants of approximate learning will be introduced and investigated with\nrespect to the question whether they can be combined with partial learning.\nFollowing the line of Fulk and Jain's research, further investigations provide\nconditions under which partial language learners can eventually output only\nfinite variants of the target language. The combinabilities of other partial\nlearning criteria will also be briefly studied.",
    "published": "2015-07-05T12:49:20Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Simple Algorithm for Maximum Margin Classification, Revisited",
    "authors": [
      "Sariel Har-Peled"
    ],
    "summary": "In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] for\ncomputing a linear maximum margin classifier. Our presentation is self\ncontained, and the algorithm itself is slightly simpler than the original\nalgorithm. The algorithm itself is a simple Perceptron like iterative\nalgorithm. For more details and background, the reader is referred to the\noriginal paper.",
    "published": "2015-07-06T18:53:09Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Bayesian Approach for Online Classifier Ensemble",
    "authors": [
      "Qinxun Bai",
      "Henry Lam",
      "Stan Sclaroff"
    ],
    "summary": "We propose a Bayesian approach for recursively estimating the classifier\nweights in online learning of a classifier ensemble. In contrast with past\nmethods, such as stochastic gradient descent or online boosting, our approach\nestimates the weights by recursively updating its posterior distribution. For a\nspecified class of loss functions, we show that it is possible to formulate a\nsuitably defined likelihood function and hence use the posterior distribution\nas an approximation to the global empirical loss minimizer. If the stream of\ntraining data is sampled from a stationary process, we can also show that our\napproach admits a superior rate of convergence to the expected loss minimizer\nthan is possible with standard stochastic gradient descent. In experiments with\nreal-world datasets, our formulation often performs better than\nstate-of-the-art stochastic gradient descent and online boosting algorithms.",
    "published": "2015-07-08T03:35:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams\n  of Graphs",
    "authors": [
      "Giovanni Da San Martino",
      "Nicolò Navarin",
      "Alessandro Sperduti"
    ],
    "summary": "Kernel methods are considered an effective technique for on-line learning.\nMany approaches have been developed for compactly representing the dual\nsolution of a kernel method when the problem imposes memory constraints.\nHowever, in literature no work is specifically tailored to streams of graphs.\nMotivated by the fact that the size of the feature space representation of many\nstate-of-the-art graph kernels is relatively small and thus it is explicitly\ncomputable, we study whether executing kernel algorithms in the feature space\ncan be more effective than the classical dual approach. We study three\ndifferent algorithms and various strategies for managing the budget. Efficiency\nand efficacy of the proposed approaches are experimentally assessed on\nrelatively large graph streams exhibiting concept drift. It turns out that,\nwhen strict memory budget constraints have to be enforced, working in feature\nspace, given the current state of the art on graph kernels, is more than a\nviable alternative to dual approaches, both in terms of speed and\nclassification performance.",
    "published": "2015-07-08T13:58:19Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Extending local features with contextual information in graph kernels",
    "authors": [
      "Nicolò Navarin",
      "Alessandro Sperduti",
      "Riccardo Tesselli"
    ],
    "summary": "Graph kernels are usually defined in terms of simpler kernels over local\nsubstructures of the original graphs. Different kernels consider different\ntypes of substructures. However, in some cases they have similar predictive\nperformances, probably because the substructures can be interpreted as\napproximations of the subgraphs they induce. In this paper, we propose to\nassociate to each feature a piece of information about the context in which the\nfeature appears in the graph. A substructure appearing in two different graphs\nwill match only if it appears with the same context in both graphs. We propose\na kernel based on this idea that considers trees as substructures, and where\nthe contexts are features too. The kernel is inspired from the framework in\n[6], even if it is not part of it. We give an efficient algorithm for computing\nthe kernel and show promising results on real-world graph classification\ndatasets.",
    "published": "2015-07-08T14:58:49Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Utility-based Dueling Bandits as a Partial Monitoring Game",
    "authors": [
      "Pratik Gajane",
      "Tanguy Urvoy"
    ],
    "summary": "Partial monitoring is a generic framework for sequential decision-making with\nincomplete feedback. It encompasses a wide class of problems such as dueling\nbandits, learning with expect advice, dynamic pricing, dark pools, and label\nefficient prediction. We study the utility-based dueling bandit problem as an\ninstance of partial monitoring problem and prove that it fits the time-regret\npartial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We\nsurvey some partial monitoring algorithms and see how they could be used to\nsolve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits,\nPartial Monitoring, Partial Feedback, Multiarmed Bandits",
    "published": "2015-07-10T00:05:38Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Spectral Smoothing via Random Matrix Perturbations",
    "authors": [
      "Jacob Abernethy",
      "Chansoo Lee",
      "Ambuj Tewari"
    ],
    "summary": "We consider stochastic smoothing of spectral functions of matrices using\nperturbations commonly studied in random matrix theory. We show that a spectral\nfunction remains spectral when smoothed using a unitarily invariant\nperturbation distribution. We then derive state-of-the-art smoothing bounds for\nthe maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE).\nSmoothing the maximum eigenvalue function is important for applications in\nsemidefinite optimization and online learning. As a direct consequence of our\nGOE smoothing results, we obtain an $O((N \\log N)^{1/4} \\sqrt{T})$ expected\nregret bound for the online variance minimization problem using an algorithm\nthat performs only a single maximum eigenvector computation per time step. Here\n$T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and\nits analysis also extend to the more general online PCA problem where the\nlearner has to output a rank $k$ subspace. The algorithm just requires\ncomputing $k$ maximum eigenvectors per step and enjoys an $O(k (N \\log N)^{1/4}\n\\sqrt{T})$ expected regret bound.",
    "published": "2015-07-10T20:52:35Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A new boosting algorithm based on dual averaging scheme",
    "authors": [
      "Nan Wang"
    ],
    "summary": "The fields of machine learning and mathematical optimization increasingly\nintertwined. The special topic on supervised learning and convex optimization\nexamines this interplay. The training part of most supervised learning\nalgorithms can usually be reduced to an optimization problem that minimizes a\nloss between model predictions and training data. While most optimization\ntechniques focus on accuracy and speed of convergence, the qualities of good\noptimization algorithm from the machine learning perspective can be quite\ndifferent since machine learning is more than fitting the data. Better\noptimization algorithms that minimize the training loss can possibly give very\npoor generalization performance. In this paper, we examine a particular kind of\nmachine learning algorithm, boosting, whose training process can be viewed as\nfunctional coordinate descent on the exponential loss. We study the relation\nbetween optimization techniques and machine learning by implementing a new\nboosting algorithm. DABoost, based on dual-averaging scheme and study its\ngeneralization performance. We show that DABoost, although slower in reducing\nthe training error, in general enjoys a better generalization error than\nAdaBoost.",
    "published": "2015-07-11T16:46:37Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Cluster-Aided Mobility Predictions",
    "authors": [
      "Jaeseong Jeong",
      "Mathieu Leconte",
      "Alexandre Proutiere"
    ],
    "summary": "Predicting the future location of users in wireless net- works has numerous\napplications, and can help service providers to improve the quality of service\nperceived by their clients. The location predictors proposed so far estimate\nthe next location of a specific user by inspecting the past individual\ntrajectories of this user. As a consequence, when the training data collected\nfor a given user is limited, the resulting prediction is inaccurate. In this\npaper, we develop cluster-aided predictors that exploit past trajectories\ncollected from all users to predict the next location of a given user. These\npredictors rely on clustering techniques and extract from the training data\nsimilarities among the mobility patterns of the various users to improve the\nprediction accuracy. Specifically, we present CAMP (Cluster-Aided Mobility\nPredictor), a cluster-aided predictor whose design is based on recent\nnon-parametric bayesian statistical tools. CAMP is robust and adaptive in the\nsense that it exploits similarities in users' mobility only if such\nsimilarities are really present in the training data. We analytically prove the\nconsistency of the predictions provided by CAMP, and investigate its\nperformance using two large-scale datasets. CAMP significantly outperforms\nexisting predictors, and in particular those that only exploit individual past\ntrajectories.",
    "published": "2015-07-12T23:27:50Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Ordered Decompositional DAG Kernels Enhancements",
    "authors": [
      "Giovanni Da San Martino",
      "Nicolò Navarin",
      "Alessandro Sperduti"
    ],
    "summary": "In this paper, we show how the Ordered Decomposition DAGs (ODD) kernel\nframework, a framework that allows the definition of graph kernels from tree\nkernels, allows to easily define new state-of-the-art graph kernels. Here we\nconsider a fast graph kernel based on the Subtree kernel (ST), and we propose\nvarious enhancements to increase its expressiveness. The proposed DAG kernel\nhas the same worst-case complexity as the one based on ST, but an improved\nexpressivity due to an augmented set of features. Moreover, we propose a novel\nweighting scheme for the features, which can be applied to other kernels of the\nODD framework. These improvements allow the proposed kernels to improve on the\nclassification performances of the ST-based kernel for several real-world\ndatasets, reaching state-of-the-art performances.",
    "published": "2015-07-13T09:50:41Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Training artificial neural networks to learn a nondeterministic game",
    "authors": [
      "Thomas E. Portegys"
    ],
    "summary": "It is well known that artificial neural networks (ANNs) can learn\ndeterministic automata. Learning nondeterministic automata is another matter.\nThis is important because much of the world is nondeterministic, taking the\nform of unpredictable or probabilistic events that must be acted upon. If ANNs\nare to engage such phenomena, then they must be able to learn how to deal with\nnondeterminism. In this project the game of Pong poses a nondeterministic\nenvironment. The learner is given an incomplete view of the game state and\nunderlying deterministic physics, resulting in a nondeterministic game. Three\nmodels were trained and tested on the game: Mona, Elman, and Numenta's NuPIC.",
    "published": "2015-07-14T21:16:23Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Towards Predicting First Daily Departure Times: a Gaussian Modeling\n  Approach for Load Shift Forecasting",
    "authors": [
      "Nicholas H. Kirk",
      "Ilya Dianov"
    ],
    "summary": "This work provides two statistical Gaussian forecasting methods for\npredicting First Daily Departure Times (FDDTs) of everyday use electric\nvehicles. This is important in smart grid applications to understand\ndisconnection times of such mobile storage units, for instance to forecast\nstorage of non dispatchable loads (e.g. wind and solar power). We provide a\nreview of the relevant state-of-the-art driving behavior features towards FDDT\nprediction, to then propose an approximated Gaussian method which qualitatively\nforecasts how many vehicles will depart within a given time frame, by assuming\nthat departure times follow a normal distribution. This method considers\nsampling sessions as Poisson distributions which are superimposed to obtain a\nsingle approximated Gaussian model. Given the Gaussian distribution assumption\nof the departure times, we also model the problem with Gaussian Mixture Models\n(GMM), in which the priorly set number of clusters represents the desired time\ngranularity. Evaluation has proven that for the dataset tested, low error and\nhigh confidence ($\\approx 95\\%$) is possible for 15 and 10 minute intervals,\nand that GMM outperforms traditional modeling but is less generalizable across\ndatasets, as it is a closer fit to the sampling data. Conclusively we discuss\nfuture possibilities and practical applications of the discussed model.",
    "published": "2015-07-16T09:28:27Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed\n  Bandits",
    "authors": [
      "Alexandra Carpentier",
      "Alessandro Lazaric",
      "Mohammad Ghavamzadeh",
      "Rémi Munos",
      "Peter Auer",
      "András Antos"
    ],
    "summary": "In this paper, we study the problem of estimating uniformly well the mean\nvalues of several distributions given a finite budget of samples. If the\nvariance of the distributions were known, one could design an optimal sampling\nstrategy by collecting a number of independent samples per distribution that is\nproportional to their variance. However, in the more realistic case where the\ndistributions are not known in advance, one needs to design adaptive sampling\nstrategies in order to select which distribution to sample from according to\nthe previously observed samples. We describe two strategies based on pulling\nthe distributions a number of times that is proportional to a high-probability\nupper-confidence-bound on their variance (built from previous observed samples)\nand report a finite-sample performance analysis on the excess estimation error\ncompared to the optimal allocation. We show that the performance of these\nallocation strategies depends not only on the variances but also on the full\nshape of the distributions.",
    "published": "2015-07-16T11:02:13Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Maximum Entropy Deep Inverse Reinforcement Learning",
    "authors": [
      "Markus Wulfmeier",
      "Peter Ondruska",
      "Ingmar Posner"
    ],
    "summary": "This paper presents a general framework for exploiting the representational\ncapacity of neural networks to approximate complex, nonlinear reward functions\nin the context of solving the inverse reinforcement learning (IRL) problem. We\nshow in this context that the Maximum Entropy paradigm for IRL lends itself\nnaturally to the efficient training of deep architectures. At test time, the\napproach leads to a computational complexity independent of the number of\ndemonstrations, which makes it especially well-suited for applications in\nlife-long learning scenarios. Our approach achieves performance commensurate to\nthe state-of-the-art on existing benchmarks while exceeding on an alternative\nbenchmark based on highly varying reward structures. Finally, we extend the\nbasic architecture - which is equivalent to a simplified subclass of Fully\nConvolutional Neural Networks (FCNNs) with width one - to include larger\nconvolutions in order to eliminate dependency on precomputed spatial features\nand work on raw input representations.",
    "published": "2015-07-17T09:30:03Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays",
    "authors": [
      "Aleksandr Vorobev",
      "Gleb Gusev"
    ],
    "summary": "We study the stochastic multi-armed bandit problem with non-equivalent\nmultiple plays where, at each step, an agent chooses not only a set of arms,\nbut also their order, which influences reward distribution. In several problem\nformulations with different assumptions, we provide lower bounds for regret\nwith standard asymptotics $O(\\log{t})$ but novel coefficients and provide\noptimal algorithms, thus proving that these bounds cannot be improved.",
    "published": "2015-07-17T10:39:52Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "2 Notes on Classes with Vapnik-Chervonenkis Dimension 1",
    "authors": [
      "Shai Ben-David"
    ],
    "summary": "The Vapnik-Chervonenkis dimension is a combinatorial parameter that reflects\nthe \"complexity\" of a set of sets (a.k.a. concept classes). It has been\nintroduced by Vapnik and Chervonenkis in their seminal 1971 paper and has since\nfound many applications, most notably in machine learning theory and in\ncomputational geometry. Arguably the most influential consequence of the VC\nanalysis is the fundamental theorem of statistical machine learning, stating\nthat a concept class is learnable (in some precise sense) if and only if its\nVC-dimension is finite. Furthermore, for such classes a most simple learning\nrule - empirical risk minimization (ERM) - is guaranteed to succeed.\n  The simplest non-trivial structures, in terms of the VC-dimension, are the\nclasses (i.e., sets of subsets) for which that dimension is 1.\n  In this note we show a couple of curious results concerning such classes. The\nfirst result shows that such classes share a very simple structure, and, as a\ncorollary, the labeling information contained in any sample labeled by such a\nclass can be compressed into a single instance.\n  The second result shows that due to some subtle measurability issues, in\nspite of the above mentioned fundamental theorem, there are classes of\ndimension 1 for which an ERM learning rule fails miserably.",
    "published": "2015-07-19T16:55:08Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "AMP: a new time-frequency feature extraction method for intermittent\n  time-series data",
    "authors": [
      "Duncan Barrack",
      "James Goulding",
      "Keith Hopcraft",
      "Simon Preston",
      "Gavin Smith"
    ],
    "summary": "The characterisation of time-series data via their most salient features is\nextremely important in a range of machine learning task, not least of all with\nregards to classification and clustering. While there exist many feature\nextraction techniques suitable for non-intermittent time-series data, these\napproaches are not always appropriate for intermittent time-series data, where\nintermittency is characterized by constant values for large periods of time\npunctuated by sharp and transient increases or decreases in value.\n  Motivated by this, we present aggregation, mode decomposition and projection\n(AMP) a feature extraction technique particularly suited to intermittent\ntime-series data which contain time-frequency patterns. For our method all\nindividual time-series within a set are combined to form a non-intermittent\naggregate. This is decomposed into a set of components which represent the\nintrinsic time-frequency signals within the data set. Individual time-series\ncan then be fit to these components to obtain a set of numerical features that\nrepresent their intrinsic time-frequency patterns. To demonstrate the\neffectiveness of AMP, we evaluate against the real word task of clustering\nintermittent time-series data. Using synthetically generated data we show that\na clustering approach which uses the features derived from AMP significantly\noutperforms traditional clustering methods. Our technique is further\nexemplified on a real world data set where AMP can be used to discover\ngroupings of individuals which correspond to real world sub-populations.",
    "published": "2015-07-20T11:48:01Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Bandit-Based Task Assignment for Heterogeneous Crowdsourcing",
    "authors": [
      "Hao Zhang",
      "Yao Ma",
      "Masashi Sugiyama"
    ],
    "summary": "We consider a task assignment problem in crowdsourcing, which is aimed at\ncollecting as many reliable labels as possible within a limited budget. A\nchallenge in this scenario is how to cope with the diversity of tasks and the\ntask-dependent reliability of workers, e.g., a worker may be good at\nrecognizing the name of sports teams, but not be familiar with cosmetics\nbrands. We refer to this practical setting as heterogeneous crowdsourcing. In\nthis paper, we propose a contextual bandit formulation for task assignment in\nheterogeneous crowdsourcing, which is able to deal with the\nexploration-exploitation trade-off in worker selection. We also theoretically\ninvestigate the regret bounds for the proposed method, and demonstrate its\npractical usefulness experimentally.",
    "published": "2015-07-21T12:17:58Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A study of the classification of low-dimensional data with supervised\n  manifold learning",
    "authors": [
      "Elif Vural",
      "Christine Guillemot"
    ],
    "summary": "Supervised manifold learning methods learn data representations by preserving\nthe geometric structure of data while enhancing the separation between data\nsamples from different classes. In this work, we propose a theoretical study of\nsupervised manifold learning for classification. We consider nonlinear\ndimensionality reduction algorithms that yield linearly separable embeddings of\ntraining data and present generalization bounds for this type of algorithms. A\nnecessary condition for satisfactory generalization performance is that the\nembedding allow the construction of a sufficiently regular interpolation\nfunction in relation with the separation margin of the embedding. We show that\nfor supervised embeddings satisfying this condition, the classification error\ndecays at an exponential rate with the number of training samples. Finally, we\nexamine the separability of supervised nonlinear embeddings that aim to\npreserve the low-dimensional geometric structure of data based on graph\nrepresentations. The proposed analysis is supported by experiments on several\nreal data sets.",
    "published": "2015-07-21T15:55:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "Deep Recurrent Q-Learning for Partially Observable MDPs",
    "authors": [
      "Matthew Hausknecht",
      "Peter Stone"
    ],
    "summary": "Deep Reinforcement Learning has yielded proficient controllers for complex\ntasks. However, these controllers have limited memory and rely on being able to\nperceive the complete game screen at each decision point. To address these\nshortcomings, this article investigates the effects of adding recurrency to a\nDeep Q-Network (DQN) by replacing the first post-convolutional fully-connected\nlayer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network}\n(DRQN), although capable of seeing only a single frame at each timestep,\nsuccessfully integrates information through time and replicates DQN's\nperformance on standard Atari games and partially observed equivalents\nfeaturing flickering game screens. Additionally, when trained with partial\nobservations and evaluated with incrementally more complete observations,\nDRQN's performance scales as a function of observability. Conversely, when\ntrained with full observations and evaluated with partial observations, DRQN's\nperformance degrades less than DQN's. Thus, given the same length of history,\nrecurrency is a viable alternative to stacking a history of frames in the DQN's\ninput layer and while recurrency confers no systematic advantage when learning\nto play the game, the recurrent net can better adapt at evaluation time if the\nquality of observations changes.",
    "published": "2015-07-23T15:16:46Z",
    "source": "arXiv",
    "category": "cs.LG"
  },
  {
    "title": "A Reinforcement Learning Approach to Online Learning of Decision Trees",
    "authors": [
      "Abhinav Garlapati",
      "Aditi Raghunathan",
      "Vaishnavh Nagarajan",
      "Balaraman Ravindran"
    ],
    "summary": "Online decision tree learning algorithms typically examine all features of a\nnew data point to update model parameters. We propose a novel alternative,\nReinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement\nLearning (RL) to actively examine a minimal number of features of a data point\nto classify it with high accuracy. Furthermore, RLDT optimizes a long term\nreturn, providing a better alternative to the traditional myopic greedy\napproach to growing decision trees. We demonstrate that this approach performs\nas well as batch learning algorithms and other online decision tree learning\nalgorithms, while making significantly fewer queries about the features of the\ndata points. We also show that RLDT can effectively handle concept drift.",
    "published": "2015-07-24T17:22:17Z",
    "source": "arXiv",
    "category": "cs.LG"
  }
]